{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.9999463144897193,
  "eval_steps": 500,
  "global_step": 18626,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0005368551028077521,
      "grad_norm": 45.460269927978516,
      "learning_rate": 8.94454382826476e-08,
      "loss": 3.5861,
      "step": 10
    },
    {
      "epoch": 0.0010737102056155043,
      "grad_norm": 24.043258666992188,
      "learning_rate": 1.788908765652952e-07,
      "loss": 3.7699,
      "step": 20
    },
    {
      "epoch": 0.0016105653084232566,
      "grad_norm": 31.925207138061523,
      "learning_rate": 2.6833631484794277e-07,
      "loss": 3.9194,
      "step": 30
    },
    {
      "epoch": 0.0021474204112310086,
      "grad_norm": 25.162328720092773,
      "learning_rate": 3.577817531305904e-07,
      "loss": 3.9822,
      "step": 40
    },
    {
      "epoch": 0.002684275514038761,
      "grad_norm": 35.98725509643555,
      "learning_rate": 4.47227191413238e-07,
      "loss": 3.6521,
      "step": 50
    },
    {
      "epoch": 0.003221130616846513,
      "grad_norm": 29.28506088256836,
      "learning_rate": 5.366726296958855e-07,
      "loss": 3.5772,
      "step": 60
    },
    {
      "epoch": 0.003757985719654265,
      "grad_norm": 31.444040298461914,
      "learning_rate": 6.26118067978533e-07,
      "loss": 3.8221,
      "step": 70
    },
    {
      "epoch": 0.004294840822462017,
      "grad_norm": 24.413436889648438,
      "learning_rate": 7.155635062611808e-07,
      "loss": 3.6556,
      "step": 80
    },
    {
      "epoch": 0.00483169592526977,
      "grad_norm": 25.861753463745117,
      "learning_rate": 8.050089445438284e-07,
      "loss": 3.0963,
      "step": 90
    },
    {
      "epoch": 0.005368551028077522,
      "grad_norm": 25.89274787902832,
      "learning_rate": 8.94454382826476e-07,
      "loss": 3.1698,
      "step": 100
    },
    {
      "epoch": 0.005905406130885274,
      "grad_norm": 22.787872314453125,
      "learning_rate": 9.838998211091236e-07,
      "loss": 2.9765,
      "step": 110
    },
    {
      "epoch": 0.006442261233693026,
      "grad_norm": 22.402860641479492,
      "learning_rate": 1.073345259391771e-06,
      "loss": 3.1309,
      "step": 120
    },
    {
      "epoch": 0.006979116336500778,
      "grad_norm": 19.699426651000977,
      "learning_rate": 1.1627906976744188e-06,
      "loss": 3.0069,
      "step": 130
    },
    {
      "epoch": 0.00751597143930853,
      "grad_norm": 20.756311416625977,
      "learning_rate": 1.252236135957066e-06,
      "loss": 2.9675,
      "step": 140
    },
    {
      "epoch": 0.008052826542116283,
      "grad_norm": 21.800580978393555,
      "learning_rate": 1.3416815742397138e-06,
      "loss": 2.8854,
      "step": 150
    },
    {
      "epoch": 0.008589681644924034,
      "grad_norm": 22.354496002197266,
      "learning_rate": 1.4311270125223615e-06,
      "loss": 2.7602,
      "step": 160
    },
    {
      "epoch": 0.009126536747731787,
      "grad_norm": 18.408493041992188,
      "learning_rate": 1.520572450805009e-06,
      "loss": 2.503,
      "step": 170
    },
    {
      "epoch": 0.00966339185053954,
      "grad_norm": 17.4278507232666,
      "learning_rate": 1.6100178890876567e-06,
      "loss": 2.4386,
      "step": 180
    },
    {
      "epoch": 0.010200246953347291,
      "grad_norm": 17.149494171142578,
      "learning_rate": 1.6994633273703042e-06,
      "loss": 2.4034,
      "step": 190
    },
    {
      "epoch": 0.010737102056155044,
      "grad_norm": 19.130016326904297,
      "learning_rate": 1.788908765652952e-06,
      "loss": 2.5062,
      "step": 200
    },
    {
      "epoch": 0.011273957158962795,
      "grad_norm": 16.2971248626709,
      "learning_rate": 1.8783542039355994e-06,
      "loss": 2.3067,
      "step": 210
    },
    {
      "epoch": 0.011810812261770548,
      "grad_norm": 18.11786460876465,
      "learning_rate": 1.967799642218247e-06,
      "loss": 2.7697,
      "step": 220
    },
    {
      "epoch": 0.0123476673645783,
      "grad_norm": 17.429473876953125,
      "learning_rate": 2.0572450805008946e-06,
      "loss": 2.2275,
      "step": 230
    },
    {
      "epoch": 0.012884522467386052,
      "grad_norm": 18.49774932861328,
      "learning_rate": 2.146690518783542e-06,
      "loss": 2.3622,
      "step": 240
    },
    {
      "epoch": 0.013421377570193805,
      "grad_norm": 15.055150985717773,
      "learning_rate": 2.2361359570661897e-06,
      "loss": 2.3412,
      "step": 250
    },
    {
      "epoch": 0.013958232673001556,
      "grad_norm": 16.20669937133789,
      "learning_rate": 2.3255813953488376e-06,
      "loss": 2.2739,
      "step": 260
    },
    {
      "epoch": 0.01449508777580931,
      "grad_norm": 19.289764404296875,
      "learning_rate": 2.415026833631485e-06,
      "loss": 2.2832,
      "step": 270
    },
    {
      "epoch": 0.01503194287861706,
      "grad_norm": 16.81325340270996,
      "learning_rate": 2.504472271914132e-06,
      "loss": 1.9154,
      "step": 280
    },
    {
      "epoch": 0.015568797981424813,
      "grad_norm": 17.578046798706055,
      "learning_rate": 2.59391771019678e-06,
      "loss": 1.8617,
      "step": 290
    },
    {
      "epoch": 0.016105653084232566,
      "grad_norm": 20.8323917388916,
      "learning_rate": 2.6833631484794276e-06,
      "loss": 1.9192,
      "step": 300
    },
    {
      "epoch": 0.016642508187040318,
      "grad_norm": 13.06420612335205,
      "learning_rate": 2.772808586762075e-06,
      "loss": 1.8327,
      "step": 310
    },
    {
      "epoch": 0.01717936328984807,
      "grad_norm": 14.27224349975586,
      "learning_rate": 2.862254025044723e-06,
      "loss": 1.8247,
      "step": 320
    },
    {
      "epoch": 0.017716218392655823,
      "grad_norm": 18.729976654052734,
      "learning_rate": 2.9516994633273705e-06,
      "loss": 1.8605,
      "step": 330
    },
    {
      "epoch": 0.018253073495463575,
      "grad_norm": 12.906477928161621,
      "learning_rate": 3.041144901610018e-06,
      "loss": 1.9161,
      "step": 340
    },
    {
      "epoch": 0.018789928598271326,
      "grad_norm": 12.647000312805176,
      "learning_rate": 3.1305903398926655e-06,
      "loss": 1.8474,
      "step": 350
    },
    {
      "epoch": 0.01932678370107908,
      "grad_norm": 15.294533729553223,
      "learning_rate": 3.2200357781753134e-06,
      "loss": 1.6509,
      "step": 360
    },
    {
      "epoch": 0.01986363880388683,
      "grad_norm": 14.74196720123291,
      "learning_rate": 3.309481216457961e-06,
      "loss": 1.8379,
      "step": 370
    },
    {
      "epoch": 0.020400493906694583,
      "grad_norm": 14.86855697631836,
      "learning_rate": 3.3989266547406084e-06,
      "loss": 1.6068,
      "step": 380
    },
    {
      "epoch": 0.020937349009502334,
      "grad_norm": 11.408946990966797,
      "learning_rate": 3.4883720930232564e-06,
      "loss": 1.6692,
      "step": 390
    },
    {
      "epoch": 0.02147420411231009,
      "grad_norm": 14.891518592834473,
      "learning_rate": 3.577817531305904e-06,
      "loss": 1.703,
      "step": 400
    },
    {
      "epoch": 0.02201105921511784,
      "grad_norm": 17.497392654418945,
      "learning_rate": 3.6672629695885514e-06,
      "loss": 1.5501,
      "step": 410
    },
    {
      "epoch": 0.02254791431792559,
      "grad_norm": 17.85817527770996,
      "learning_rate": 3.756708407871199e-06,
      "loss": 1.6012,
      "step": 420
    },
    {
      "epoch": 0.023084769420733346,
      "grad_norm": 14.89450740814209,
      "learning_rate": 3.846153846153847e-06,
      "loss": 1.6431,
      "step": 430
    },
    {
      "epoch": 0.023621624523541097,
      "grad_norm": 13.834882736206055,
      "learning_rate": 3.935599284436494e-06,
      "loss": 1.8257,
      "step": 440
    },
    {
      "epoch": 0.024158479626348848,
      "grad_norm": 18.38890266418457,
      "learning_rate": 4.025044722719142e-06,
      "loss": 1.6789,
      "step": 450
    },
    {
      "epoch": 0.0246953347291566,
      "grad_norm": 11.043734550476074,
      "learning_rate": 4.114490161001789e-06,
      "loss": 1.5056,
      "step": 460
    },
    {
      "epoch": 0.025232189831964354,
      "grad_norm": 15.59850788116455,
      "learning_rate": 4.203935599284437e-06,
      "loss": 1.5113,
      "step": 470
    },
    {
      "epoch": 0.025769044934772105,
      "grad_norm": 15.630155563354492,
      "learning_rate": 4.293381037567084e-06,
      "loss": 1.6656,
      "step": 480
    },
    {
      "epoch": 0.026305900037579856,
      "grad_norm": 12.143364906311035,
      "learning_rate": 4.382826475849732e-06,
      "loss": 2.1671,
      "step": 490
    },
    {
      "epoch": 0.02684275514038761,
      "grad_norm": 12.963175773620605,
      "learning_rate": 4.472271914132379e-06,
      "loss": 1.7896,
      "step": 500
    },
    {
      "epoch": 0.027379610243195362,
      "grad_norm": 14.984971046447754,
      "learning_rate": 4.561717352415027e-06,
      "loss": 1.4947,
      "step": 510
    },
    {
      "epoch": 0.027916465346003113,
      "grad_norm": 15.819538116455078,
      "learning_rate": 4.651162790697675e-06,
      "loss": 1.6294,
      "step": 520
    },
    {
      "epoch": 0.028453320448810868,
      "grad_norm": 14.638567924499512,
      "learning_rate": 4.740608228980323e-06,
      "loss": 1.8444,
      "step": 530
    },
    {
      "epoch": 0.02899017555161862,
      "grad_norm": 12.825884819030762,
      "learning_rate": 4.83005366726297e-06,
      "loss": 1.4168,
      "step": 540
    },
    {
      "epoch": 0.02952703065442637,
      "grad_norm": 17.444271087646484,
      "learning_rate": 4.919499105545618e-06,
      "loss": 1.6703,
      "step": 550
    },
    {
      "epoch": 0.03006388575723412,
      "grad_norm": 13.558156967163086,
      "learning_rate": 4.999999962208895e-06,
      "loss": 1.3585,
      "step": 560
    },
    {
      "epoch": 0.030600740860041876,
      "grad_norm": 14.0825834274292,
      "learning_rate": 4.999995427277649e-06,
      "loss": 1.663,
      "step": 570
    },
    {
      "epoch": 0.031137595962849627,
      "grad_norm": 13.44847297668457,
      "learning_rate": 4.9999833341410635e-06,
      "loss": 1.2031,
      "step": 580
    },
    {
      "epoch": 0.03167445106565738,
      "grad_norm": 14.0843505859375,
      "learning_rate": 4.9999636828357e-06,
      "loss": 1.7972,
      "step": 590
    },
    {
      "epoch": 0.03221130616846513,
      "grad_norm": 15.103700637817383,
      "learning_rate": 4.999936473420971e-06,
      "loss": 1.4777,
      "step": 600
    },
    {
      "epoch": 0.032748161271272884,
      "grad_norm": 16.787925720214844,
      "learning_rate": 4.999901705979138e-06,
      "loss": 1.7091,
      "step": 610
    },
    {
      "epoch": 0.033285016374080635,
      "grad_norm": 11.518420219421387,
      "learning_rate": 4.9998593806153125e-06,
      "loss": 1.2518,
      "step": 620
    },
    {
      "epoch": 0.033821871476888386,
      "grad_norm": 11.527627944946289,
      "learning_rate": 4.999809497457457e-06,
      "loss": 1.5757,
      "step": 630
    },
    {
      "epoch": 0.03435872657969614,
      "grad_norm": 10.920843124389648,
      "learning_rate": 4.999752056656382e-06,
      "loss": 1.5223,
      "step": 640
    },
    {
      "epoch": 0.034895581682503896,
      "grad_norm": 19.04953956604004,
      "learning_rate": 4.999687058385747e-06,
      "loss": 1.7055,
      "step": 650
    },
    {
      "epoch": 0.03543243678531165,
      "grad_norm": 11.170331954956055,
      "learning_rate": 4.9996145028420615e-06,
      "loss": 1.3072,
      "step": 660
    },
    {
      "epoch": 0.0359692918881194,
      "grad_norm": 17.498394012451172,
      "learning_rate": 4.999534390244681e-06,
      "loss": 1.6721,
      "step": 670
    },
    {
      "epoch": 0.03650614699092715,
      "grad_norm": 12.15459156036377,
      "learning_rate": 4.9994467208358115e-06,
      "loss": 1.2925,
      "step": 680
    },
    {
      "epoch": 0.0370430020937349,
      "grad_norm": 11.764025688171387,
      "learning_rate": 4.999351494880499e-06,
      "loss": 1.3774,
      "step": 690
    },
    {
      "epoch": 0.03757985719654265,
      "grad_norm": 10.374739646911621,
      "learning_rate": 4.99924871266664e-06,
      "loss": 1.2881,
      "step": 700
    },
    {
      "epoch": 0.0381167122993504,
      "grad_norm": 11.001517295837402,
      "learning_rate": 4.999138374504977e-06,
      "loss": 1.1331,
      "step": 710
    },
    {
      "epoch": 0.03865356740215816,
      "grad_norm": 13.292499542236328,
      "learning_rate": 4.999020480729092e-06,
      "loss": 1.5097,
      "step": 720
    },
    {
      "epoch": 0.03919042250496591,
      "grad_norm": 9.959339141845703,
      "learning_rate": 4.998895031695412e-06,
      "loss": 1.4131,
      "step": 730
    },
    {
      "epoch": 0.03972727760777366,
      "grad_norm": 17.609384536743164,
      "learning_rate": 4.998762027783206e-06,
      "loss": 1.9142,
      "step": 740
    },
    {
      "epoch": 0.040264132710581414,
      "grad_norm": 16.252046585083008,
      "learning_rate": 4.998621469394582e-06,
      "loss": 1.8993,
      "step": 750
    },
    {
      "epoch": 0.040800987813389165,
      "grad_norm": 9.33243465423584,
      "learning_rate": 4.99847335695449e-06,
      "loss": 1.4534,
      "step": 760
    },
    {
      "epoch": 0.04133784291619692,
      "grad_norm": 11.797158241271973,
      "learning_rate": 4.998317690910716e-06,
      "loss": 1.4666,
      "step": 770
    },
    {
      "epoch": 0.04187469801900467,
      "grad_norm": 13.059012413024902,
      "learning_rate": 4.9981544717338824e-06,
      "loss": 1.1603,
      "step": 780
    },
    {
      "epoch": 0.042411553121812426,
      "grad_norm": 15.020527839660645,
      "learning_rate": 4.997983699917448e-06,
      "loss": 1.2212,
      "step": 790
    },
    {
      "epoch": 0.04294840822462018,
      "grad_norm": 11.446976661682129,
      "learning_rate": 4.997805375977707e-06,
      "loss": 1.3623,
      "step": 800
    },
    {
      "epoch": 0.04348526332742793,
      "grad_norm": 9.434560775756836,
      "learning_rate": 4.997619500453781e-06,
      "loss": 1.2487,
      "step": 810
    },
    {
      "epoch": 0.04402211843023568,
      "grad_norm": 14.297173500061035,
      "learning_rate": 4.997426073907626e-06,
      "loss": 1.5364,
      "step": 820
    },
    {
      "epoch": 0.04455897353304343,
      "grad_norm": 12.35734748840332,
      "learning_rate": 4.997225096924028e-06,
      "loss": 1.3807,
      "step": 830
    },
    {
      "epoch": 0.04509582863585118,
      "grad_norm": 10.338175773620605,
      "learning_rate": 4.997016570110597e-06,
      "loss": 1.5869,
      "step": 840
    },
    {
      "epoch": 0.04563268373865893,
      "grad_norm": 14.130302429199219,
      "learning_rate": 4.996800494097768e-06,
      "loss": 1.5674,
      "step": 850
    },
    {
      "epoch": 0.04616953884146669,
      "grad_norm": 11.551674842834473,
      "learning_rate": 4.996576869538805e-06,
      "loss": 1.3932,
      "step": 860
    },
    {
      "epoch": 0.04670639394427444,
      "grad_norm": 16.6386775970459,
      "learning_rate": 4.996345697109785e-06,
      "loss": 1.4229,
      "step": 870
    },
    {
      "epoch": 0.04724324904708219,
      "grad_norm": 10.258707046508789,
      "learning_rate": 4.99610697750961e-06,
      "loss": 1.399,
      "step": 880
    },
    {
      "epoch": 0.047780104149889945,
      "grad_norm": 12.27953052520752,
      "learning_rate": 4.99586071146e-06,
      "loss": 1.3652,
      "step": 890
    },
    {
      "epoch": 0.048316959252697696,
      "grad_norm": 18.203651428222656,
      "learning_rate": 4.995606899705486e-06,
      "loss": 1.7541,
      "step": 900
    },
    {
      "epoch": 0.04885381435550545,
      "grad_norm": 11.889490127563477,
      "learning_rate": 4.9953455430134144e-06,
      "loss": 0.9897,
      "step": 910
    },
    {
      "epoch": 0.0493906694583132,
      "grad_norm": 9.412690162658691,
      "learning_rate": 4.995076642173941e-06,
      "loss": 0.9678,
      "step": 920
    },
    {
      "epoch": 0.049927524561120956,
      "grad_norm": 16.13521957397461,
      "learning_rate": 4.994800198000033e-06,
      "loss": 1.6555,
      "step": 930
    },
    {
      "epoch": 0.05046437966392871,
      "grad_norm": 20.689594268798828,
      "learning_rate": 4.9945162113274585e-06,
      "loss": 1.6006,
      "step": 940
    },
    {
      "epoch": 0.05100123476673646,
      "grad_norm": 18.512807846069336,
      "learning_rate": 4.994224683014791e-06,
      "loss": 1.7834,
      "step": 950
    },
    {
      "epoch": 0.05153808986954421,
      "grad_norm": 10.637067794799805,
      "learning_rate": 4.993925613943405e-06,
      "loss": 1.6591,
      "step": 960
    },
    {
      "epoch": 0.05207494497235196,
      "grad_norm": 10.417563438415527,
      "learning_rate": 4.993619005017472e-06,
      "loss": 1.0251,
      "step": 970
    },
    {
      "epoch": 0.05261180007515971,
      "grad_norm": 12.903435707092285,
      "learning_rate": 4.993304857163959e-06,
      "loss": 1.5916,
      "step": 980
    },
    {
      "epoch": 0.05314865517796746,
      "grad_norm": 11.987259864807129,
      "learning_rate": 4.992983171332625e-06,
      "loss": 1.3659,
      "step": 990
    },
    {
      "epoch": 0.05368551028077522,
      "grad_norm": 14.259053230285645,
      "learning_rate": 4.99265394849602e-06,
      "loss": 1.1913,
      "step": 1000
    },
    {
      "epoch": 0.05422236538358297,
      "grad_norm": 10.52511215209961,
      "learning_rate": 4.992317189649478e-06,
      "loss": 0.9519,
      "step": 1010
    },
    {
      "epoch": 0.054759220486390724,
      "grad_norm": 11.156561851501465,
      "learning_rate": 4.991972895811119e-06,
      "loss": 1.3438,
      "step": 1020
    },
    {
      "epoch": 0.055296075589198475,
      "grad_norm": 16.05250358581543,
      "learning_rate": 4.991621068021842e-06,
      "loss": 1.6984,
      "step": 1030
    },
    {
      "epoch": 0.055832930692006226,
      "grad_norm": 15.92250919342041,
      "learning_rate": 4.991261707345323e-06,
      "loss": 1.5235,
      "step": 1040
    },
    {
      "epoch": 0.05636978579481398,
      "grad_norm": 13.844340324401855,
      "learning_rate": 4.990894814868012e-06,
      "loss": 1.1451,
      "step": 1050
    },
    {
      "epoch": 0.056906640897621735,
      "grad_norm": 13.630294799804688,
      "learning_rate": 4.990520391699133e-06,
      "loss": 1.2051,
      "step": 1060
    },
    {
      "epoch": 0.057443496000429486,
      "grad_norm": 15.19543170928955,
      "learning_rate": 4.990138438970673e-06,
      "loss": 1.2836,
      "step": 1070
    },
    {
      "epoch": 0.05798035110323724,
      "grad_norm": 14.003798484802246,
      "learning_rate": 4.9897489578373855e-06,
      "loss": 1.6094,
      "step": 1080
    },
    {
      "epoch": 0.05851720620604499,
      "grad_norm": 10.518413543701172,
      "learning_rate": 4.989351949476785e-06,
      "loss": 1.1312,
      "step": 1090
    },
    {
      "epoch": 0.05905406130885274,
      "grad_norm": 11.736445426940918,
      "learning_rate": 4.9889474150891396e-06,
      "loss": 1.5919,
      "step": 1100
    },
    {
      "epoch": 0.05959091641166049,
      "grad_norm": 10.473881721496582,
      "learning_rate": 4.988535355897475e-06,
      "loss": 1.3014,
      "step": 1110
    },
    {
      "epoch": 0.06012777151446824,
      "grad_norm": 10.007888793945312,
      "learning_rate": 4.988115773147563e-06,
      "loss": 1.2997,
      "step": 1120
    },
    {
      "epoch": 0.060664626617276,
      "grad_norm": 17.150348663330078,
      "learning_rate": 4.987688668107926e-06,
      "loss": 1.7064,
      "step": 1130
    },
    {
      "epoch": 0.06120148172008375,
      "grad_norm": 11.549002647399902,
      "learning_rate": 4.987254042069823e-06,
      "loss": 0.9953,
      "step": 1140
    },
    {
      "epoch": 0.0617383368228915,
      "grad_norm": 12.428756713867188,
      "learning_rate": 4.986811896347254e-06,
      "loss": 1.2676,
      "step": 1150
    },
    {
      "epoch": 0.062275191925699254,
      "grad_norm": 8.988959312438965,
      "learning_rate": 4.986362232276953e-06,
      "loss": 1.4153,
      "step": 1160
    },
    {
      "epoch": 0.06281204702850701,
      "grad_norm": 11.30033016204834,
      "learning_rate": 4.9859050512183835e-06,
      "loss": 1.3826,
      "step": 1170
    },
    {
      "epoch": 0.06334890213131476,
      "grad_norm": 9.830764770507812,
      "learning_rate": 4.985440354553735e-06,
      "loss": 1.5355,
      "step": 1180
    },
    {
      "epoch": 0.06388575723412251,
      "grad_norm": 19.010128021240234,
      "learning_rate": 4.98496814368792e-06,
      "loss": 1.169,
      "step": 1190
    },
    {
      "epoch": 0.06442261233693027,
      "grad_norm": 12.570661544799805,
      "learning_rate": 4.984488420048567e-06,
      "loss": 1.093,
      "step": 1200
    },
    {
      "epoch": 0.06495946743973802,
      "grad_norm": 19.42620277404785,
      "learning_rate": 4.98400118508602e-06,
      "loss": 1.3871,
      "step": 1210
    },
    {
      "epoch": 0.06549632254254577,
      "grad_norm": 14.955121994018555,
      "learning_rate": 4.9835064402733285e-06,
      "loss": 1.5047,
      "step": 1220
    },
    {
      "epoch": 0.06603317764535352,
      "grad_norm": 16.1697998046875,
      "learning_rate": 4.98300418710625e-06,
      "loss": 1.4022,
      "step": 1230
    },
    {
      "epoch": 0.06657003274816127,
      "grad_norm": 11.658538818359375,
      "learning_rate": 4.982494427103239e-06,
      "loss": 1.2772,
      "step": 1240
    },
    {
      "epoch": 0.06710688785096902,
      "grad_norm": 16.234020233154297,
      "learning_rate": 4.981977161805448e-06,
      "loss": 1.2066,
      "step": 1250
    },
    {
      "epoch": 0.06764374295377677,
      "grad_norm": 11.486591339111328,
      "learning_rate": 4.981452392776718e-06,
      "loss": 1.1038,
      "step": 1260
    },
    {
      "epoch": 0.06818059805658452,
      "grad_norm": 8.788490295410156,
      "learning_rate": 4.980920121603577e-06,
      "loss": 1.5938,
      "step": 1270
    },
    {
      "epoch": 0.06871745315939227,
      "grad_norm": 10.579232215881348,
      "learning_rate": 4.980380349895232e-06,
      "loss": 0.8626,
      "step": 1280
    },
    {
      "epoch": 0.06925430826220003,
      "grad_norm": 15.67060661315918,
      "learning_rate": 4.979833079283572e-06,
      "loss": 1.2095,
      "step": 1290
    },
    {
      "epoch": 0.06979116336500779,
      "grad_norm": 12.815376281738281,
      "learning_rate": 4.97927831142315e-06,
      "loss": 1.4065,
      "step": 1300
    },
    {
      "epoch": 0.07032801846781554,
      "grad_norm": 15.765726089477539,
      "learning_rate": 4.978716047991191e-06,
      "loss": 1.5561,
      "step": 1310
    },
    {
      "epoch": 0.0708648735706233,
      "grad_norm": 12.804259300231934,
      "learning_rate": 4.978146290687578e-06,
      "loss": 1.1205,
      "step": 1320
    },
    {
      "epoch": 0.07140172867343104,
      "grad_norm": 11.372343063354492,
      "learning_rate": 4.977569041234851e-06,
      "loss": 1.4068,
      "step": 1330
    },
    {
      "epoch": 0.0719385837762388,
      "grad_norm": 10.030970573425293,
      "learning_rate": 4.9769843013782016e-06,
      "loss": 1.5787,
      "step": 1340
    },
    {
      "epoch": 0.07247543887904655,
      "grad_norm": 10.832488059997559,
      "learning_rate": 4.9763920728854674e-06,
      "loss": 1.4609,
      "step": 1350
    },
    {
      "epoch": 0.0730122939818543,
      "grad_norm": 8.727134704589844,
      "learning_rate": 4.975792357547124e-06,
      "loss": 0.8255,
      "step": 1360
    },
    {
      "epoch": 0.07354914908466205,
      "grad_norm": 11.516378402709961,
      "learning_rate": 4.975185157176284e-06,
      "loss": 0.8334,
      "step": 1370
    },
    {
      "epoch": 0.0740860041874698,
      "grad_norm": 10.055022239685059,
      "learning_rate": 4.9745704736086894e-06,
      "loss": 0.8389,
      "step": 1380
    },
    {
      "epoch": 0.07462285929027755,
      "grad_norm": 10.08651065826416,
      "learning_rate": 4.9739483087027056e-06,
      "loss": 0.8668,
      "step": 1390
    },
    {
      "epoch": 0.0751597143930853,
      "grad_norm": 9.150856018066406,
      "learning_rate": 4.973318664339314e-06,
      "loss": 1.1964,
      "step": 1400
    },
    {
      "epoch": 0.07569656949589305,
      "grad_norm": 8.913542747497559,
      "learning_rate": 4.972681542422113e-06,
      "loss": 1.1585,
      "step": 1410
    },
    {
      "epoch": 0.0762334245987008,
      "grad_norm": 7.615073204040527,
      "learning_rate": 4.972036944877304e-06,
      "loss": 1.0935,
      "step": 1420
    },
    {
      "epoch": 0.07677027970150856,
      "grad_norm": 15.864347457885742,
      "learning_rate": 4.971384873653693e-06,
      "loss": 1.0599,
      "step": 1430
    },
    {
      "epoch": 0.07730713480431632,
      "grad_norm": 13.237309455871582,
      "learning_rate": 4.970725330722676e-06,
      "loss": 0.9812,
      "step": 1440
    },
    {
      "epoch": 0.07784398990712407,
      "grad_norm": 8.938522338867188,
      "learning_rate": 4.970058318078241e-06,
      "loss": 1.4464,
      "step": 1450
    },
    {
      "epoch": 0.07838084500993182,
      "grad_norm": 14.562503814697266,
      "learning_rate": 4.969383837736962e-06,
      "loss": 1.4037,
      "step": 1460
    },
    {
      "epoch": 0.07891770011273957,
      "grad_norm": 13.009746551513672,
      "learning_rate": 4.968701891737984e-06,
      "loss": 1.1917,
      "step": 1470
    },
    {
      "epoch": 0.07945455521554733,
      "grad_norm": 8.079020500183105,
      "learning_rate": 4.968012482143029e-06,
      "loss": 0.7777,
      "step": 1480
    },
    {
      "epoch": 0.07999141031835508,
      "grad_norm": 7.997498512268066,
      "learning_rate": 4.967315611036377e-06,
      "loss": 1.0018,
      "step": 1490
    },
    {
      "epoch": 0.08052826542116283,
      "grad_norm": 13.039105415344238,
      "learning_rate": 4.966611280524873e-06,
      "loss": 1.176,
      "step": 1500
    },
    {
      "epoch": 0.08106512052397058,
      "grad_norm": 12.74972152709961,
      "learning_rate": 4.965899492737909e-06,
      "loss": 1.3196,
      "step": 1510
    },
    {
      "epoch": 0.08160197562677833,
      "grad_norm": 11.960359573364258,
      "learning_rate": 4.965180249827425e-06,
      "loss": 1.1877,
      "step": 1520
    },
    {
      "epoch": 0.08213883072958608,
      "grad_norm": 16.979598999023438,
      "learning_rate": 4.964453553967899e-06,
      "loss": 1.3422,
      "step": 1530
    },
    {
      "epoch": 0.08267568583239383,
      "grad_norm": 9.746969223022461,
      "learning_rate": 4.96371940735634e-06,
      "loss": 1.1504,
      "step": 1540
    },
    {
      "epoch": 0.08321254093520158,
      "grad_norm": 16.20099449157715,
      "learning_rate": 4.962977812212287e-06,
      "loss": 1.6627,
      "step": 1550
    },
    {
      "epoch": 0.08374939603800934,
      "grad_norm": 10.558563232421875,
      "learning_rate": 4.962228770777795e-06,
      "loss": 0.794,
      "step": 1560
    },
    {
      "epoch": 0.08428625114081709,
      "grad_norm": 9.463888168334961,
      "learning_rate": 4.96147228531743e-06,
      "loss": 1.1804,
      "step": 1570
    },
    {
      "epoch": 0.08482310624362485,
      "grad_norm": 19.106138229370117,
      "learning_rate": 4.960708358118267e-06,
      "loss": 1.5006,
      "step": 1580
    },
    {
      "epoch": 0.0853599613464326,
      "grad_norm": 9.386693000793457,
      "learning_rate": 4.959936991489876e-06,
      "loss": 1.079,
      "step": 1590
    },
    {
      "epoch": 0.08589681644924035,
      "grad_norm": 15.190378189086914,
      "learning_rate": 4.959158187764323e-06,
      "loss": 1.2437,
      "step": 1600
    },
    {
      "epoch": 0.0864336715520481,
      "grad_norm": 9.169631004333496,
      "learning_rate": 4.958371949296153e-06,
      "loss": 1.346,
      "step": 1610
    },
    {
      "epoch": 0.08697052665485586,
      "grad_norm": 9.272505760192871,
      "learning_rate": 4.95757827846239e-06,
      "loss": 2.0485,
      "step": 1620
    },
    {
      "epoch": 0.08750738175766361,
      "grad_norm": 19.823707580566406,
      "learning_rate": 4.956777177662533e-06,
      "loss": 1.4599,
      "step": 1630
    },
    {
      "epoch": 0.08804423686047136,
      "grad_norm": 17.77994728088379,
      "learning_rate": 4.955968649318539e-06,
      "loss": 1.8431,
      "step": 1640
    },
    {
      "epoch": 0.08858109196327911,
      "grad_norm": 19.136898040771484,
      "learning_rate": 4.9551526958748195e-06,
      "loss": 1.3929,
      "step": 1650
    },
    {
      "epoch": 0.08911794706608686,
      "grad_norm": 15.444260597229004,
      "learning_rate": 4.954329319798239e-06,
      "loss": 1.4569,
      "step": 1660
    },
    {
      "epoch": 0.08965480216889461,
      "grad_norm": 16.38795280456543,
      "learning_rate": 4.953498523578098e-06,
      "loss": 1.5632,
      "step": 1670
    },
    {
      "epoch": 0.09019165727170236,
      "grad_norm": 8.564276695251465,
      "learning_rate": 4.952660309726135e-06,
      "loss": 1.1102,
      "step": 1680
    },
    {
      "epoch": 0.09072851237451011,
      "grad_norm": 8.032626152038574,
      "learning_rate": 4.95181468077651e-06,
      "loss": 0.9917,
      "step": 1690
    },
    {
      "epoch": 0.09126536747731787,
      "grad_norm": 13.140206336975098,
      "learning_rate": 4.950961639285803e-06,
      "loss": 1.1924,
      "step": 1700
    },
    {
      "epoch": 0.09180222258012563,
      "grad_norm": 10.744111061096191,
      "learning_rate": 4.950101187833004e-06,
      "loss": 1.1445,
      "step": 1710
    },
    {
      "epoch": 0.09233907768293338,
      "grad_norm": 9.438612937927246,
      "learning_rate": 4.949233329019505e-06,
      "loss": 1.3699,
      "step": 1720
    },
    {
      "epoch": 0.09287593278574113,
      "grad_norm": 9.144245147705078,
      "learning_rate": 4.9483580654690925e-06,
      "loss": 1.4553,
      "step": 1730
    },
    {
      "epoch": 0.09341278788854888,
      "grad_norm": 10.721570014953613,
      "learning_rate": 4.947475399827941e-06,
      "loss": 1.1819,
      "step": 1740
    },
    {
      "epoch": 0.09394964299135664,
      "grad_norm": 8.538431167602539,
      "learning_rate": 4.946585334764601e-06,
      "loss": 1.5456,
      "step": 1750
    },
    {
      "epoch": 0.09448649809416439,
      "grad_norm": 8.384821891784668,
      "learning_rate": 4.945687872969997e-06,
      "loss": 1.8654,
      "step": 1760
    },
    {
      "epoch": 0.09502335319697214,
      "grad_norm": 10.373551368713379,
      "learning_rate": 4.944783017157413e-06,
      "loss": 0.7645,
      "step": 1770
    },
    {
      "epoch": 0.09556020829977989,
      "grad_norm": 10.283239364624023,
      "learning_rate": 4.943870770062489e-06,
      "loss": 1.2206,
      "step": 1780
    },
    {
      "epoch": 0.09609706340258764,
      "grad_norm": 10.200223922729492,
      "learning_rate": 4.94295113444321e-06,
      "loss": 1.1307,
      "step": 1790
    },
    {
      "epoch": 0.09663391850539539,
      "grad_norm": 10.227203369140625,
      "learning_rate": 4.9420241130799e-06,
      "loss": 1.8022,
      "step": 1800
    },
    {
      "epoch": 0.09717077360820314,
      "grad_norm": 10.653651237487793,
      "learning_rate": 4.94108970877521e-06,
      "loss": 1.2727,
      "step": 1810
    },
    {
      "epoch": 0.0977076287110109,
      "grad_norm": 10.678781509399414,
      "learning_rate": 4.940147924354112e-06,
      "loss": 0.7975,
      "step": 1820
    },
    {
      "epoch": 0.09824448381381864,
      "grad_norm": 9.054078102111816,
      "learning_rate": 4.939198762663895e-06,
      "loss": 1.0851,
      "step": 1830
    },
    {
      "epoch": 0.0987813389166264,
      "grad_norm": 8.074854850769043,
      "learning_rate": 4.938242226574144e-06,
      "loss": 1.0273,
      "step": 1840
    },
    {
      "epoch": 0.09931819401943416,
      "grad_norm": 9.461112976074219,
      "learning_rate": 4.9372783189767455e-06,
      "loss": 1.1771,
      "step": 1850
    },
    {
      "epoch": 0.09985504912224191,
      "grad_norm": 10.421544075012207,
      "learning_rate": 4.936307042785867e-06,
      "loss": 0.9638,
      "step": 1860
    },
    {
      "epoch": 0.10039190422504966,
      "grad_norm": 8.310418128967285,
      "learning_rate": 4.935328400937959e-06,
      "loss": 1.0195,
      "step": 1870
    },
    {
      "epoch": 0.10092875932785741,
      "grad_norm": 9.356950759887695,
      "learning_rate": 4.934342396391734e-06,
      "loss": 0.9511,
      "step": 1880
    },
    {
      "epoch": 0.10146561443066517,
      "grad_norm": 7.01657772064209,
      "learning_rate": 4.9333490321281695e-06,
      "loss": 1.0369,
      "step": 1890
    },
    {
      "epoch": 0.10200246953347292,
      "grad_norm": 10.344090461730957,
      "learning_rate": 4.93234831115049e-06,
      "loss": 1.6856,
      "step": 1900
    },
    {
      "epoch": 0.10253932463628067,
      "grad_norm": 8.103877067565918,
      "learning_rate": 4.9313402364841636e-06,
      "loss": 1.0313,
      "step": 1910
    },
    {
      "epoch": 0.10307617973908842,
      "grad_norm": 18.170270919799805,
      "learning_rate": 4.9303248111768905e-06,
      "loss": 1.5853,
      "step": 1920
    },
    {
      "epoch": 0.10361303484189617,
      "grad_norm": 8.632328987121582,
      "learning_rate": 4.929302038298593e-06,
      "loss": 0.9836,
      "step": 1930
    },
    {
      "epoch": 0.10414988994470392,
      "grad_norm": 10.443403244018555,
      "learning_rate": 4.928271920941407e-06,
      "loss": 1.2209,
      "step": 1940
    },
    {
      "epoch": 0.10468674504751167,
      "grad_norm": 10.140669822692871,
      "learning_rate": 4.927234462219675e-06,
      "loss": 0.749,
      "step": 1950
    },
    {
      "epoch": 0.10522360015031942,
      "grad_norm": 7.7643141746521,
      "learning_rate": 4.926189665269933e-06,
      "loss": 1.335,
      "step": 1960
    },
    {
      "epoch": 0.10576045525312718,
      "grad_norm": 11.005542755126953,
      "learning_rate": 4.9251375332509015e-06,
      "loss": 0.8093,
      "step": 1970
    },
    {
      "epoch": 0.10629731035593493,
      "grad_norm": 7.971055030822754,
      "learning_rate": 4.9240780693434794e-06,
      "loss": 1.1904,
      "step": 1980
    },
    {
      "epoch": 0.10683416545874269,
      "grad_norm": 9.07944107055664,
      "learning_rate": 4.9230112767507295e-06,
      "loss": 1.0182,
      "step": 1990
    },
    {
      "epoch": 0.10737102056155044,
      "grad_norm": 10.084610939025879,
      "learning_rate": 4.921937158697874e-06,
      "loss": 1.3015,
      "step": 2000
    },
    {
      "epoch": 0.1079078756643582,
      "grad_norm": 8.114959716796875,
      "learning_rate": 4.920855718432282e-06,
      "loss": 1.1102,
      "step": 2010
    },
    {
      "epoch": 0.10844473076716594,
      "grad_norm": 8.14728832244873,
      "learning_rate": 4.919766959223456e-06,
      "loss": 1.3611,
      "step": 2020
    },
    {
      "epoch": 0.1089815858699737,
      "grad_norm": 10.183863639831543,
      "learning_rate": 4.918670884363028e-06,
      "loss": 0.9366,
      "step": 2030
    },
    {
      "epoch": 0.10951844097278145,
      "grad_norm": 11.224695205688477,
      "learning_rate": 4.91756749716475e-06,
      "loss": 1.5853,
      "step": 2040
    },
    {
      "epoch": 0.1100552960755892,
      "grad_norm": 9.279414176940918,
      "learning_rate": 4.916456800964478e-06,
      "loss": 1.5224,
      "step": 2050
    },
    {
      "epoch": 0.11059215117839695,
      "grad_norm": 7.420502185821533,
      "learning_rate": 4.915338799120165e-06,
      "loss": 0.816,
      "step": 2060
    },
    {
      "epoch": 0.1111290062812047,
      "grad_norm": 17.925199508666992,
      "learning_rate": 4.914213495011854e-06,
      "loss": 1.1427,
      "step": 2070
    },
    {
      "epoch": 0.11166586138401245,
      "grad_norm": 7.620808124542236,
      "learning_rate": 4.913080892041661e-06,
      "loss": 1.0403,
      "step": 2080
    },
    {
      "epoch": 0.1122027164868202,
      "grad_norm": 12.792862892150879,
      "learning_rate": 4.911940993633772e-06,
      "loss": 1.3583,
      "step": 2090
    },
    {
      "epoch": 0.11273957158962795,
      "grad_norm": 17.190523147583008,
      "learning_rate": 4.9107938032344275e-06,
      "loss": 1.6445,
      "step": 2100
    },
    {
      "epoch": 0.1132764266924357,
      "grad_norm": 7.116855144500732,
      "learning_rate": 4.909639324311914e-06,
      "loss": 1.2134,
      "step": 2110
    },
    {
      "epoch": 0.11381328179524347,
      "grad_norm": 8.309609413146973,
      "learning_rate": 4.908477560356554e-06,
      "loss": 1.2422,
      "step": 2120
    },
    {
      "epoch": 0.11435013689805122,
      "grad_norm": 11.83594036102295,
      "learning_rate": 4.907308514880693e-06,
      "loss": 1.411,
      "step": 2130
    },
    {
      "epoch": 0.11488699200085897,
      "grad_norm": 10.184101104736328,
      "learning_rate": 4.9061321914186925e-06,
      "loss": 1.6207,
      "step": 2140
    },
    {
      "epoch": 0.11542384710366672,
      "grad_norm": 11.125953674316406,
      "learning_rate": 4.904948593526916e-06,
      "loss": 0.9211,
      "step": 2150
    },
    {
      "epoch": 0.11596070220647448,
      "grad_norm": 9.776846885681152,
      "learning_rate": 4.9037577247837205e-06,
      "loss": 0.9311,
      "step": 2160
    },
    {
      "epoch": 0.11649755730928223,
      "grad_norm": 11.215271949768066,
      "learning_rate": 4.902559588789446e-06,
      "loss": 0.7538,
      "step": 2170
    },
    {
      "epoch": 0.11703441241208998,
      "grad_norm": 10.415063858032227,
      "learning_rate": 4.901354189166402e-06,
      "loss": 1.3579,
      "step": 2180
    },
    {
      "epoch": 0.11757126751489773,
      "grad_norm": 11.879575729370117,
      "learning_rate": 4.900141529558857e-06,
      "loss": 1.1946,
      "step": 2190
    },
    {
      "epoch": 0.11810812261770548,
      "grad_norm": 8.953886032104492,
      "learning_rate": 4.898921613633031e-06,
      "loss": 0.9325,
      "step": 2200
    },
    {
      "epoch": 0.11864497772051323,
      "grad_norm": 8.201791763305664,
      "learning_rate": 4.897694445077081e-06,
      "loss": 1.3203,
      "step": 2210
    },
    {
      "epoch": 0.11918183282332098,
      "grad_norm": 7.558645248413086,
      "learning_rate": 4.896460027601089e-06,
      "loss": 0.9742,
      "step": 2220
    },
    {
      "epoch": 0.11971868792612873,
      "grad_norm": 7.915459632873535,
      "learning_rate": 4.895218364937056e-06,
      "loss": 0.93,
      "step": 2230
    },
    {
      "epoch": 0.12025554302893648,
      "grad_norm": 9.397565841674805,
      "learning_rate": 4.8939694608388835e-06,
      "loss": 1.2277,
      "step": 2240
    },
    {
      "epoch": 0.12079239813174424,
      "grad_norm": 15.724913597106934,
      "learning_rate": 4.892713319082369e-06,
      "loss": 1.3145,
      "step": 2250
    },
    {
      "epoch": 0.121329253234552,
      "grad_norm": 7.835499286651611,
      "learning_rate": 4.89144994346519e-06,
      "loss": 1.2269,
      "step": 2260
    },
    {
      "epoch": 0.12186610833735975,
      "grad_norm": 18.65610694885254,
      "learning_rate": 4.890179337806896e-06,
      "loss": 0.9395,
      "step": 2270
    },
    {
      "epoch": 0.1224029634401675,
      "grad_norm": 7.627039909362793,
      "learning_rate": 4.888901505948892e-06,
      "loss": 1.6445,
      "step": 2280
    },
    {
      "epoch": 0.12293981854297525,
      "grad_norm": 10.299637794494629,
      "learning_rate": 4.88761645175443e-06,
      "loss": 0.9958,
      "step": 2290
    },
    {
      "epoch": 0.123476673645783,
      "grad_norm": 11.802054405212402,
      "learning_rate": 4.886324179108601e-06,
      "loss": 1.3692,
      "step": 2300
    },
    {
      "epoch": 0.12401352874859076,
      "grad_norm": 7.465868949890137,
      "learning_rate": 4.8850246919183156e-06,
      "loss": 0.7659,
      "step": 2310
    },
    {
      "epoch": 0.12455038385139851,
      "grad_norm": 8.01265811920166,
      "learning_rate": 4.883717994112297e-06,
      "loss": 1.3204,
      "step": 2320
    },
    {
      "epoch": 0.12508723895420626,
      "grad_norm": 18.70553970336914,
      "learning_rate": 4.882404089641069e-06,
      "loss": 0.9778,
      "step": 2330
    },
    {
      "epoch": 0.12562409405701402,
      "grad_norm": 8.817856788635254,
      "learning_rate": 4.881082982476945e-06,
      "loss": 0.777,
      "step": 2340
    },
    {
      "epoch": 0.12616094915982176,
      "grad_norm": 8.14919376373291,
      "learning_rate": 4.879754676614008e-06,
      "loss": 0.9001,
      "step": 2350
    },
    {
      "epoch": 0.12669780426262953,
      "grad_norm": 6.689359188079834,
      "learning_rate": 4.878419176068111e-06,
      "loss": 1.1594,
      "step": 2360
    },
    {
      "epoch": 0.12723465936543726,
      "grad_norm": 7.795644283294678,
      "learning_rate": 4.877076484876856e-06,
      "loss": 1.3507,
      "step": 2370
    },
    {
      "epoch": 0.12777151446824503,
      "grad_norm": 18.106992721557617,
      "learning_rate": 4.875726607099586e-06,
      "loss": 1.3009,
      "step": 2380
    },
    {
      "epoch": 0.12830836957105277,
      "grad_norm": 7.038511753082275,
      "learning_rate": 4.874369546817367e-06,
      "loss": 1.2801,
      "step": 2390
    },
    {
      "epoch": 0.12884522467386053,
      "grad_norm": 8.319135665893555,
      "learning_rate": 4.873005308132985e-06,
      "loss": 0.9816,
      "step": 2400
    },
    {
      "epoch": 0.12938207977666827,
      "grad_norm": 7.838551998138428,
      "learning_rate": 4.871633895170924e-06,
      "loss": 0.7602,
      "step": 2410
    },
    {
      "epoch": 0.12991893487947603,
      "grad_norm": 8.532792091369629,
      "learning_rate": 4.870255312077362e-06,
      "loss": 1.3234,
      "step": 2420
    },
    {
      "epoch": 0.13045578998228377,
      "grad_norm": 7.035489082336426,
      "learning_rate": 4.868869563020152e-06,
      "loss": 1.3425,
      "step": 2430
    },
    {
      "epoch": 0.13099264508509154,
      "grad_norm": 10.039881706237793,
      "learning_rate": 4.86747665218881e-06,
      "loss": 1.1401,
      "step": 2440
    },
    {
      "epoch": 0.13152950018789927,
      "grad_norm": 15.929950714111328,
      "learning_rate": 4.866076583794508e-06,
      "loss": 1.527,
      "step": 2450
    },
    {
      "epoch": 0.13206635529070704,
      "grad_norm": 8.255677223205566,
      "learning_rate": 4.864669362070055e-06,
      "loss": 1.3244,
      "step": 2460
    },
    {
      "epoch": 0.1326032103935148,
      "grad_norm": 7.576117038726807,
      "learning_rate": 4.863254991269888e-06,
      "loss": 1.1138,
      "step": 2470
    },
    {
      "epoch": 0.13314006549632254,
      "grad_norm": 8.603239059448242,
      "learning_rate": 4.861833475670055e-06,
      "loss": 0.9075,
      "step": 2480
    },
    {
      "epoch": 0.1336769205991303,
      "grad_norm": 7.866832733154297,
      "learning_rate": 4.8604048195682065e-06,
      "loss": 1.1651,
      "step": 2490
    },
    {
      "epoch": 0.13421377570193804,
      "grad_norm": 15.445428848266602,
      "learning_rate": 4.858969027283583e-06,
      "loss": 1.3481,
      "step": 2500
    },
    {
      "epoch": 0.1347506308047458,
      "grad_norm": 8.422149658203125,
      "learning_rate": 4.857526103156997e-06,
      "loss": 1.2102,
      "step": 2510
    },
    {
      "epoch": 0.13528748590755355,
      "grad_norm": 8.69704532623291,
      "learning_rate": 4.856076051550821e-06,
      "loss": 1.4626,
      "step": 2520
    },
    {
      "epoch": 0.1358243410103613,
      "grad_norm": 13.394875526428223,
      "learning_rate": 4.854618876848981e-06,
      "loss": 1.2638,
      "step": 2530
    },
    {
      "epoch": 0.13636119611316905,
      "grad_norm": 10.491499900817871,
      "learning_rate": 4.853154583456933e-06,
      "loss": 0.851,
      "step": 2540
    },
    {
      "epoch": 0.1368980512159768,
      "grad_norm": 10.67133903503418,
      "learning_rate": 4.851683175801659e-06,
      "loss": 1.174,
      "step": 2550
    },
    {
      "epoch": 0.13743490631878455,
      "grad_norm": 15.081131935119629,
      "learning_rate": 4.850204658331647e-06,
      "loss": 1.1255,
      "step": 2560
    },
    {
      "epoch": 0.13797176142159231,
      "grad_norm": 9.563088417053223,
      "learning_rate": 4.84871903551688e-06,
      "loss": 1.0273,
      "step": 2570
    },
    {
      "epoch": 0.13850861652440005,
      "grad_norm": 8.483844757080078,
      "learning_rate": 4.847226311848824e-06,
      "loss": 0.731,
      "step": 2580
    },
    {
      "epoch": 0.13904547162720782,
      "grad_norm": 17.15674591064453,
      "learning_rate": 4.845726491840411e-06,
      "loss": 1.4118,
      "step": 2590
    },
    {
      "epoch": 0.13958232673001558,
      "grad_norm": 16.473581314086914,
      "learning_rate": 4.84421958002603e-06,
      "loss": 1.3442,
      "step": 2600
    },
    {
      "epoch": 0.14011918183282332,
      "grad_norm": 9.595731735229492,
      "learning_rate": 4.842705580961507e-06,
      "loss": 0.7458,
      "step": 2610
    },
    {
      "epoch": 0.14065603693563108,
      "grad_norm": 10.634049415588379,
      "learning_rate": 4.841184499224098e-06,
      "loss": 1.0499,
      "step": 2620
    },
    {
      "epoch": 0.14119289203843882,
      "grad_norm": 14.892288208007812,
      "learning_rate": 4.83965633941247e-06,
      "loss": 1.4751,
      "step": 2630
    },
    {
      "epoch": 0.1417297471412466,
      "grad_norm": 14.603962898254395,
      "learning_rate": 4.8381211061466904e-06,
      "loss": 1.2603,
      "step": 2640
    },
    {
      "epoch": 0.14226660224405432,
      "grad_norm": 12.587336540222168,
      "learning_rate": 4.83657880406821e-06,
      "loss": 1.1345,
      "step": 2650
    },
    {
      "epoch": 0.1428034573468621,
      "grad_norm": 12.996410369873047,
      "learning_rate": 4.835029437839852e-06,
      "loss": 0.9015,
      "step": 2660
    },
    {
      "epoch": 0.14334031244966983,
      "grad_norm": 8.754450798034668,
      "learning_rate": 4.8334730121457976e-06,
      "loss": 0.9886,
      "step": 2670
    },
    {
      "epoch": 0.1438771675524776,
      "grad_norm": 8.651375770568848,
      "learning_rate": 4.831909531691566e-06,
      "loss": 0.9807,
      "step": 2680
    },
    {
      "epoch": 0.14441402265528533,
      "grad_norm": 7.417424201965332,
      "learning_rate": 4.830339001204013e-06,
      "loss": 1.4295,
      "step": 2690
    },
    {
      "epoch": 0.1449508777580931,
      "grad_norm": 14.969225883483887,
      "learning_rate": 4.8287614254313e-06,
      "loss": 1.0203,
      "step": 2700
    },
    {
      "epoch": 0.14548773286090083,
      "grad_norm": 9.608999252319336,
      "learning_rate": 4.827176809142895e-06,
      "loss": 1.0072,
      "step": 2710
    },
    {
      "epoch": 0.1460245879637086,
      "grad_norm": 15.650749206542969,
      "learning_rate": 4.825585157129547e-06,
      "loss": 1.0366,
      "step": 2720
    },
    {
      "epoch": 0.14656144306651633,
      "grad_norm": 7.252901554107666,
      "learning_rate": 4.823986474203279e-06,
      "loss": 1.2627,
      "step": 2730
    },
    {
      "epoch": 0.1470982981693241,
      "grad_norm": 7.711190223693848,
      "learning_rate": 4.82238076519737e-06,
      "loss": 0.7549,
      "step": 2740
    },
    {
      "epoch": 0.14763515327213186,
      "grad_norm": 7.226834297180176,
      "learning_rate": 4.820768034966339e-06,
      "loss": 1.02,
      "step": 2750
    },
    {
      "epoch": 0.1481720083749396,
      "grad_norm": 9.336762428283691,
      "learning_rate": 4.819148288385934e-06,
      "loss": 0.7146,
      "step": 2760
    },
    {
      "epoch": 0.14870886347774737,
      "grad_norm": 8.765752792358398,
      "learning_rate": 4.817521530353114e-06,
      "loss": 1.4538,
      "step": 2770
    },
    {
      "epoch": 0.1492457185805551,
      "grad_norm": 6.761751651763916,
      "learning_rate": 4.815887765786037e-06,
      "loss": 1.1009,
      "step": 2780
    },
    {
      "epoch": 0.14978257368336287,
      "grad_norm": 11.577098846435547,
      "learning_rate": 4.814246999624043e-06,
      "loss": 0.9509,
      "step": 2790
    },
    {
      "epoch": 0.1503194287861706,
      "grad_norm": 7.294278621673584,
      "learning_rate": 4.812599236827642e-06,
      "loss": 1.3031,
      "step": 2800
    },
    {
      "epoch": 0.15085628388897837,
      "grad_norm": 16.320560455322266,
      "learning_rate": 4.810944482378493e-06,
      "loss": 1.3366,
      "step": 2810
    },
    {
      "epoch": 0.1513931389917861,
      "grad_norm": 15.34796142578125,
      "learning_rate": 4.809282741279395e-06,
      "loss": 1.1496,
      "step": 2820
    },
    {
      "epoch": 0.15192999409459387,
      "grad_norm": 16.42742347717285,
      "learning_rate": 4.80761401855427e-06,
      "loss": 1.3646,
      "step": 2830
    },
    {
      "epoch": 0.1524668491974016,
      "grad_norm": 7.500369071960449,
      "learning_rate": 4.805938319248145e-06,
      "loss": 0.9573,
      "step": 2840
    },
    {
      "epoch": 0.15300370430020938,
      "grad_norm": 8.783166885375977,
      "learning_rate": 4.804255648427143e-06,
      "loss": 0.7209,
      "step": 2850
    },
    {
      "epoch": 0.1535405594030171,
      "grad_norm": 6.690563678741455,
      "learning_rate": 4.802566011178462e-06,
      "loss": 1.2434,
      "step": 2860
    },
    {
      "epoch": 0.15407741450582488,
      "grad_norm": 10.404061317443848,
      "learning_rate": 4.8008694126103606e-06,
      "loss": 1.1704,
      "step": 2870
    },
    {
      "epoch": 0.15461426960863264,
      "grad_norm": 12.526078224182129,
      "learning_rate": 4.799165857852144e-06,
      "loss": 1.0608,
      "step": 2880
    },
    {
      "epoch": 0.15515112471144038,
      "grad_norm": 10.033717155456543,
      "learning_rate": 4.797455352054149e-06,
      "loss": 1.1065,
      "step": 2890
    },
    {
      "epoch": 0.15568797981424815,
      "grad_norm": 10.32773494720459,
      "learning_rate": 4.795737900387727e-06,
      "loss": 1.0353,
      "step": 2900
    },
    {
      "epoch": 0.15622483491705588,
      "grad_norm": 16.10841178894043,
      "learning_rate": 4.794013508045228e-06,
      "loss": 0.9874,
      "step": 2910
    },
    {
      "epoch": 0.15676169001986365,
      "grad_norm": 10.619322776794434,
      "learning_rate": 4.792282180239985e-06,
      "loss": 1.3791,
      "step": 2920
    },
    {
      "epoch": 0.15729854512267138,
      "grad_norm": 9.037995338439941,
      "learning_rate": 4.790543922206302e-06,
      "loss": 1.215,
      "step": 2930
    },
    {
      "epoch": 0.15783540022547915,
      "grad_norm": 11.363986015319824,
      "learning_rate": 4.788798739199431e-06,
      "loss": 1.0512,
      "step": 2940
    },
    {
      "epoch": 0.1583722553282869,
      "grad_norm": 14.480243682861328,
      "learning_rate": 4.7870466364955645e-06,
      "loss": 1.0164,
      "step": 2950
    },
    {
      "epoch": 0.15890911043109465,
      "grad_norm": 7.173501014709473,
      "learning_rate": 4.785287619391811e-06,
      "loss": 1.5059,
      "step": 2960
    },
    {
      "epoch": 0.1594459655339024,
      "grad_norm": 13.592036247253418,
      "learning_rate": 4.783521693206187e-06,
      "loss": 1.1783,
      "step": 2970
    },
    {
      "epoch": 0.15998282063671015,
      "grad_norm": 7.588423728942871,
      "learning_rate": 4.781748863277593e-06,
      "loss": 1.1444,
      "step": 2980
    },
    {
      "epoch": 0.1605196757395179,
      "grad_norm": 10.718658447265625,
      "learning_rate": 4.7799691349658065e-06,
      "loss": 1.1792,
      "step": 2990
    },
    {
      "epoch": 0.16105653084232566,
      "grad_norm": 16.28377914428711,
      "learning_rate": 4.778182513651456e-06,
      "loss": 1.0944,
      "step": 3000
    },
    {
      "epoch": 0.16159338594513342,
      "grad_norm": 14.51433277130127,
      "learning_rate": 4.776389004736014e-06,
      "loss": 0.9336,
      "step": 3010
    },
    {
      "epoch": 0.16213024104794116,
      "grad_norm": 16.199079513549805,
      "learning_rate": 4.7745886136417715e-06,
      "loss": 1.1706,
      "step": 3020
    },
    {
      "epoch": 0.16266709615074892,
      "grad_norm": 8.45963191986084,
      "learning_rate": 4.77278134581183e-06,
      "loss": 0.9536,
      "step": 3030
    },
    {
      "epoch": 0.16320395125355666,
      "grad_norm": 6.116096496582031,
      "learning_rate": 4.770967206710079e-06,
      "loss": 1.3605,
      "step": 3040
    },
    {
      "epoch": 0.16374080635636443,
      "grad_norm": 7.275299549102783,
      "learning_rate": 4.769146201821184e-06,
      "loss": 1.5195,
      "step": 3050
    },
    {
      "epoch": 0.16427766145917216,
      "grad_norm": 7.1150102615356445,
      "learning_rate": 4.767318336650567e-06,
      "loss": 1.3784,
      "step": 3060
    },
    {
      "epoch": 0.16481451656197993,
      "grad_norm": 15.433606147766113,
      "learning_rate": 4.765483616724389e-06,
      "loss": 0.7422,
      "step": 3070
    },
    {
      "epoch": 0.16535137166478767,
      "grad_norm": 9.253497123718262,
      "learning_rate": 4.763642047589536e-06,
      "loss": 1.3474,
      "step": 3080
    },
    {
      "epoch": 0.16588822676759543,
      "grad_norm": 12.682035446166992,
      "learning_rate": 4.761793634813602e-06,
      "loss": 1.0052,
      "step": 3090
    },
    {
      "epoch": 0.16642508187040317,
      "grad_norm": 6.347700595855713,
      "learning_rate": 4.7599383839848706e-06,
      "loss": 1.2812,
      "step": 3100
    },
    {
      "epoch": 0.16696193697321093,
      "grad_norm": 14.941232681274414,
      "learning_rate": 4.758076300712299e-06,
      "loss": 1.646,
      "step": 3110
    },
    {
      "epoch": 0.16749879207601867,
      "grad_norm": 15.418540954589844,
      "learning_rate": 4.756207390625499e-06,
      "loss": 1.7149,
      "step": 3120
    },
    {
      "epoch": 0.16803564717882644,
      "grad_norm": 6.847586154937744,
      "learning_rate": 4.754331659374725e-06,
      "loss": 1.3781,
      "step": 3130
    },
    {
      "epoch": 0.16857250228163417,
      "grad_norm": 17.684858322143555,
      "learning_rate": 4.752449112630851e-06,
      "loss": 1.4679,
      "step": 3140
    },
    {
      "epoch": 0.16910935738444194,
      "grad_norm": 17.267284393310547,
      "learning_rate": 4.750559756085359e-06,
      "loss": 0.927,
      "step": 3150
    },
    {
      "epoch": 0.1696462124872497,
      "grad_norm": 16.058073043823242,
      "learning_rate": 4.748663595450316e-06,
      "loss": 0.889,
      "step": 3160
    },
    {
      "epoch": 0.17018306759005744,
      "grad_norm": 5.927127838134766,
      "learning_rate": 4.746760636458362e-06,
      "loss": 0.9169,
      "step": 3170
    },
    {
      "epoch": 0.1707199226928652,
      "grad_norm": 9.675300598144531,
      "learning_rate": 4.744850884862688e-06,
      "loss": 0.8619,
      "step": 3180
    },
    {
      "epoch": 0.17125677779567294,
      "grad_norm": 11.003336906433105,
      "learning_rate": 4.742934346437024e-06,
      "loss": 1.2914,
      "step": 3190
    },
    {
      "epoch": 0.1717936328984807,
      "grad_norm": 7.526270389556885,
      "learning_rate": 4.741011026975615e-06,
      "loss": 1.1146,
      "step": 3200
    },
    {
      "epoch": 0.17233048800128845,
      "grad_norm": 7.820011615753174,
      "learning_rate": 4.739080932293211e-06,
      "loss": 1.324,
      "step": 3210
    },
    {
      "epoch": 0.1728673431040962,
      "grad_norm": 9.366170883178711,
      "learning_rate": 4.737144068225043e-06,
      "loss": 0.7544,
      "step": 3220
    },
    {
      "epoch": 0.17340419820690395,
      "grad_norm": 16.416717529296875,
      "learning_rate": 4.735200440626808e-06,
      "loss": 1.5546,
      "step": 3230
    },
    {
      "epoch": 0.1739410533097117,
      "grad_norm": 15.016637802124023,
      "learning_rate": 4.733250055374651e-06,
      "loss": 1.4925,
      "step": 3240
    },
    {
      "epoch": 0.17447790841251945,
      "grad_norm": 22.81689453125,
      "learning_rate": 4.731292918365148e-06,
      "loss": 1.5428,
      "step": 3250
    },
    {
      "epoch": 0.17501476351532722,
      "grad_norm": 16.02472496032715,
      "learning_rate": 4.729329035515287e-06,
      "loss": 1.6626,
      "step": 3260
    },
    {
      "epoch": 0.17555161861813495,
      "grad_norm": 7.230112075805664,
      "learning_rate": 4.727358412762452e-06,
      "loss": 1.442,
      "step": 3270
    },
    {
      "epoch": 0.17608847372094272,
      "grad_norm": 15.36326789855957,
      "learning_rate": 4.7253810560643995e-06,
      "loss": 1.3902,
      "step": 3280
    },
    {
      "epoch": 0.17662532882375048,
      "grad_norm": 6.477517604827881,
      "learning_rate": 4.723396971399251e-06,
      "loss": 1.2639,
      "step": 3290
    },
    {
      "epoch": 0.17716218392655822,
      "grad_norm": 10.079001426696777,
      "learning_rate": 4.721406164765464e-06,
      "loss": 1.0775,
      "step": 3300
    },
    {
      "epoch": 0.17769903902936598,
      "grad_norm": 6.46017599105835,
      "learning_rate": 4.719408642181819e-06,
      "loss": 0.7374,
      "step": 3310
    },
    {
      "epoch": 0.17823589413217372,
      "grad_norm": 7.657912731170654,
      "learning_rate": 4.717404409687401e-06,
      "loss": 1.0972,
      "step": 3320
    },
    {
      "epoch": 0.1787727492349815,
      "grad_norm": 6.374989986419678,
      "learning_rate": 4.715393473341583e-06,
      "loss": 0.9075,
      "step": 3330
    },
    {
      "epoch": 0.17930960433778922,
      "grad_norm": 16.391956329345703,
      "learning_rate": 4.713375839224003e-06,
      "loss": 1.1557,
      "step": 3340
    },
    {
      "epoch": 0.179846459440597,
      "grad_norm": 10.927242279052734,
      "learning_rate": 4.711351513434549e-06,
      "loss": 1.3464,
      "step": 3350
    },
    {
      "epoch": 0.18038331454340473,
      "grad_norm": 22.445711135864258,
      "learning_rate": 4.7093205020933405e-06,
      "loss": 1.2944,
      "step": 3360
    },
    {
      "epoch": 0.1809201696462125,
      "grad_norm": 7.17965030670166,
      "learning_rate": 4.707282811340711e-06,
      "loss": 0.9778,
      "step": 3370
    },
    {
      "epoch": 0.18145702474902023,
      "grad_norm": 5.98141622543335,
      "learning_rate": 4.705238447337182e-06,
      "loss": 1.164,
      "step": 3380
    },
    {
      "epoch": 0.181993879851828,
      "grad_norm": 8.436300277709961,
      "learning_rate": 4.703187416263458e-06,
      "loss": 1.3916,
      "step": 3390
    },
    {
      "epoch": 0.18253073495463573,
      "grad_norm": 7.561093330383301,
      "learning_rate": 4.701129724320393e-06,
      "loss": 1.7676,
      "step": 3400
    },
    {
      "epoch": 0.1830675900574435,
      "grad_norm": 8.606691360473633,
      "learning_rate": 4.699065377728983e-06,
      "loss": 1.1369,
      "step": 3410
    },
    {
      "epoch": 0.18360444516025126,
      "grad_norm": 8.222113609313965,
      "learning_rate": 4.696994382730341e-06,
      "loss": 1.1455,
      "step": 3420
    },
    {
      "epoch": 0.184141300263059,
      "grad_norm": 17.85607147216797,
      "learning_rate": 4.694916745585681e-06,
      "loss": 0.9619,
      "step": 3430
    },
    {
      "epoch": 0.18467815536586676,
      "grad_norm": 9.32919692993164,
      "learning_rate": 4.692832472576298e-06,
      "loss": 1.3925,
      "step": 3440
    },
    {
      "epoch": 0.1852150104686745,
      "grad_norm": 14.702792167663574,
      "learning_rate": 4.690741570003548e-06,
      "loss": 1.5476,
      "step": 3450
    },
    {
      "epoch": 0.18575186557148227,
      "grad_norm": 7.033271312713623,
      "learning_rate": 4.68864404418883e-06,
      "loss": 1.3116,
      "step": 3460
    },
    {
      "epoch": 0.18628872067429,
      "grad_norm": 14.276156425476074,
      "learning_rate": 4.686539901473572e-06,
      "loss": 1.1645,
      "step": 3470
    },
    {
      "epoch": 0.18682557577709777,
      "grad_norm": 9.640213012695312,
      "learning_rate": 4.684429148219199e-06,
      "loss": 0.7534,
      "step": 3480
    },
    {
      "epoch": 0.1873624308799055,
      "grad_norm": 15.213176727294922,
      "learning_rate": 4.6823117908071265e-06,
      "loss": 1.4295,
      "step": 3490
    },
    {
      "epoch": 0.18789928598271327,
      "grad_norm": 8.654376983642578,
      "learning_rate": 4.6801878356387345e-06,
      "loss": 1.1231,
      "step": 3500
    },
    {
      "epoch": 0.188436141085521,
      "grad_norm": 10.635987281799316,
      "learning_rate": 4.678057289135351e-06,
      "loss": 1.0757,
      "step": 3510
    },
    {
      "epoch": 0.18897299618832877,
      "grad_norm": 9.857612609863281,
      "learning_rate": 4.675920157738232e-06,
      "loss": 1.0959,
      "step": 3520
    },
    {
      "epoch": 0.1895098512911365,
      "grad_norm": 8.886661529541016,
      "learning_rate": 4.673776447908538e-06,
      "loss": 1.468,
      "step": 3530
    },
    {
      "epoch": 0.19004670639394428,
      "grad_norm": 16.391977310180664,
      "learning_rate": 4.671626166127323e-06,
      "loss": 0.9711,
      "step": 3540
    },
    {
      "epoch": 0.190583561496752,
      "grad_norm": 7.238748550415039,
      "learning_rate": 4.669469318895505e-06,
      "loss": 1.0924,
      "step": 3550
    },
    {
      "epoch": 0.19112041659955978,
      "grad_norm": 15.763091087341309,
      "learning_rate": 4.667305912733856e-06,
      "loss": 1.6906,
      "step": 3560
    },
    {
      "epoch": 0.19165727170236754,
      "grad_norm": 14.378523826599121,
      "learning_rate": 4.665135954182974e-06,
      "loss": 1.3764,
      "step": 3570
    },
    {
      "epoch": 0.19219412680517528,
      "grad_norm": 8.820269584655762,
      "learning_rate": 4.6629594498032674e-06,
      "loss": 1.5632,
      "step": 3580
    },
    {
      "epoch": 0.19273098190798305,
      "grad_norm": 16.15110206604004,
      "learning_rate": 4.660776406174936e-06,
      "loss": 1.0799,
      "step": 3590
    },
    {
      "epoch": 0.19326783701079078,
      "grad_norm": 17.78586196899414,
      "learning_rate": 4.658586829897947e-06,
      "loss": 1.3501,
      "step": 3600
    },
    {
      "epoch": 0.19380469211359855,
      "grad_norm": 6.3675079345703125,
      "learning_rate": 4.656390727592023e-06,
      "loss": 1.591,
      "step": 3610
    },
    {
      "epoch": 0.19434154721640629,
      "grad_norm": 8.369041442871094,
      "learning_rate": 4.654188105896609e-06,
      "loss": 1.1597,
      "step": 3620
    },
    {
      "epoch": 0.19487840231921405,
      "grad_norm": 8.07697868347168,
      "learning_rate": 4.651978971470865e-06,
      "loss": 1.2871,
      "step": 3630
    },
    {
      "epoch": 0.1954152574220218,
      "grad_norm": 6.705743789672852,
      "learning_rate": 4.649763330993642e-06,
      "loss": 0.715,
      "step": 3640
    },
    {
      "epoch": 0.19595211252482955,
      "grad_norm": 9.2643404006958,
      "learning_rate": 4.647541191163457e-06,
      "loss": 1.141,
      "step": 3650
    },
    {
      "epoch": 0.1964889676276373,
      "grad_norm": 7.585899829864502,
      "learning_rate": 4.645312558698477e-06,
      "loss": 0.9469,
      "step": 3660
    },
    {
      "epoch": 0.19702582273044505,
      "grad_norm": 8.355679512023926,
      "learning_rate": 4.643077440336501e-06,
      "loss": 0.6867,
      "step": 3670
    },
    {
      "epoch": 0.1975626778332528,
      "grad_norm": 7.4701738357543945,
      "learning_rate": 4.640835842834933e-06,
      "loss": 0.8878,
      "step": 3680
    },
    {
      "epoch": 0.19809953293606056,
      "grad_norm": 7.792985439300537,
      "learning_rate": 4.638587772970768e-06,
      "loss": 0.87,
      "step": 3690
    },
    {
      "epoch": 0.19863638803886832,
      "grad_norm": 8.34260368347168,
      "learning_rate": 4.636333237540568e-06,
      "loss": 0.8852,
      "step": 3700
    },
    {
      "epoch": 0.19917324314167606,
      "grad_norm": 10.951787948608398,
      "learning_rate": 4.634072243360442e-06,
      "loss": 0.9154,
      "step": 3710
    },
    {
      "epoch": 0.19971009824448382,
      "grad_norm": 16.14868927001953,
      "learning_rate": 4.631804797266025e-06,
      "loss": 1.4312,
      "step": 3720
    },
    {
      "epoch": 0.20024695334729156,
      "grad_norm": 14.808999061584473,
      "learning_rate": 4.62953090611246e-06,
      "loss": 1.4565,
      "step": 3730
    },
    {
      "epoch": 0.20078380845009933,
      "grad_norm": 6.217281341552734,
      "learning_rate": 4.6272505767743745e-06,
      "loss": 1.1886,
      "step": 3740
    },
    {
      "epoch": 0.20132066355290706,
      "grad_norm": 6.214573860168457,
      "learning_rate": 4.624963816145858e-06,
      "loss": 1.0689,
      "step": 3750
    },
    {
      "epoch": 0.20185751865571483,
      "grad_norm": 13.440322875976562,
      "learning_rate": 4.622670631140447e-06,
      "loss": 0.927,
      "step": 3760
    },
    {
      "epoch": 0.20239437375852257,
      "grad_norm": 6.943330764770508,
      "learning_rate": 4.6203710286910995e-06,
      "loss": 1.2594,
      "step": 3770
    },
    {
      "epoch": 0.20293122886133033,
      "grad_norm": 6.708223342895508,
      "learning_rate": 4.618065015750175e-06,
      "loss": 0.8903,
      "step": 3780
    },
    {
      "epoch": 0.20346808396413807,
      "grad_norm": 11.089399337768555,
      "learning_rate": 4.615752599289415e-06,
      "loss": 0.9476,
      "step": 3790
    },
    {
      "epoch": 0.20400493906694583,
      "grad_norm": 8.430839538574219,
      "learning_rate": 4.6134337862999175e-06,
      "loss": 1.1134,
      "step": 3800
    },
    {
      "epoch": 0.20454179416975357,
      "grad_norm": 16.15500831604004,
      "learning_rate": 4.6111085837921224e-06,
      "loss": 1.2967,
      "step": 3810
    },
    {
      "epoch": 0.20507864927256134,
      "grad_norm": 7.458437442779541,
      "learning_rate": 4.608776998795786e-06,
      "loss": 1.0297,
      "step": 3820
    },
    {
      "epoch": 0.2056155043753691,
      "grad_norm": 7.544371604919434,
      "learning_rate": 4.60643903835996e-06,
      "loss": 1.2447,
      "step": 3830
    },
    {
      "epoch": 0.20615235947817684,
      "grad_norm": 6.732742786407471,
      "learning_rate": 4.604094709552972e-06,
      "loss": 0.9469,
      "step": 3840
    },
    {
      "epoch": 0.2066892145809846,
      "grad_norm": 8.69398021697998,
      "learning_rate": 4.601744019462402e-06,
      "loss": 1.0445,
      "step": 3850
    },
    {
      "epoch": 0.20722606968379234,
      "grad_norm": 15.88547420501709,
      "learning_rate": 4.599386975195062e-06,
      "loss": 0.8682,
      "step": 3860
    },
    {
      "epoch": 0.2077629247866001,
      "grad_norm": 6.793948173522949,
      "learning_rate": 4.597023583876975e-06,
      "loss": 1.1128,
      "step": 3870
    },
    {
      "epoch": 0.20829977988940784,
      "grad_norm": 10.572693824768066,
      "learning_rate": 4.594653852653354e-06,
      "loss": 0.8921,
      "step": 3880
    },
    {
      "epoch": 0.2088366349922156,
      "grad_norm": 9.42236614227295,
      "learning_rate": 4.592277788688575e-06,
      "loss": 1.3462,
      "step": 3890
    },
    {
      "epoch": 0.20937349009502335,
      "grad_norm": 7.901852607727051,
      "learning_rate": 4.5898953991661665e-06,
      "loss": 0.8884,
      "step": 3900
    },
    {
      "epoch": 0.2099103451978311,
      "grad_norm": 10.362621307373047,
      "learning_rate": 4.587506691288776e-06,
      "loss": 1.3021,
      "step": 3910
    },
    {
      "epoch": 0.21044720030063885,
      "grad_norm": 9.954042434692383,
      "learning_rate": 4.585111672278154e-06,
      "loss": 1.123,
      "step": 3920
    },
    {
      "epoch": 0.2109840554034466,
      "grad_norm": 10.41720199584961,
      "learning_rate": 4.582710349375133e-06,
      "loss": 1.5889,
      "step": 3930
    },
    {
      "epoch": 0.21152091050625435,
      "grad_norm": 14.810001373291016,
      "learning_rate": 4.580302729839601e-06,
      "loss": 1.2988,
      "step": 3940
    },
    {
      "epoch": 0.21205776560906212,
      "grad_norm": 7.708392143249512,
      "learning_rate": 4.5778888209504865e-06,
      "loss": 0.7598,
      "step": 3950
    },
    {
      "epoch": 0.21259462071186985,
      "grad_norm": 6.894002914428711,
      "learning_rate": 4.57546863000573e-06,
      "loss": 0.7634,
      "step": 3960
    },
    {
      "epoch": 0.21313147581467762,
      "grad_norm": 10.723433494567871,
      "learning_rate": 4.573042164322264e-06,
      "loss": 0.8674,
      "step": 3970
    },
    {
      "epoch": 0.21366833091748538,
      "grad_norm": 7.566904544830322,
      "learning_rate": 4.570609431235993e-06,
      "loss": 0.851,
      "step": 3980
    },
    {
      "epoch": 0.21420518602029312,
      "grad_norm": 16.29374122619629,
      "learning_rate": 4.568170438101769e-06,
      "loss": 0.9303,
      "step": 3990
    },
    {
      "epoch": 0.21474204112310089,
      "grad_norm": 7.390998840332031,
      "learning_rate": 4.56572519229337e-06,
      "loss": 0.9644,
      "step": 4000
    },
    {
      "epoch": 0.21527889622590862,
      "grad_norm": 13.77817440032959,
      "learning_rate": 4.563273701203477e-06,
      "loss": 1.0735,
      "step": 4010
    },
    {
      "epoch": 0.2158157513287164,
      "grad_norm": 14.56369686126709,
      "learning_rate": 4.560815972243653e-06,
      "loss": 1.8325,
      "step": 4020
    },
    {
      "epoch": 0.21635260643152412,
      "grad_norm": 11.950592994689941,
      "learning_rate": 4.5583520128443185e-06,
      "loss": 0.6602,
      "step": 4030
    },
    {
      "epoch": 0.2168894615343319,
      "grad_norm": 13.711971282958984,
      "learning_rate": 4.555881830454733e-06,
      "loss": 1.4431,
      "step": 4040
    },
    {
      "epoch": 0.21742631663713963,
      "grad_norm": 6.953356742858887,
      "learning_rate": 4.55340543254297e-06,
      "loss": 1.2922,
      "step": 4050
    },
    {
      "epoch": 0.2179631717399474,
      "grad_norm": 6.404519557952881,
      "learning_rate": 4.55092282659589e-06,
      "loss": 1.2742,
      "step": 4060
    },
    {
      "epoch": 0.21850002684275513,
      "grad_norm": 6.642707824707031,
      "learning_rate": 4.548434020119125e-06,
      "loss": 0.6859,
      "step": 4070
    },
    {
      "epoch": 0.2190368819455629,
      "grad_norm": 8.551284790039062,
      "learning_rate": 4.545939020637053e-06,
      "loss": 1.2755,
      "step": 4080
    },
    {
      "epoch": 0.21957373704837063,
      "grad_norm": 14.737210273742676,
      "learning_rate": 4.543437835692778e-06,
      "loss": 1.1543,
      "step": 4090
    },
    {
      "epoch": 0.2201105921511784,
      "grad_norm": 7.733486652374268,
      "learning_rate": 4.540930472848098e-06,
      "loss": 0.9638,
      "step": 4100
    },
    {
      "epoch": 0.22064744725398616,
      "grad_norm": 7.7226152420043945,
      "learning_rate": 4.538416939683494e-06,
      "loss": 1.2959,
      "step": 4110
    },
    {
      "epoch": 0.2211843023567939,
      "grad_norm": 14.974197387695312,
      "learning_rate": 4.535897243798099e-06,
      "loss": 1.3088,
      "step": 4120
    },
    {
      "epoch": 0.22172115745960166,
      "grad_norm": 15.22043228149414,
      "learning_rate": 4.5333713928096806e-06,
      "loss": 1.2555,
      "step": 4130
    },
    {
      "epoch": 0.2222580125624094,
      "grad_norm": 8.928786277770996,
      "learning_rate": 4.530839394354611e-06,
      "loss": 1.1724,
      "step": 4140
    },
    {
      "epoch": 0.22279486766521717,
      "grad_norm": 7.477701187133789,
      "learning_rate": 4.528301256087849e-06,
      "loss": 0.7379,
      "step": 4150
    },
    {
      "epoch": 0.2233317227680249,
      "grad_norm": 5.9267473220825195,
      "learning_rate": 4.525756985682918e-06,
      "loss": 0.6724,
      "step": 4160
    },
    {
      "epoch": 0.22386857787083267,
      "grad_norm": 8.869007110595703,
      "learning_rate": 4.523206590831879e-06,
      "loss": 0.902,
      "step": 4170
    },
    {
      "epoch": 0.2244054329736404,
      "grad_norm": 15.672279357910156,
      "learning_rate": 4.52065007924531e-06,
      "loss": 1.2025,
      "step": 4180
    },
    {
      "epoch": 0.22494228807644817,
      "grad_norm": 10.466862678527832,
      "learning_rate": 4.5180874586522815e-06,
      "loss": 1.3093,
      "step": 4190
    },
    {
      "epoch": 0.2254791431792559,
      "grad_norm": 16.26676368713379,
      "learning_rate": 4.51551873680033e-06,
      "loss": 1.0487,
      "step": 4200
    },
    {
      "epoch": 0.22601599828206367,
      "grad_norm": 8.973531723022461,
      "learning_rate": 4.512943921455443e-06,
      "loss": 1.1198,
      "step": 4210
    },
    {
      "epoch": 0.2265528533848714,
      "grad_norm": 7.409702777862549,
      "learning_rate": 4.510363020402027e-06,
      "loss": 1.5771,
      "step": 4220
    },
    {
      "epoch": 0.22708970848767918,
      "grad_norm": 7.401615619659424,
      "learning_rate": 4.50777604144289e-06,
      "loss": 1.0369,
      "step": 4230
    },
    {
      "epoch": 0.22762656359048694,
      "grad_norm": 13.788765907287598,
      "learning_rate": 4.505182992399211e-06,
      "loss": 1.1304,
      "step": 4240
    },
    {
      "epoch": 0.22816341869329468,
      "grad_norm": 5.465855121612549,
      "learning_rate": 4.502583881110524e-06,
      "loss": 0.9463,
      "step": 4250
    },
    {
      "epoch": 0.22870027379610244,
      "grad_norm": 6.6857428550720215,
      "learning_rate": 4.499978715434691e-06,
      "loss": 1.4107,
      "step": 4260
    },
    {
      "epoch": 0.22923712889891018,
      "grad_norm": 16.070804595947266,
      "learning_rate": 4.4973675032478764e-06,
      "loss": 1.2539,
      "step": 4270
    },
    {
      "epoch": 0.22977398400171795,
      "grad_norm": 8.641080856323242,
      "learning_rate": 4.494750252444526e-06,
      "loss": 0.9522,
      "step": 4280
    },
    {
      "epoch": 0.23031083910452568,
      "grad_norm": 5.626453399658203,
      "learning_rate": 4.492126970937343e-06,
      "loss": 0.9396,
      "step": 4290
    },
    {
      "epoch": 0.23084769420733345,
      "grad_norm": 7.115457534790039,
      "learning_rate": 4.48949766665726e-06,
      "loss": 1.2279,
      "step": 4300
    },
    {
      "epoch": 0.23138454931014119,
      "grad_norm": 7.935233116149902,
      "learning_rate": 4.486862347553421e-06,
      "loss": 0.9312,
      "step": 4310
    },
    {
      "epoch": 0.23192140441294895,
      "grad_norm": 9.546976089477539,
      "learning_rate": 4.484221021593154e-06,
      "loss": 0.81,
      "step": 4320
    },
    {
      "epoch": 0.2324582595157567,
      "grad_norm": 16.181175231933594,
      "learning_rate": 4.4815736967619475e-06,
      "loss": 1.2958,
      "step": 4330
    },
    {
      "epoch": 0.23299511461856445,
      "grad_norm": 8.141857147216797,
      "learning_rate": 4.478920381063426e-06,
      "loss": 0.8708,
      "step": 4340
    },
    {
      "epoch": 0.2335319697213722,
      "grad_norm": 15.22232723236084,
      "learning_rate": 4.476261082519325e-06,
      "loss": 1.3204,
      "step": 4350
    },
    {
      "epoch": 0.23406882482417996,
      "grad_norm": 5.935036659240723,
      "learning_rate": 4.47359580916947e-06,
      "loss": 0.957,
      "step": 4360
    },
    {
      "epoch": 0.2346056799269877,
      "grad_norm": 5.433386325836182,
      "learning_rate": 4.47092456907175e-06,
      "loss": 1.4029,
      "step": 4370
    },
    {
      "epoch": 0.23514253502979546,
      "grad_norm": 15.733256340026855,
      "learning_rate": 4.468247370302089e-06,
      "loss": 1.4933,
      "step": 4380
    },
    {
      "epoch": 0.23567939013260322,
      "grad_norm": 13.008451461791992,
      "learning_rate": 4.465564220954433e-06,
      "loss": 1.684,
      "step": 4390
    },
    {
      "epoch": 0.23621624523541096,
      "grad_norm": 6.19653844833374,
      "learning_rate": 4.462875129140711e-06,
      "loss": 1.2625,
      "step": 4400
    },
    {
      "epoch": 0.23675310033821872,
      "grad_norm": 7.171701908111572,
      "learning_rate": 4.460180102990823e-06,
      "loss": 0.6682,
      "step": 4410
    },
    {
      "epoch": 0.23728995544102646,
      "grad_norm": 5.662790775299072,
      "learning_rate": 4.457479150652607e-06,
      "loss": 0.71,
      "step": 4420
    },
    {
      "epoch": 0.23782681054383423,
      "grad_norm": 7.740350246429443,
      "learning_rate": 4.454772280291821e-06,
      "loss": 1.028,
      "step": 4430
    },
    {
      "epoch": 0.23836366564664196,
      "grad_norm": 7.4511399269104,
      "learning_rate": 4.452059500092111e-06,
      "loss": 0.6745,
      "step": 4440
    },
    {
      "epoch": 0.23890052074944973,
      "grad_norm": 14.263236045837402,
      "learning_rate": 4.449340818254992e-06,
      "loss": 0.9609,
      "step": 4450
    },
    {
      "epoch": 0.23943737585225747,
      "grad_norm": 13.44764232635498,
      "learning_rate": 4.446616242999822e-06,
      "loss": 1.153,
      "step": 4460
    },
    {
      "epoch": 0.23997423095506523,
      "grad_norm": 9.663025856018066,
      "learning_rate": 4.443885782563775e-06,
      "loss": 0.9713,
      "step": 4470
    },
    {
      "epoch": 0.24051108605787297,
      "grad_norm": 14.856078147888184,
      "learning_rate": 4.4411494452018185e-06,
      "loss": 1.4246,
      "step": 4480
    },
    {
      "epoch": 0.24104794116068073,
      "grad_norm": 7.624509334564209,
      "learning_rate": 4.438407239186689e-06,
      "loss": 0.7398,
      "step": 4490
    },
    {
      "epoch": 0.24158479626348847,
      "grad_norm": 6.766681671142578,
      "learning_rate": 4.435659172808862e-06,
      "loss": 0.7128,
      "step": 4500
    },
    {
      "epoch": 0.24212165136629624,
      "grad_norm": 7.576805591583252,
      "learning_rate": 4.432905254376534e-06,
      "loss": 0.8224,
      "step": 4510
    },
    {
      "epoch": 0.242658506469104,
      "grad_norm": 6.9703192710876465,
      "learning_rate": 4.4301454922155915e-06,
      "loss": 1.2548,
      "step": 4520
    },
    {
      "epoch": 0.24319536157191174,
      "grad_norm": 15.487590789794922,
      "learning_rate": 4.427379894669591e-06,
      "loss": 1.0456,
      "step": 4530
    },
    {
      "epoch": 0.2437322166747195,
      "grad_norm": 15.843807220458984,
      "learning_rate": 4.424608470099728e-06,
      "loss": 1.4801,
      "step": 4540
    },
    {
      "epoch": 0.24426907177752724,
      "grad_norm": 7.35399055480957,
      "learning_rate": 4.421831226884817e-06,
      "loss": 0.892,
      "step": 4550
    },
    {
      "epoch": 0.244805926880335,
      "grad_norm": 4.954522609710693,
      "learning_rate": 4.419048173421262e-06,
      "loss": 1.2843,
      "step": 4560
    },
    {
      "epoch": 0.24534278198314274,
      "grad_norm": 8.045175552368164,
      "learning_rate": 4.416259318123036e-06,
      "loss": 1.6394,
      "step": 4570
    },
    {
      "epoch": 0.2458796370859505,
      "grad_norm": 15.304893493652344,
      "learning_rate": 4.41346466942165e-06,
      "loss": 1.2693,
      "step": 4580
    },
    {
      "epoch": 0.24641649218875825,
      "grad_norm": 8.72211742401123,
      "learning_rate": 4.410664235766131e-06,
      "loss": 1.3058,
      "step": 4590
    },
    {
      "epoch": 0.246953347291566,
      "grad_norm": 11.725997924804688,
      "learning_rate": 4.4078580256229956e-06,
      "loss": 0.9553,
      "step": 4600
    },
    {
      "epoch": 0.24749020239437375,
      "grad_norm": 15.206296920776367,
      "learning_rate": 4.405046047476224e-06,
      "loss": 1.298,
      "step": 4610
    },
    {
      "epoch": 0.2480270574971815,
      "grad_norm": 8.865577697753906,
      "learning_rate": 4.402228309827234e-06,
      "loss": 1.1366,
      "step": 4620
    },
    {
      "epoch": 0.24856391259998925,
      "grad_norm": 16.285247802734375,
      "learning_rate": 4.399404821194859e-06,
      "loss": 1.2946,
      "step": 4630
    },
    {
      "epoch": 0.24910076770279702,
      "grad_norm": 9.764721870422363,
      "learning_rate": 4.396575590115317e-06,
      "loss": 0.9195,
      "step": 4640
    },
    {
      "epoch": 0.24963762280560478,
      "grad_norm": 6.334207057952881,
      "learning_rate": 4.393740625142187e-06,
      "loss": 0.9134,
      "step": 4650
    },
    {
      "epoch": 0.2501744779084125,
      "grad_norm": 14.907050132751465,
      "learning_rate": 4.390899934846383e-06,
      "loss": 1.2896,
      "step": 4660
    },
    {
      "epoch": 0.2507113330112203,
      "grad_norm": 5.170718669891357,
      "learning_rate": 4.388053527816131e-06,
      "loss": 0.9204,
      "step": 4670
    },
    {
      "epoch": 0.25124818811402805,
      "grad_norm": 16.60308265686035,
      "learning_rate": 4.3852014126569355e-06,
      "loss": 0.8742,
      "step": 4680
    },
    {
      "epoch": 0.25178504321683576,
      "grad_norm": 7.8123016357421875,
      "learning_rate": 4.382343597991563e-06,
      "loss": 1.0942,
      "step": 4690
    },
    {
      "epoch": 0.2523218983196435,
      "grad_norm": 9.841978073120117,
      "learning_rate": 4.379480092460009e-06,
      "loss": 0.9558,
      "step": 4700
    },
    {
      "epoch": 0.2528587534224513,
      "grad_norm": 11.607048988342285,
      "learning_rate": 4.3766109047194735e-06,
      "loss": 1.6471,
      "step": 4710
    },
    {
      "epoch": 0.25339560852525905,
      "grad_norm": 7.525402545928955,
      "learning_rate": 4.373736043444338e-06,
      "loss": 1.075,
      "step": 4720
    },
    {
      "epoch": 0.25393246362806676,
      "grad_norm": 8.131234169006348,
      "learning_rate": 4.370855517326133e-06,
      "loss": 1.066,
      "step": 4730
    },
    {
      "epoch": 0.2544693187308745,
      "grad_norm": 16.357973098754883,
      "learning_rate": 4.36796933507352e-06,
      "loss": 1.4238,
      "step": 4740
    },
    {
      "epoch": 0.2550061738336823,
      "grad_norm": 7.250372886657715,
      "learning_rate": 4.365077505412256e-06,
      "loss": 1.3864,
      "step": 4750
    },
    {
      "epoch": 0.25554302893649006,
      "grad_norm": 14.458534240722656,
      "learning_rate": 4.362180037085177e-06,
      "loss": 1.1255,
      "step": 4760
    },
    {
      "epoch": 0.25607988403929777,
      "grad_norm": 16.62920570373535,
      "learning_rate": 4.359276938852159e-06,
      "loss": 1.739,
      "step": 4770
    },
    {
      "epoch": 0.25661673914210553,
      "grad_norm": 13.904657363891602,
      "learning_rate": 4.356368219490107e-06,
      "loss": 1.3442,
      "step": 4780
    },
    {
      "epoch": 0.2571535942449133,
      "grad_norm": 6.338520050048828,
      "learning_rate": 4.3534538877929135e-06,
      "loss": 1.5581,
      "step": 4790
    },
    {
      "epoch": 0.25769044934772106,
      "grad_norm": 5.327105522155762,
      "learning_rate": 4.350533952571444e-06,
      "loss": 1.1037,
      "step": 4800
    },
    {
      "epoch": 0.2582273044505288,
      "grad_norm": 5.9833292961120605,
      "learning_rate": 4.3476084226535e-06,
      "loss": 1.1269,
      "step": 4810
    },
    {
      "epoch": 0.25876415955333654,
      "grad_norm": 7.314708709716797,
      "learning_rate": 4.344677306883802e-06,
      "loss": 0.9375,
      "step": 4820
    },
    {
      "epoch": 0.2593010146561443,
      "grad_norm": 7.332765102386475,
      "learning_rate": 4.341740614123956e-06,
      "loss": 1.1505,
      "step": 4830
    },
    {
      "epoch": 0.25983786975895207,
      "grad_norm": 6.323610782623291,
      "learning_rate": 4.338798353252429e-06,
      "loss": 0.8946,
      "step": 4840
    },
    {
      "epoch": 0.26037472486175983,
      "grad_norm": 7.983599662780762,
      "learning_rate": 4.33585053316452e-06,
      "loss": 1.5263,
      "step": 4850
    },
    {
      "epoch": 0.26091157996456754,
      "grad_norm": 9.077059745788574,
      "learning_rate": 4.33289716277234e-06,
      "loss": 0.9208,
      "step": 4860
    },
    {
      "epoch": 0.2614484350673753,
      "grad_norm": 6.8345513343811035,
      "learning_rate": 4.329938251004776e-06,
      "loss": 1.0719,
      "step": 4870
    },
    {
      "epoch": 0.26198529017018307,
      "grad_norm": 5.419963359832764,
      "learning_rate": 4.326973806807468e-06,
      "loss": 1.0648,
      "step": 4880
    },
    {
      "epoch": 0.26252214527299084,
      "grad_norm": 6.073435306549072,
      "learning_rate": 4.3240038391427865e-06,
      "loss": 0.9139,
      "step": 4890
    },
    {
      "epoch": 0.26305900037579855,
      "grad_norm": 17.399202346801758,
      "learning_rate": 4.321028356989797e-06,
      "loss": 1.4703,
      "step": 4900
    },
    {
      "epoch": 0.2635958554786063,
      "grad_norm": 14.989962577819824,
      "learning_rate": 4.318047369344236e-06,
      "loss": 1.6238,
      "step": 4910
    },
    {
      "epoch": 0.2641327105814141,
      "grad_norm": 10.254854202270508,
      "learning_rate": 4.3150608852184895e-06,
      "loss": 1.3254,
      "step": 4920
    },
    {
      "epoch": 0.26466956568422184,
      "grad_norm": 10.899751663208008,
      "learning_rate": 4.312068913641556e-06,
      "loss": 1.6861,
      "step": 4930
    },
    {
      "epoch": 0.2652064207870296,
      "grad_norm": 8.73538875579834,
      "learning_rate": 4.309071463659028e-06,
      "loss": 0.8937,
      "step": 4940
    },
    {
      "epoch": 0.2657432758898373,
      "grad_norm": 5.894237518310547,
      "learning_rate": 4.306068544333057e-06,
      "loss": 1.3974,
      "step": 4950
    },
    {
      "epoch": 0.2662801309926451,
      "grad_norm": 8.414528846740723,
      "learning_rate": 4.303060164742334e-06,
      "loss": 0.9099,
      "step": 4960
    },
    {
      "epoch": 0.26681698609545285,
      "grad_norm": 7.305602073669434,
      "learning_rate": 4.300046333982056e-06,
      "loss": 1.3039,
      "step": 4970
    },
    {
      "epoch": 0.2673538411982606,
      "grad_norm": 14.907671928405762,
      "learning_rate": 4.297027061163898e-06,
      "loss": 1.0622,
      "step": 4980
    },
    {
      "epoch": 0.2678906963010683,
      "grad_norm": 6.324799060821533,
      "learning_rate": 4.294002355415992e-06,
      "loss": 1.0926,
      "step": 4990
    },
    {
      "epoch": 0.2684275514038761,
      "grad_norm": 8.188894271850586,
      "learning_rate": 4.290972225882894e-06,
      "loss": 1.4569,
      "step": 5000
    },
    {
      "epoch": 0.26896440650668385,
      "grad_norm": 7.2968573570251465,
      "learning_rate": 4.287936681725556e-06,
      "loss": 1.1138,
      "step": 5010
    },
    {
      "epoch": 0.2695012616094916,
      "grad_norm": 7.2989726066589355,
      "learning_rate": 4.284895732121302e-06,
      "loss": 0.8433,
      "step": 5020
    },
    {
      "epoch": 0.2700381167122993,
      "grad_norm": 7.334841728210449,
      "learning_rate": 4.281849386263797e-06,
      "loss": 1.0306,
      "step": 5030
    },
    {
      "epoch": 0.2705749718151071,
      "grad_norm": 6.495748996734619,
      "learning_rate": 4.278797653363021e-06,
      "loss": 1.1629,
      "step": 5040
    },
    {
      "epoch": 0.27111182691791486,
      "grad_norm": 8.87549114227295,
      "learning_rate": 4.27574054264524e-06,
      "loss": 1.2897,
      "step": 5050
    },
    {
      "epoch": 0.2716486820207226,
      "grad_norm": 16.03841781616211,
      "learning_rate": 4.272678063352981e-06,
      "loss": 1.332,
      "step": 5060
    },
    {
      "epoch": 0.2721855371235304,
      "grad_norm": 5.857301235198975,
      "learning_rate": 4.269610224744996e-06,
      "loss": 0.8517,
      "step": 5070
    },
    {
      "epoch": 0.2727223922263381,
      "grad_norm": 7.983285427093506,
      "learning_rate": 4.266537036096249e-06,
      "loss": 0.9129,
      "step": 5080
    },
    {
      "epoch": 0.27325924732914586,
      "grad_norm": 6.68778133392334,
      "learning_rate": 4.263458506697869e-06,
      "loss": 1.0733,
      "step": 5090
    },
    {
      "epoch": 0.2737961024319536,
      "grad_norm": 7.71735143661499,
      "learning_rate": 4.260374645857137e-06,
      "loss": 1.0463,
      "step": 5100
    },
    {
      "epoch": 0.2743329575347614,
      "grad_norm": 8.701825141906738,
      "learning_rate": 4.2572854628974525e-06,
      "loss": 0.9092,
      "step": 5110
    },
    {
      "epoch": 0.2748698126375691,
      "grad_norm": 6.744924068450928,
      "learning_rate": 4.2541909671583035e-06,
      "loss": 0.8516,
      "step": 5120
    },
    {
      "epoch": 0.27540666774037686,
      "grad_norm": 7.448139667510986,
      "learning_rate": 4.2510911679952405e-06,
      "loss": 0.6841,
      "step": 5130
    },
    {
      "epoch": 0.27594352284318463,
      "grad_norm": 5.38520622253418,
      "learning_rate": 4.247986074779849e-06,
      "loss": 1.2909,
      "step": 5140
    },
    {
      "epoch": 0.2764803779459924,
      "grad_norm": 7.976722240447998,
      "learning_rate": 4.244875696899718e-06,
      "loss": 1.5932,
      "step": 5150
    },
    {
      "epoch": 0.2770172330488001,
      "grad_norm": 14.545150756835938,
      "learning_rate": 4.241760043758415e-06,
      "loss": 1.2443,
      "step": 5160
    },
    {
      "epoch": 0.27755408815160787,
      "grad_norm": 6.2145676612854,
      "learning_rate": 4.238639124775456e-06,
      "loss": 0.967,
      "step": 5170
    },
    {
      "epoch": 0.27809094325441563,
      "grad_norm": 16.64395523071289,
      "learning_rate": 4.2355129493862765e-06,
      "loss": 1.2965,
      "step": 5180
    },
    {
      "epoch": 0.2786277983572234,
      "grad_norm": 5.606618404388428,
      "learning_rate": 4.232381527042203e-06,
      "loss": 0.8345,
      "step": 5190
    },
    {
      "epoch": 0.27916465346003116,
      "grad_norm": 9.887587547302246,
      "learning_rate": 4.2292448672104296e-06,
      "loss": 1.3318,
      "step": 5200
    },
    {
      "epoch": 0.2797015085628389,
      "grad_norm": 7.770495891571045,
      "learning_rate": 4.226102979373977e-06,
      "loss": 1.1236,
      "step": 5210
    },
    {
      "epoch": 0.28023836366564664,
      "grad_norm": 14.979072570800781,
      "learning_rate": 4.222955873031678e-06,
      "loss": 1.4303,
      "step": 5220
    },
    {
      "epoch": 0.2807752187684544,
      "grad_norm": 5.548940181732178,
      "learning_rate": 4.21980355769814e-06,
      "loss": 1.2761,
      "step": 5230
    },
    {
      "epoch": 0.28131207387126217,
      "grad_norm": 9.678160667419434,
      "learning_rate": 4.21664604290372e-06,
      "loss": 1.4008,
      "step": 5240
    },
    {
      "epoch": 0.2818489289740699,
      "grad_norm": 13.987789154052734,
      "learning_rate": 4.213483338194492e-06,
      "loss": 0.8581,
      "step": 5250
    },
    {
      "epoch": 0.28238578407687764,
      "grad_norm": 14.557610511779785,
      "learning_rate": 4.210315453132224e-06,
      "loss": 1.2654,
      "step": 5260
    },
    {
      "epoch": 0.2829226391796854,
      "grad_norm": 14.873409271240234,
      "learning_rate": 4.207142397294342e-06,
      "loss": 1.4547,
      "step": 5270
    },
    {
      "epoch": 0.2834594942824932,
      "grad_norm": 8.590595245361328,
      "learning_rate": 4.2039641802739076e-06,
      "loss": 1.2765,
      "step": 5280
    },
    {
      "epoch": 0.2839963493853009,
      "grad_norm": 6.578243732452393,
      "learning_rate": 4.200780811679584e-06,
      "loss": 1.3259,
      "step": 5290
    },
    {
      "epoch": 0.28453320448810865,
      "grad_norm": 6.469790935516357,
      "learning_rate": 4.197592301135611e-06,
      "loss": 1.4357,
      "step": 5300
    },
    {
      "epoch": 0.2850700595909164,
      "grad_norm": 6.369358539581299,
      "learning_rate": 4.194398658281774e-06,
      "loss": 1.3226,
      "step": 5310
    },
    {
      "epoch": 0.2856069146937242,
      "grad_norm": 6.664346218109131,
      "learning_rate": 4.191199892773373e-06,
      "loss": 1.5481,
      "step": 5320
    },
    {
      "epoch": 0.28614376979653194,
      "grad_norm": 11.716681480407715,
      "learning_rate": 4.187996014281197e-06,
      "loss": 1.1174,
      "step": 5330
    },
    {
      "epoch": 0.28668062489933965,
      "grad_norm": 9.262947082519531,
      "learning_rate": 4.184787032491491e-06,
      "loss": 0.8648,
      "step": 5340
    },
    {
      "epoch": 0.2872174800021474,
      "grad_norm": 6.628335952758789,
      "learning_rate": 4.181572957105932e-06,
      "loss": 0.9386,
      "step": 5350
    },
    {
      "epoch": 0.2877543351049552,
      "grad_norm": 16.815969467163086,
      "learning_rate": 4.178353797841592e-06,
      "loss": 1.3468,
      "step": 5360
    },
    {
      "epoch": 0.28829119020776295,
      "grad_norm": 9.601729393005371,
      "learning_rate": 4.175129564430919e-06,
      "loss": 1.183,
      "step": 5370
    },
    {
      "epoch": 0.28882804531057066,
      "grad_norm": 16.4572696685791,
      "learning_rate": 4.171900266621695e-06,
      "loss": 0.8782,
      "step": 5380
    },
    {
      "epoch": 0.2893649004133784,
      "grad_norm": 7.192978858947754,
      "learning_rate": 4.1686659141770166e-06,
      "loss": 0.9901,
      "step": 5390
    },
    {
      "epoch": 0.2899017555161862,
      "grad_norm": 7.557520866394043,
      "learning_rate": 4.165426516875263e-06,
      "loss": 0.8602,
      "step": 5400
    },
    {
      "epoch": 0.29043861061899395,
      "grad_norm": 5.464683532714844,
      "learning_rate": 4.1621820845100624e-06,
      "loss": 0.8847,
      "step": 5410
    },
    {
      "epoch": 0.29097546572180166,
      "grad_norm": 12.930455207824707,
      "learning_rate": 4.158932626890269e-06,
      "loss": 1.1192,
      "step": 5420
    },
    {
      "epoch": 0.2915123208246094,
      "grad_norm": 6.370907306671143,
      "learning_rate": 4.155678153839927e-06,
      "loss": 1.0656,
      "step": 5430
    },
    {
      "epoch": 0.2920491759274172,
      "grad_norm": 6.343400001525879,
      "learning_rate": 4.152418675198245e-06,
      "loss": 0.8562,
      "step": 5440
    },
    {
      "epoch": 0.29258603103022496,
      "grad_norm": 15.066439628601074,
      "learning_rate": 4.149154200819564e-06,
      "loss": 1.2986,
      "step": 5450
    },
    {
      "epoch": 0.29312288613303267,
      "grad_norm": 7.98778772354126,
      "learning_rate": 4.145884740573329e-06,
      "loss": 0.646,
      "step": 5460
    },
    {
      "epoch": 0.29365974123584043,
      "grad_norm": 6.593790531158447,
      "learning_rate": 4.142610304344061e-06,
      "loss": 1.0345,
      "step": 5470
    },
    {
      "epoch": 0.2941965963386482,
      "grad_norm": 8.7335205078125,
      "learning_rate": 4.139330902031319e-06,
      "loss": 0.6564,
      "step": 5480
    },
    {
      "epoch": 0.29473345144145596,
      "grad_norm": 14.440370559692383,
      "learning_rate": 4.136046543549683e-06,
      "loss": 1.321,
      "step": 5490
    },
    {
      "epoch": 0.2952703065442637,
      "grad_norm": 14.811610221862793,
      "learning_rate": 4.1327572388287105e-06,
      "loss": 1.3228,
      "step": 5500
    },
    {
      "epoch": 0.29580716164707144,
      "grad_norm": 7.305030822753906,
      "learning_rate": 4.129462997812918e-06,
      "loss": 0.9261,
      "step": 5510
    },
    {
      "epoch": 0.2963440167498792,
      "grad_norm": 14.89034366607666,
      "learning_rate": 4.126163830461744e-06,
      "loss": 1.3928,
      "step": 5520
    },
    {
      "epoch": 0.29688087185268697,
      "grad_norm": 9.645459175109863,
      "learning_rate": 4.1228597467495185e-06,
      "loss": 0.9012,
      "step": 5530
    },
    {
      "epoch": 0.29741772695549473,
      "grad_norm": 12.189104080200195,
      "learning_rate": 4.119550756665438e-06,
      "loss": 0.6854,
      "step": 5540
    },
    {
      "epoch": 0.29795458205830244,
      "grad_norm": 4.6539411544799805,
      "learning_rate": 4.1162368702135326e-06,
      "loss": 1.3102,
      "step": 5550
    },
    {
      "epoch": 0.2984914371611102,
      "grad_norm": 15.340166091918945,
      "learning_rate": 4.112918097412633e-06,
      "loss": 1.1095,
      "step": 5560
    },
    {
      "epoch": 0.29902829226391797,
      "grad_norm": 15.2852144241333,
      "learning_rate": 4.1095944482963436e-06,
      "loss": 1.0254,
      "step": 5570
    },
    {
      "epoch": 0.29956514736672574,
      "grad_norm": 9.176116943359375,
      "learning_rate": 4.106265932913013e-06,
      "loss": 1.2259,
      "step": 5580
    },
    {
      "epoch": 0.30010200246953345,
      "grad_norm": 7.110942363739014,
      "learning_rate": 4.102932561325699e-06,
      "loss": 1.1049,
      "step": 5590
    },
    {
      "epoch": 0.3006388575723412,
      "grad_norm": 14.922239303588867,
      "learning_rate": 4.099594343612146e-06,
      "loss": 1.4992,
      "step": 5600
    },
    {
      "epoch": 0.301175712675149,
      "grad_norm": 16.830324172973633,
      "learning_rate": 4.096251289864743e-06,
      "loss": 1.4807,
      "step": 5610
    },
    {
      "epoch": 0.30171256777795674,
      "grad_norm": 5.010047912597656,
      "learning_rate": 4.092903410190505e-06,
      "loss": 1.4759,
      "step": 5620
    },
    {
      "epoch": 0.3022494228807645,
      "grad_norm": 8.636159896850586,
      "learning_rate": 4.0895507147110345e-06,
      "loss": 0.807,
      "step": 5630
    },
    {
      "epoch": 0.3027862779835722,
      "grad_norm": 10.074819564819336,
      "learning_rate": 4.086193213562495e-06,
      "loss": 0.8583,
      "step": 5640
    },
    {
      "epoch": 0.30332313308638,
      "grad_norm": 7.2127366065979,
      "learning_rate": 4.082830916895578e-06,
      "loss": 0.8729,
      "step": 5650
    },
    {
      "epoch": 0.30385998818918775,
      "grad_norm": 7.143892288208008,
      "learning_rate": 4.079463834875474e-06,
      "loss": 1.3094,
      "step": 5660
    },
    {
      "epoch": 0.3043968432919955,
      "grad_norm": 6.193157196044922,
      "learning_rate": 4.07609197768184e-06,
      "loss": 1.1032,
      "step": 5670
    },
    {
      "epoch": 0.3049336983948032,
      "grad_norm": 7.1880083084106445,
      "learning_rate": 4.07271535550877e-06,
      "loss": 1.352,
      "step": 5680
    },
    {
      "epoch": 0.305470553497611,
      "grad_norm": 8.085671424865723,
      "learning_rate": 4.0693339785647655e-06,
      "loss": 1.2552,
      "step": 5690
    },
    {
      "epoch": 0.30600740860041875,
      "grad_norm": 5.260685443878174,
      "learning_rate": 4.065947857072701e-06,
      "loss": 1.0968,
      "step": 5700
    },
    {
      "epoch": 0.3065442637032265,
      "grad_norm": 7.686944484710693,
      "learning_rate": 4.0625570012697944e-06,
      "loss": 1.4507,
      "step": 5710
    },
    {
      "epoch": 0.3070811188060342,
      "grad_norm": 13.07571029663086,
      "learning_rate": 4.05916142140758e-06,
      "loss": 1.0355,
      "step": 5720
    },
    {
      "epoch": 0.307617973908842,
      "grad_norm": 5.8516058921813965,
      "learning_rate": 4.0557611277518725e-06,
      "loss": 0.6393,
      "step": 5730
    },
    {
      "epoch": 0.30815482901164976,
      "grad_norm": 13.10562801361084,
      "learning_rate": 4.052356130582738e-06,
      "loss": 1.442,
      "step": 5740
    },
    {
      "epoch": 0.3086916841144575,
      "grad_norm": 6.067203044891357,
      "learning_rate": 4.04894644019446e-06,
      "loss": 1.1442,
      "step": 5750
    },
    {
      "epoch": 0.3092285392172653,
      "grad_norm": 5.709597587585449,
      "learning_rate": 4.045532066895516e-06,
      "loss": 1.2771,
      "step": 5760
    },
    {
      "epoch": 0.309765394320073,
      "grad_norm": 7.8425774574279785,
      "learning_rate": 4.042113021008538e-06,
      "loss": 0.9141,
      "step": 5770
    },
    {
      "epoch": 0.31030224942288076,
      "grad_norm": 7.025063991546631,
      "learning_rate": 4.038689312870284e-06,
      "loss": 0.6666,
      "step": 5780
    },
    {
      "epoch": 0.3108391045256885,
      "grad_norm": 8.906791687011719,
      "learning_rate": 4.03526095283161e-06,
      "loss": 1.0256,
      "step": 5790
    },
    {
      "epoch": 0.3113759596284963,
      "grad_norm": 4.622925758361816,
      "learning_rate": 4.031827951257435e-06,
      "loss": 0.6632,
      "step": 5800
    },
    {
      "epoch": 0.311912814731304,
      "grad_norm": 7.205989360809326,
      "learning_rate": 4.028390318526709e-06,
      "loss": 1.2302,
      "step": 5810
    },
    {
      "epoch": 0.31244966983411177,
      "grad_norm": 11.43480110168457,
      "learning_rate": 4.024948065032385e-06,
      "loss": 1.2903,
      "step": 5820
    },
    {
      "epoch": 0.31298652493691953,
      "grad_norm": 12.686202049255371,
      "learning_rate": 4.021501201181386e-06,
      "loss": 0.8516,
      "step": 5830
    },
    {
      "epoch": 0.3135233800397273,
      "grad_norm": 5.660851955413818,
      "learning_rate": 4.018049737394572e-06,
      "loss": 1.0664,
      "step": 5840
    },
    {
      "epoch": 0.314060235142535,
      "grad_norm": 7.292784214019775,
      "learning_rate": 4.014593684106713e-06,
      "loss": 1.2384,
      "step": 5850
    },
    {
      "epoch": 0.31459709024534277,
      "grad_norm": 5.596077919006348,
      "learning_rate": 4.011133051766451e-06,
      "loss": 0.7139,
      "step": 5860
    },
    {
      "epoch": 0.31513394534815053,
      "grad_norm": 14.660816192626953,
      "learning_rate": 4.007667850836271e-06,
      "loss": 1.1554,
      "step": 5870
    },
    {
      "epoch": 0.3156708004509583,
      "grad_norm": 15.471632957458496,
      "learning_rate": 4.004198091792475e-06,
      "loss": 1.3381,
      "step": 5880
    },
    {
      "epoch": 0.31620765555376606,
      "grad_norm": 6.381232261657715,
      "learning_rate": 4.000723785125142e-06,
      "loss": 0.8823,
      "step": 5890
    },
    {
      "epoch": 0.3167445106565738,
      "grad_norm": 5.229235649108887,
      "learning_rate": 3.997244941338101e-06,
      "loss": 1.3409,
      "step": 5900
    },
    {
      "epoch": 0.31728136575938154,
      "grad_norm": 4.810164928436279,
      "learning_rate": 3.993761570948896e-06,
      "loss": 1.3034,
      "step": 5910
    },
    {
      "epoch": 0.3178182208621893,
      "grad_norm": 5.827229022979736,
      "learning_rate": 3.990273684488759e-06,
      "loss": 0.9301,
      "step": 5920
    },
    {
      "epoch": 0.31835507596499707,
      "grad_norm": 13.622220993041992,
      "learning_rate": 3.9867812925025734e-06,
      "loss": 1.2238,
      "step": 5930
    },
    {
      "epoch": 0.3188919310678048,
      "grad_norm": 10.48565673828125,
      "learning_rate": 3.983284405548846e-06,
      "loss": 0.9263,
      "step": 5940
    },
    {
      "epoch": 0.31942878617061254,
      "grad_norm": 8.33576488494873,
      "learning_rate": 3.97978303419967e-06,
      "loss": 1.1506,
      "step": 5950
    },
    {
      "epoch": 0.3199656412734203,
      "grad_norm": 4.851088523864746,
      "learning_rate": 3.9762771890407e-06,
      "loss": 0.6912,
      "step": 5960
    },
    {
      "epoch": 0.3205024963762281,
      "grad_norm": 13.780621528625488,
      "learning_rate": 3.972766880671113e-06,
      "loss": 1.1464,
      "step": 5970
    },
    {
      "epoch": 0.3210393514790358,
      "grad_norm": 7.191285610198975,
      "learning_rate": 3.969252119703583e-06,
      "loss": 1.0302,
      "step": 5980
    },
    {
      "epoch": 0.32157620658184355,
      "grad_norm": 13.856903076171875,
      "learning_rate": 3.965732916764241e-06,
      "loss": 1.3769,
      "step": 5990
    },
    {
      "epoch": 0.3221130616846513,
      "grad_norm": 14.530896186828613,
      "learning_rate": 3.962209282492652e-06,
      "loss": 1.5762,
      "step": 6000
    },
    {
      "epoch": 0.3226499167874591,
      "grad_norm": 14.211827278137207,
      "learning_rate": 3.958681227541775e-06,
      "loss": 1.0321,
      "step": 6010
    },
    {
      "epoch": 0.32318677189026684,
      "grad_norm": 15.23973560333252,
      "learning_rate": 3.955148762577934e-06,
      "loss": 1.4733,
      "step": 6020
    },
    {
      "epoch": 0.32372362699307455,
      "grad_norm": 14.812249183654785,
      "learning_rate": 3.951611898280789e-06,
      "loss": 0.8615,
      "step": 6030
    },
    {
      "epoch": 0.3242604820958823,
      "grad_norm": 5.459301948547363,
      "learning_rate": 3.948070645343297e-06,
      "loss": 1.1625,
      "step": 6040
    },
    {
      "epoch": 0.3247973371986901,
      "grad_norm": 6.634744644165039,
      "learning_rate": 3.944525014471684e-06,
      "loss": 1.2098,
      "step": 6050
    },
    {
      "epoch": 0.32533419230149785,
      "grad_norm": 9.980705261230469,
      "learning_rate": 3.940975016385414e-06,
      "loss": 1.2521,
      "step": 6060
    },
    {
      "epoch": 0.32587104740430556,
      "grad_norm": 5.953916072845459,
      "learning_rate": 3.937420661817149e-06,
      "loss": 0.804,
      "step": 6070
    },
    {
      "epoch": 0.3264079025071133,
      "grad_norm": 6.309021949768066,
      "learning_rate": 3.933861961512728e-06,
      "loss": 1.0544,
      "step": 6080
    },
    {
      "epoch": 0.3269447576099211,
      "grad_norm": 7.107146263122559,
      "learning_rate": 3.930298926231125e-06,
      "loss": 1.1879,
      "step": 6090
    },
    {
      "epoch": 0.32748161271272885,
      "grad_norm": 7.152374744415283,
      "learning_rate": 3.92673156674442e-06,
      "loss": 0.7911,
      "step": 6100
    },
    {
      "epoch": 0.32801846781553656,
      "grad_norm": 7.954428672790527,
      "learning_rate": 3.923159893837768e-06,
      "loss": 1.2758,
      "step": 6110
    },
    {
      "epoch": 0.32855532291834433,
      "grad_norm": 14.012812614440918,
      "learning_rate": 3.919583918309363e-06,
      "loss": 0.8328,
      "step": 6120
    },
    {
      "epoch": 0.3290921780211521,
      "grad_norm": 8.462418556213379,
      "learning_rate": 3.916003650970408e-06,
      "loss": 1.0507,
      "step": 6130
    },
    {
      "epoch": 0.32962903312395986,
      "grad_norm": 16.7008113861084,
      "learning_rate": 3.912419102645082e-06,
      "loss": 1.5476,
      "step": 6140
    },
    {
      "epoch": 0.3301658882267676,
      "grad_norm": 9.216864585876465,
      "learning_rate": 3.9088302841705026e-06,
      "loss": 1.0944,
      "step": 6150
    },
    {
      "epoch": 0.33070274332957533,
      "grad_norm": 5.740019798278809,
      "learning_rate": 3.9052372063967045e-06,
      "loss": 1.0736,
      "step": 6160
    },
    {
      "epoch": 0.3312395984323831,
      "grad_norm": 7.434545993804932,
      "learning_rate": 3.901639880186592e-06,
      "loss": 1.3061,
      "step": 6170
    },
    {
      "epoch": 0.33177645353519086,
      "grad_norm": 8.90616226196289,
      "learning_rate": 3.89803831641592e-06,
      "loss": 0.6575,
      "step": 6180
    },
    {
      "epoch": 0.33231330863799863,
      "grad_norm": 5.130674362182617,
      "learning_rate": 3.894432525973248e-06,
      "loss": 0.9277,
      "step": 6190
    },
    {
      "epoch": 0.33285016374080634,
      "grad_norm": 8.033321380615234,
      "learning_rate": 3.8908225197599225e-06,
      "loss": 1.8148,
      "step": 6200
    },
    {
      "epoch": 0.3333870188436141,
      "grad_norm": 14.978224754333496,
      "learning_rate": 3.887208308690027e-06,
      "loss": 1.1482,
      "step": 6210
    },
    {
      "epoch": 0.33392387394642187,
      "grad_norm": 14.250873565673828,
      "learning_rate": 3.883589903690363e-06,
      "loss": 1.0421,
      "step": 6220
    },
    {
      "epoch": 0.33446072904922963,
      "grad_norm": 6.839808940887451,
      "learning_rate": 3.879967315700409e-06,
      "loss": 1.2419,
      "step": 6230
    },
    {
      "epoch": 0.33499758415203734,
      "grad_norm": 5.03639554977417,
      "learning_rate": 3.876340555672291e-06,
      "loss": 0.8643,
      "step": 6240
    },
    {
      "epoch": 0.3355344392548451,
      "grad_norm": 5.803928375244141,
      "learning_rate": 3.872709634570748e-06,
      "loss": 1.3,
      "step": 6250
    },
    {
      "epoch": 0.33607129435765287,
      "grad_norm": 8.988733291625977,
      "learning_rate": 3.869074563373099e-06,
      "loss": 0.8798,
      "step": 6260
    },
    {
      "epoch": 0.33660814946046064,
      "grad_norm": 7.779637336730957,
      "learning_rate": 3.865435353069209e-06,
      "loss": 1.434,
      "step": 6270
    },
    {
      "epoch": 0.33714500456326835,
      "grad_norm": 8.458354949951172,
      "learning_rate": 3.861792014661458e-06,
      "loss": 0.8852,
      "step": 6280
    },
    {
      "epoch": 0.3376818596660761,
      "grad_norm": 15.749419212341309,
      "learning_rate": 3.858144559164707e-06,
      "loss": 1.4662,
      "step": 6290
    },
    {
      "epoch": 0.3382187147688839,
      "grad_norm": 7.231342792510986,
      "learning_rate": 3.854492997606262e-06,
      "loss": 0.6994,
      "step": 6300
    },
    {
      "epoch": 0.33875556987169164,
      "grad_norm": 7.197060585021973,
      "learning_rate": 3.850837341025846e-06,
      "loss": 1.5055,
      "step": 6310
    },
    {
      "epoch": 0.3392924249744994,
      "grad_norm": 6.228940010070801,
      "learning_rate": 3.847177600475558e-06,
      "loss": 1.1282,
      "step": 6320
    },
    {
      "epoch": 0.3398292800773071,
      "grad_norm": 6.368260860443115,
      "learning_rate": 3.843513787019847e-06,
      "loss": 0.928,
      "step": 6330
    },
    {
      "epoch": 0.3403661351801149,
      "grad_norm": 6.207856178283691,
      "learning_rate": 3.839845911735476e-06,
      "loss": 0.7534,
      "step": 6340
    },
    {
      "epoch": 0.34090299028292265,
      "grad_norm": 7.9572553634643555,
      "learning_rate": 3.836173985711486e-06,
      "loss": 0.8441,
      "step": 6350
    },
    {
      "epoch": 0.3414398453857304,
      "grad_norm": 5.6978912353515625,
      "learning_rate": 3.832498020049167e-06,
      "loss": 1.0152,
      "step": 6360
    },
    {
      "epoch": 0.3419767004885381,
      "grad_norm": 7.680033206939697,
      "learning_rate": 3.82881802586202e-06,
      "loss": 0.9793,
      "step": 6370
    },
    {
      "epoch": 0.3425135555913459,
      "grad_norm": 5.439462184906006,
      "learning_rate": 3.825134014275725e-06,
      "loss": 1.0765,
      "step": 6380
    },
    {
      "epoch": 0.34305041069415365,
      "grad_norm": 10.672872543334961,
      "learning_rate": 3.821445996428111e-06,
      "loss": 1.1338,
      "step": 6390
    },
    {
      "epoch": 0.3435872657969614,
      "grad_norm": 12.346087455749512,
      "learning_rate": 3.8177539834691145e-06,
      "loss": 1.3784,
      "step": 6400
    },
    {
      "epoch": 0.3441241208997691,
      "grad_norm": 5.221214771270752,
      "learning_rate": 3.8140579865607548e-06,
      "loss": 0.8561,
      "step": 6410
    },
    {
      "epoch": 0.3446609760025769,
      "grad_norm": 8.814742088317871,
      "learning_rate": 3.810358016877092e-06,
      "loss": 1.0338,
      "step": 6420
    },
    {
      "epoch": 0.34519783110538466,
      "grad_norm": 7.251826286315918,
      "learning_rate": 3.8066540856042e-06,
      "loss": 0.8517,
      "step": 6430
    },
    {
      "epoch": 0.3457346862081924,
      "grad_norm": 8.360296249389648,
      "learning_rate": 3.8029462039401283e-06,
      "loss": 0.913,
      "step": 6440
    },
    {
      "epoch": 0.3462715413110002,
      "grad_norm": 6.797801971435547,
      "learning_rate": 3.7992343830948697e-06,
      "loss": 1.1002,
      "step": 6450
    },
    {
      "epoch": 0.3468083964138079,
      "grad_norm": 5.847208023071289,
      "learning_rate": 3.7955186342903256e-06,
      "loss": 0.8652,
      "step": 6460
    },
    {
      "epoch": 0.34734525151661566,
      "grad_norm": 7.225777626037598,
      "learning_rate": 3.7917989687602747e-06,
      "loss": 1.219,
      "step": 6470
    },
    {
      "epoch": 0.3478821066194234,
      "grad_norm": 15.496307373046875,
      "learning_rate": 3.788075397750336e-06,
      "loss": 1.0296,
      "step": 6480
    },
    {
      "epoch": 0.3484189617222312,
      "grad_norm": 14.892407417297363,
      "learning_rate": 3.7843479325179345e-06,
      "loss": 1.27,
      "step": 6490
    },
    {
      "epoch": 0.3489558168250389,
      "grad_norm": 6.993169784545898,
      "learning_rate": 3.78061658433227e-06,
      "loss": 0.8956,
      "step": 6500
    },
    {
      "epoch": 0.34949267192784667,
      "grad_norm": 7.215968608856201,
      "learning_rate": 3.776881364474283e-06,
      "loss": 1.4104,
      "step": 6510
    },
    {
      "epoch": 0.35002952703065443,
      "grad_norm": 13.780594825744629,
      "learning_rate": 3.7731422842366165e-06,
      "loss": 1.0045,
      "step": 6520
    },
    {
      "epoch": 0.3505663821334622,
      "grad_norm": 6.66459321975708,
      "learning_rate": 3.7693993549235864e-06,
      "loss": 1.0188,
      "step": 6530
    },
    {
      "epoch": 0.3511032372362699,
      "grad_norm": 6.512606143951416,
      "learning_rate": 3.765652587851143e-06,
      "loss": 0.6912,
      "step": 6540
    },
    {
      "epoch": 0.35164009233907767,
      "grad_norm": 5.75010347366333,
      "learning_rate": 3.7619019943468432e-06,
      "loss": 1.231,
      "step": 6550
    },
    {
      "epoch": 0.35217694744188544,
      "grad_norm": 6.6261372566223145,
      "learning_rate": 3.758147585749809e-06,
      "loss": 1.0525,
      "step": 6560
    },
    {
      "epoch": 0.3527138025446932,
      "grad_norm": 7.9663825035095215,
      "learning_rate": 3.7543893734106974e-06,
      "loss": 1.0022,
      "step": 6570
    },
    {
      "epoch": 0.35325065764750097,
      "grad_norm": 7.838817119598389,
      "learning_rate": 3.7506273686916654e-06,
      "loss": 1.3049,
      "step": 6580
    },
    {
      "epoch": 0.3537875127503087,
      "grad_norm": 13.217529296875,
      "learning_rate": 3.7468615829663357e-06,
      "loss": 0.8755,
      "step": 6590
    },
    {
      "epoch": 0.35432436785311644,
      "grad_norm": 6.292603015899658,
      "learning_rate": 3.7430920276197623e-06,
      "loss": 1.2115,
      "step": 6600
    },
    {
      "epoch": 0.3548612229559242,
      "grad_norm": 7.872997760772705,
      "learning_rate": 3.739318714048395e-06,
      "loss": 1.094,
      "step": 6610
    },
    {
      "epoch": 0.35539807805873197,
      "grad_norm": 5.330392360687256,
      "learning_rate": 3.735541653660046e-06,
      "loss": 1.0761,
      "step": 6620
    },
    {
      "epoch": 0.3559349331615397,
      "grad_norm": 5.569629192352295,
      "learning_rate": 3.7317608578738553e-06,
      "loss": 1.2746,
      "step": 6630
    },
    {
      "epoch": 0.35647178826434744,
      "grad_norm": 15.43908405303955,
      "learning_rate": 3.727976338120256e-06,
      "loss": 0.9218,
      "step": 6640
    },
    {
      "epoch": 0.3570086433671552,
      "grad_norm": 7.962451934814453,
      "learning_rate": 3.7241881058409413e-06,
      "loss": 0.8843,
      "step": 6650
    },
    {
      "epoch": 0.357545498469963,
      "grad_norm": 7.071935653686523,
      "learning_rate": 3.720396172488825e-06,
      "loss": 0.6699,
      "step": 6660
    },
    {
      "epoch": 0.3580823535727707,
      "grad_norm": 4.685774803161621,
      "learning_rate": 3.7166005495280144e-06,
      "loss": 0.8456,
      "step": 6670
    },
    {
      "epoch": 0.35861920867557845,
      "grad_norm": 8.111054420471191,
      "learning_rate": 3.7128012484337687e-06,
      "loss": 0.8254,
      "step": 6680
    },
    {
      "epoch": 0.3591560637783862,
      "grad_norm": 5.9869585037231445,
      "learning_rate": 3.7089982806924677e-06,
      "loss": 1.295,
      "step": 6690
    },
    {
      "epoch": 0.359692918881194,
      "grad_norm": 6.055211067199707,
      "learning_rate": 3.705191657801578e-06,
      "loss": 1.4206,
      "step": 6700
    },
    {
      "epoch": 0.36022977398400174,
      "grad_norm": 15.921135902404785,
      "learning_rate": 3.7013813912696137e-06,
      "loss": 0.9116,
      "step": 6710
    },
    {
      "epoch": 0.36076662908680945,
      "grad_norm": 6.481049537658691,
      "learning_rate": 3.697567492616109e-06,
      "loss": 0.8416,
      "step": 6720
    },
    {
      "epoch": 0.3613034841896172,
      "grad_norm": 5.300196647644043,
      "learning_rate": 3.6937499733715754e-06,
      "loss": 0.8294,
      "step": 6730
    },
    {
      "epoch": 0.361840339292425,
      "grad_norm": 7.917925834655762,
      "learning_rate": 3.6899288450774713e-06,
      "loss": 1.213,
      "step": 6740
    },
    {
      "epoch": 0.36237719439523275,
      "grad_norm": 13.216570854187012,
      "learning_rate": 3.6861041192861667e-06,
      "loss": 1.3886,
      "step": 6750
    },
    {
      "epoch": 0.36291404949804046,
      "grad_norm": 6.413755893707275,
      "learning_rate": 3.6822758075609093e-06,
      "loss": 1.0481,
      "step": 6760
    },
    {
      "epoch": 0.3634509046008482,
      "grad_norm": 4.810872554779053,
      "learning_rate": 3.6784439214757862e-06,
      "loss": 1.2706,
      "step": 6770
    },
    {
      "epoch": 0.363987759703656,
      "grad_norm": 5.1552276611328125,
      "learning_rate": 3.6746084726156906e-06,
      "loss": 1.3631,
      "step": 6780
    },
    {
      "epoch": 0.36452461480646375,
      "grad_norm": 6.352284908294678,
      "learning_rate": 3.670769472576288e-06,
      "loss": 1.0553,
      "step": 6790
    },
    {
      "epoch": 0.36506146990927146,
      "grad_norm": 15.445347785949707,
      "learning_rate": 3.66692693296398e-06,
      "loss": 0.7704,
      "step": 6800
    },
    {
      "epoch": 0.36559832501207923,
      "grad_norm": 11.609169006347656,
      "learning_rate": 3.6630808653958694e-06,
      "loss": 0.8296,
      "step": 6810
    },
    {
      "epoch": 0.366135180114887,
      "grad_norm": 13.41612434387207,
      "learning_rate": 3.6592312814997244e-06,
      "loss": 1.2943,
      "step": 6820
    },
    {
      "epoch": 0.36667203521769476,
      "grad_norm": 8.015268325805664,
      "learning_rate": 3.6553781929139454e-06,
      "loss": 0.9025,
      "step": 6830
    },
    {
      "epoch": 0.3672088903205025,
      "grad_norm": 7.323563575744629,
      "learning_rate": 3.651521611287527e-06,
      "loss": 1.5019,
      "step": 6840
    },
    {
      "epoch": 0.36774574542331023,
      "grad_norm": 5.2865166664123535,
      "learning_rate": 3.647661548280024e-06,
      "loss": 1.4556,
      "step": 6850
    },
    {
      "epoch": 0.368282600526118,
      "grad_norm": 6.444762706756592,
      "learning_rate": 3.6437980155615187e-06,
      "loss": 1.1787,
      "step": 6860
    },
    {
      "epoch": 0.36881945562892576,
      "grad_norm": 4.8634724617004395,
      "learning_rate": 3.6399310248125807e-06,
      "loss": 1.3888,
      "step": 6870
    },
    {
      "epoch": 0.36935631073173353,
      "grad_norm": 5.923860549926758,
      "learning_rate": 3.6360605877242374e-06,
      "loss": 1.099,
      "step": 6880
    },
    {
      "epoch": 0.36989316583454124,
      "grad_norm": 15.466293334960938,
      "learning_rate": 3.6321867159979305e-06,
      "loss": 1.0395,
      "step": 6890
    },
    {
      "epoch": 0.370430020937349,
      "grad_norm": 13.171893119812012,
      "learning_rate": 3.628309421345492e-06,
      "loss": 1.4445,
      "step": 6900
    },
    {
      "epoch": 0.37096687604015677,
      "grad_norm": 15.021842956542969,
      "learning_rate": 3.6244287154890955e-06,
      "loss": 0.9264,
      "step": 6910
    },
    {
      "epoch": 0.37150373114296453,
      "grad_norm": 6.725245952606201,
      "learning_rate": 3.6205446101612346e-06,
      "loss": 1.1987,
      "step": 6920
    },
    {
      "epoch": 0.37204058624577224,
      "grad_norm": 7.165319442749023,
      "learning_rate": 3.6166571171046744e-06,
      "loss": 0.8448,
      "step": 6930
    },
    {
      "epoch": 0.37257744134858,
      "grad_norm": 6.477161407470703,
      "learning_rate": 3.612766248072426e-06,
      "loss": 1.134,
      "step": 6940
    },
    {
      "epoch": 0.3731142964513878,
      "grad_norm": 9.55146598815918,
      "learning_rate": 3.6088720148277047e-06,
      "loss": 1.2794,
      "step": 6950
    },
    {
      "epoch": 0.37365115155419554,
      "grad_norm": 15.76423168182373,
      "learning_rate": 3.604974429143899e-06,
      "loss": 1.4887,
      "step": 6960
    },
    {
      "epoch": 0.3741880066570033,
      "grad_norm": 8.252311706542969,
      "learning_rate": 3.6010735028045313e-06,
      "loss": 1.0018,
      "step": 6970
    },
    {
      "epoch": 0.374724861759811,
      "grad_norm": 8.215311050415039,
      "learning_rate": 3.597169247603224e-06,
      "loss": 1.2952,
      "step": 6980
    },
    {
      "epoch": 0.3752617168626188,
      "grad_norm": 15.390189170837402,
      "learning_rate": 3.5932616753436633e-06,
      "loss": 1.466,
      "step": 6990
    },
    {
      "epoch": 0.37579857196542654,
      "grad_norm": 5.137002944946289,
      "learning_rate": 3.589350797839565e-06,
      "loss": 0.9247,
      "step": 7000
    },
    {
      "epoch": 0.3763354270682343,
      "grad_norm": 6.00876522064209,
      "learning_rate": 3.5854366269146364e-06,
      "loss": 0.6605,
      "step": 7010
    },
    {
      "epoch": 0.376872282171042,
      "grad_norm": 7.127292633056641,
      "learning_rate": 3.581519174402541e-06,
      "loss": 0.8728,
      "step": 7020
    },
    {
      "epoch": 0.3774091372738498,
      "grad_norm": 15.037744522094727,
      "learning_rate": 3.577598452146867e-06,
      "loss": 1.6742,
      "step": 7030
    },
    {
      "epoch": 0.37794599237665755,
      "grad_norm": 5.431376934051514,
      "learning_rate": 3.573674472001084e-06,
      "loss": 1.3105,
      "step": 7040
    },
    {
      "epoch": 0.3784828474794653,
      "grad_norm": 6.964735507965088,
      "learning_rate": 3.569747245828513e-06,
      "loss": 1.0223,
      "step": 7050
    },
    {
      "epoch": 0.379019702582273,
      "grad_norm": 6.909992694854736,
      "learning_rate": 3.5658167855022897e-06,
      "loss": 1.5261,
      "step": 7060
    },
    {
      "epoch": 0.3795565576850808,
      "grad_norm": 5.938471794128418,
      "learning_rate": 3.5618831029053245e-06,
      "loss": 1.2489,
      "step": 7070
    },
    {
      "epoch": 0.38009341278788855,
      "grad_norm": 8.535066604614258,
      "learning_rate": 3.5579462099302726e-06,
      "loss": 1.068,
      "step": 7080
    },
    {
      "epoch": 0.3806302678906963,
      "grad_norm": 5.7000508308410645,
      "learning_rate": 3.5540061184794932e-06,
      "loss": 0.8173,
      "step": 7090
    },
    {
      "epoch": 0.381167122993504,
      "grad_norm": 5.840850353240967,
      "learning_rate": 3.550062840465017e-06,
      "loss": 1.4385,
      "step": 7100
    },
    {
      "epoch": 0.3817039780963118,
      "grad_norm": 6.917598247528076,
      "learning_rate": 3.546116387808507e-06,
      "loss": 0.8166,
      "step": 7110
    },
    {
      "epoch": 0.38224083319911956,
      "grad_norm": 6.453629016876221,
      "learning_rate": 3.5421667724412257e-06,
      "loss": 1.1095,
      "step": 7120
    },
    {
      "epoch": 0.3827776883019273,
      "grad_norm": 4.757483959197998,
      "learning_rate": 3.538214006303996e-06,
      "loss": 0.9876,
      "step": 7130
    },
    {
      "epoch": 0.3833145434047351,
      "grad_norm": 5.638991355895996,
      "learning_rate": 3.5342581013471668e-06,
      "loss": 1.0527,
      "step": 7140
    },
    {
      "epoch": 0.3838513985075428,
      "grad_norm": 6.814233779907227,
      "learning_rate": 3.5302990695305774e-06,
      "loss": 0.9288,
      "step": 7150
    },
    {
      "epoch": 0.38438825361035056,
      "grad_norm": 12.772412300109863,
      "learning_rate": 3.526336922823519e-06,
      "loss": 1.2538,
      "step": 7160
    },
    {
      "epoch": 0.3849251087131583,
      "grad_norm": 6.531960964202881,
      "learning_rate": 3.5223716732047016e-06,
      "loss": 1.2826,
      "step": 7170
    },
    {
      "epoch": 0.3854619638159661,
      "grad_norm": 6.4597578048706055,
      "learning_rate": 3.518403332662215e-06,
      "loss": 0.9213,
      "step": 7180
    },
    {
      "epoch": 0.3859988189187738,
      "grad_norm": 8.034599304199219,
      "learning_rate": 3.5144319131934947e-06,
      "loss": 0.9748,
      "step": 7190
    },
    {
      "epoch": 0.38653567402158157,
      "grad_norm": 7.181233882904053,
      "learning_rate": 3.5104574268052836e-06,
      "loss": 1.4504,
      "step": 7200
    },
    {
      "epoch": 0.38707252912438933,
      "grad_norm": 16.946565628051758,
      "learning_rate": 3.5064798855135984e-06,
      "loss": 1.4265,
      "step": 7210
    },
    {
      "epoch": 0.3876093842271971,
      "grad_norm": 10.453386306762695,
      "learning_rate": 3.5024993013436894e-06,
      "loss": 1.2822,
      "step": 7220
    },
    {
      "epoch": 0.3881462393300048,
      "grad_norm": 6.382622241973877,
      "learning_rate": 3.498515686330008e-06,
      "loss": 0.8473,
      "step": 7230
    },
    {
      "epoch": 0.38868309443281257,
      "grad_norm": 6.078360557556152,
      "learning_rate": 3.494529052516169e-06,
      "loss": 0.8154,
      "step": 7240
    },
    {
      "epoch": 0.38921994953562034,
      "grad_norm": 7.111160755157471,
      "learning_rate": 3.4905394119549125e-06,
      "loss": 0.8272,
      "step": 7250
    },
    {
      "epoch": 0.3897568046384281,
      "grad_norm": 6.3529953956604,
      "learning_rate": 3.48654677670807e-06,
      "loss": 1.018,
      "step": 7260
    },
    {
      "epoch": 0.39029365974123587,
      "grad_norm": 9.240813255310059,
      "learning_rate": 3.4825511588465256e-06,
      "loss": 1.0163,
      "step": 7270
    },
    {
      "epoch": 0.3908305148440436,
      "grad_norm": 6.422667980194092,
      "learning_rate": 3.4785525704501825e-06,
      "loss": 1.1041,
      "step": 7280
    },
    {
      "epoch": 0.39136736994685134,
      "grad_norm": 7.837161064147949,
      "learning_rate": 3.4745510236079232e-06,
      "loss": 1.0534,
      "step": 7290
    },
    {
      "epoch": 0.3919042250496591,
      "grad_norm": 6.506810188293457,
      "learning_rate": 3.4705465304175746e-06,
      "loss": 1.2526,
      "step": 7300
    },
    {
      "epoch": 0.39244108015246687,
      "grad_norm": 14.010235786437988,
      "learning_rate": 3.466539102985873e-06,
      "loss": 1.3739,
      "step": 7310
    },
    {
      "epoch": 0.3929779352552746,
      "grad_norm": 4.4636077880859375,
      "learning_rate": 3.4625287534284224e-06,
      "loss": 1.1527,
      "step": 7320
    },
    {
      "epoch": 0.39351479035808234,
      "grad_norm": 13.947052955627441,
      "learning_rate": 3.458515493869664e-06,
      "loss": 1.0723,
      "step": 7330
    },
    {
      "epoch": 0.3940516454608901,
      "grad_norm": 7.284400939941406,
      "learning_rate": 3.4544993364428363e-06,
      "loss": 1.3235,
      "step": 7340
    },
    {
      "epoch": 0.3945885005636979,
      "grad_norm": 6.541877746582031,
      "learning_rate": 3.4504802932899383e-06,
      "loss": 1.2866,
      "step": 7350
    },
    {
      "epoch": 0.3951253556665056,
      "grad_norm": 5.859488487243652,
      "learning_rate": 3.446458376561693e-06,
      "loss": 0.8158,
      "step": 7360
    },
    {
      "epoch": 0.39566221076931335,
      "grad_norm": 5.667947769165039,
      "learning_rate": 3.4424335984175127e-06,
      "loss": 0.6145,
      "step": 7370
    },
    {
      "epoch": 0.3961990658721211,
      "grad_norm": 4.055055618286133,
      "learning_rate": 3.4384059710254586e-06,
      "loss": 1.0586,
      "step": 7380
    },
    {
      "epoch": 0.3967359209749289,
      "grad_norm": 9.238423347473145,
      "learning_rate": 3.4343755065622075e-06,
      "loss": 0.7898,
      "step": 7390
    },
    {
      "epoch": 0.39727277607773664,
      "grad_norm": 4.55826997756958,
      "learning_rate": 3.4303422172130124e-06,
      "loss": 1.1088,
      "step": 7400
    },
    {
      "epoch": 0.39780963118054435,
      "grad_norm": 12.696539878845215,
      "learning_rate": 3.426306115171667e-06,
      "loss": 1.2367,
      "step": 7410
    },
    {
      "epoch": 0.3983464862833521,
      "grad_norm": 7.366796970367432,
      "learning_rate": 3.42226721264047e-06,
      "loss": 1.0136,
      "step": 7420
    },
    {
      "epoch": 0.3988833413861599,
      "grad_norm": 6.594090938568115,
      "learning_rate": 3.4182255218301853e-06,
      "loss": 1.0199,
      "step": 7430
    },
    {
      "epoch": 0.39942019648896765,
      "grad_norm": 4.732074737548828,
      "learning_rate": 3.414181054960005e-06,
      "loss": 1.0349,
      "step": 7440
    },
    {
      "epoch": 0.39995705159177536,
      "grad_norm": 13.215561866760254,
      "learning_rate": 3.4101338242575184e-06,
      "loss": 0.8068,
      "step": 7450
    },
    {
      "epoch": 0.4004939066945831,
      "grad_norm": 4.427876949310303,
      "learning_rate": 3.406083841958667e-06,
      "loss": 0.9249,
      "step": 7460
    },
    {
      "epoch": 0.4010307617973909,
      "grad_norm": 6.425782203674316,
      "learning_rate": 3.4020311203077123e-06,
      "loss": 1.3636,
      "step": 7470
    },
    {
      "epoch": 0.40156761690019865,
      "grad_norm": 5.951717853546143,
      "learning_rate": 3.3979756715571976e-06,
      "loss": 1.1058,
      "step": 7480
    },
    {
      "epoch": 0.40210447200300636,
      "grad_norm": 6.354161262512207,
      "learning_rate": 3.3939175079679116e-06,
      "loss": 1.1174,
      "step": 7490
    },
    {
      "epoch": 0.40264132710581413,
      "grad_norm": 5.274303913116455,
      "learning_rate": 3.3898566418088496e-06,
      "loss": 0.8335,
      "step": 7500
    },
    {
      "epoch": 0.4031781822086219,
      "grad_norm": 7.011325836181641,
      "learning_rate": 3.385793085357179e-06,
      "loss": 1.1274,
      "step": 7510
    },
    {
      "epoch": 0.40371503731142966,
      "grad_norm": 7.219629764556885,
      "learning_rate": 3.381726850898199e-06,
      "loss": 1.1187,
      "step": 7520
    },
    {
      "epoch": 0.4042518924142374,
      "grad_norm": 4.974544525146484,
      "learning_rate": 3.377657950725307e-06,
      "loss": 0.8275,
      "step": 7530
    },
    {
      "epoch": 0.40478874751704513,
      "grad_norm": 12.928973197937012,
      "learning_rate": 3.3735863971399572e-06,
      "loss": 0.8031,
      "step": 7540
    },
    {
      "epoch": 0.4053256026198529,
      "grad_norm": 13.713397026062012,
      "learning_rate": 3.369512202451629e-06,
      "loss": 1.3894,
      "step": 7550
    },
    {
      "epoch": 0.40586245772266066,
      "grad_norm": 9.99059772491455,
      "learning_rate": 3.365435378977785e-06,
      "loss": 1.0684,
      "step": 7560
    },
    {
      "epoch": 0.40639931282546843,
      "grad_norm": 13.649321556091309,
      "learning_rate": 3.361355939043833e-06,
      "loss": 1.1266,
      "step": 7570
    },
    {
      "epoch": 0.40693616792827614,
      "grad_norm": 5.922238349914551,
      "learning_rate": 3.3572738949830965e-06,
      "loss": 1.0098,
      "step": 7580
    },
    {
      "epoch": 0.4074730230310839,
      "grad_norm": 11.885506629943848,
      "learning_rate": 3.353189259136767e-06,
      "loss": 0.8531,
      "step": 7590
    },
    {
      "epoch": 0.40800987813389167,
      "grad_norm": 4.846322536468506,
      "learning_rate": 3.349102043853874e-06,
      "loss": 0.6209,
      "step": 7600
    },
    {
      "epoch": 0.40854673323669943,
      "grad_norm": 6.769039154052734,
      "learning_rate": 3.345012261491245e-06,
      "loss": 0.9809,
      "step": 7610
    },
    {
      "epoch": 0.40908358833950714,
      "grad_norm": 5.1128740310668945,
      "learning_rate": 3.340919924413469e-06,
      "loss": 0.8612,
      "step": 7620
    },
    {
      "epoch": 0.4096204434423149,
      "grad_norm": 6.599756717681885,
      "learning_rate": 3.3368250449928583e-06,
      "loss": 0.6683,
      "step": 7630
    },
    {
      "epoch": 0.4101572985451227,
      "grad_norm": 9.075181007385254,
      "learning_rate": 3.332727635609411e-06,
      "loss": 0.8461,
      "step": 7640
    },
    {
      "epoch": 0.41069415364793044,
      "grad_norm": 7.2439422607421875,
      "learning_rate": 3.328627708650774e-06,
      "loss": 0.6353,
      "step": 7650
    },
    {
      "epoch": 0.4112310087507382,
      "grad_norm": 7.6226887702941895,
      "learning_rate": 3.3245252765122062e-06,
      "loss": 1.1172,
      "step": 7660
    },
    {
      "epoch": 0.4117678638535459,
      "grad_norm": 14.243639945983887,
      "learning_rate": 3.3204203515965406e-06,
      "loss": 1.7491,
      "step": 7670
    },
    {
      "epoch": 0.4123047189563537,
      "grad_norm": 5.846193790435791,
      "learning_rate": 3.316312946314145e-06,
      "loss": 0.6509,
      "step": 7680
    },
    {
      "epoch": 0.41284157405916144,
      "grad_norm": 8.373210906982422,
      "learning_rate": 3.3122030730828884e-06,
      "loss": 0.611,
      "step": 7690
    },
    {
      "epoch": 0.4133784291619692,
      "grad_norm": 4.798507213592529,
      "learning_rate": 3.308090744328099e-06,
      "loss": 0.789,
      "step": 7700
    },
    {
      "epoch": 0.4139152842647769,
      "grad_norm": 6.225909233093262,
      "learning_rate": 3.3039759724825302e-06,
      "loss": 2.0175,
      "step": 7710
    },
    {
      "epoch": 0.4144521393675847,
      "grad_norm": 6.156948566436768,
      "learning_rate": 3.2998587699863205e-06,
      "loss": 0.6919,
      "step": 7720
    },
    {
      "epoch": 0.41498899447039245,
      "grad_norm": 6.317250728607178,
      "learning_rate": 3.2957391492869566e-06,
      "loss": 1.3024,
      "step": 7730
    },
    {
      "epoch": 0.4155258495732002,
      "grad_norm": 6.011458396911621,
      "learning_rate": 3.2916171228392386e-06,
      "loss": 1.029,
      "step": 7740
    },
    {
      "epoch": 0.4160627046760079,
      "grad_norm": 6.314315319061279,
      "learning_rate": 3.2874927031052377e-06,
      "loss": 0.9962,
      "step": 7750
    },
    {
      "epoch": 0.4165995597788157,
      "grad_norm": 5.141458511352539,
      "learning_rate": 3.28336590255426e-06,
      "loss": 1.1461,
      "step": 7760
    },
    {
      "epoch": 0.41713641488162345,
      "grad_norm": 7.1783294677734375,
      "learning_rate": 3.2792367336628126e-06,
      "loss": 1.7377,
      "step": 7770
    },
    {
      "epoch": 0.4176732699844312,
      "grad_norm": 13.391210556030273,
      "learning_rate": 3.2751052089145597e-06,
      "loss": 1.6056,
      "step": 7780
    },
    {
      "epoch": 0.418210125087239,
      "grad_norm": 6.9083051681518555,
      "learning_rate": 3.270971340800289e-06,
      "loss": 0.864,
      "step": 7790
    },
    {
      "epoch": 0.4187469801900467,
      "grad_norm": 15.448162078857422,
      "learning_rate": 3.2668351418178735e-06,
      "loss": 1.0406,
      "step": 7800
    },
    {
      "epoch": 0.41928383529285446,
      "grad_norm": 8.051915168762207,
      "learning_rate": 3.2626966244722325e-06,
      "loss": 1.1413,
      "step": 7810
    },
    {
      "epoch": 0.4198206903956622,
      "grad_norm": 9.55269718170166,
      "learning_rate": 3.2585558012752943e-06,
      "loss": 1.1617,
      "step": 7820
    },
    {
      "epoch": 0.42035754549847,
      "grad_norm": 14.073532104492188,
      "learning_rate": 3.2544126847459586e-06,
      "loss": 1.2802,
      "step": 7830
    },
    {
      "epoch": 0.4208944006012777,
      "grad_norm": 4.811277389526367,
      "learning_rate": 3.2502672874100584e-06,
      "loss": 0.8846,
      "step": 7840
    },
    {
      "epoch": 0.42143125570408546,
      "grad_norm": 6.619278430938721,
      "learning_rate": 3.2461196218003225e-06,
      "loss": 0.7893,
      "step": 7850
    },
    {
      "epoch": 0.4219681108068932,
      "grad_norm": 5.148118019104004,
      "learning_rate": 3.241969700456337e-06,
      "loss": 1.0149,
      "step": 7860
    },
    {
      "epoch": 0.422504965909701,
      "grad_norm": 13.387690544128418,
      "learning_rate": 3.237817535924509e-06,
      "loss": 1.4499,
      "step": 7870
    },
    {
      "epoch": 0.4230418210125087,
      "grad_norm": 5.4978108406066895,
      "learning_rate": 3.2336631407580248e-06,
      "loss": 1.0466,
      "step": 7880
    },
    {
      "epoch": 0.42357867611531647,
      "grad_norm": 7.373843193054199,
      "learning_rate": 3.2295065275168165e-06,
      "loss": 1.2951,
      "step": 7890
    },
    {
      "epoch": 0.42411553121812423,
      "grad_norm": 8.450349807739258,
      "learning_rate": 3.2253477087675217e-06,
      "loss": 1.1763,
      "step": 7900
    },
    {
      "epoch": 0.424652386320932,
      "grad_norm": 15.04806137084961,
      "learning_rate": 3.2211866970834464e-06,
      "loss": 0.8166,
      "step": 7910
    },
    {
      "epoch": 0.4251892414237397,
      "grad_norm": 9.653497695922852,
      "learning_rate": 3.2170235050445255e-06,
      "loss": 1.0539,
      "step": 7920
    },
    {
      "epoch": 0.42572609652654747,
      "grad_norm": 9.561826705932617,
      "learning_rate": 3.212858145237285e-06,
      "loss": 1.033,
      "step": 7930
    },
    {
      "epoch": 0.42626295162935524,
      "grad_norm": 6.748555660247803,
      "learning_rate": 3.208690630254808e-06,
      "loss": 0.8647,
      "step": 7940
    },
    {
      "epoch": 0.426799806732163,
      "grad_norm": 4.782110691070557,
      "learning_rate": 3.2045209726966895e-06,
      "loss": 1.0937,
      "step": 7950
    },
    {
      "epoch": 0.42733666183497077,
      "grad_norm": 5.785258769989014,
      "learning_rate": 3.200349185169004e-06,
      "loss": 0.9565,
      "step": 7960
    },
    {
      "epoch": 0.4278735169377785,
      "grad_norm": 4.441384792327881,
      "learning_rate": 3.196175280284266e-06,
      "loss": 1.3113,
      "step": 7970
    },
    {
      "epoch": 0.42841037204058624,
      "grad_norm": 7.647723197937012,
      "learning_rate": 3.1919992706613902e-06,
      "loss": 0.6548,
      "step": 7980
    },
    {
      "epoch": 0.428947227143394,
      "grad_norm": 13.294577598571777,
      "learning_rate": 3.187821168925655e-06,
      "loss": 1.0902,
      "step": 7990
    },
    {
      "epoch": 0.42948408224620177,
      "grad_norm": 5.372689723968506,
      "learning_rate": 3.1836409877086638e-06,
      "loss": 1.3198,
      "step": 8000
    },
    {
      "epoch": 0.4300209373490095,
      "grad_norm": 13.624720573425293,
      "learning_rate": 3.179458739648307e-06,
      "loss": 1.4518,
      "step": 8010
    },
    {
      "epoch": 0.43055779245181725,
      "grad_norm": 5.792115211486816,
      "learning_rate": 3.1752744373887227e-06,
      "loss": 1.0651,
      "step": 8020
    },
    {
      "epoch": 0.431094647554625,
      "grad_norm": 6.3531084060668945,
      "learning_rate": 3.1710880935802624e-06,
      "loss": 1.0533,
      "step": 8030
    },
    {
      "epoch": 0.4316315026574328,
      "grad_norm": 13.421025276184082,
      "learning_rate": 3.1668997208794455e-06,
      "loss": 1.2189,
      "step": 8040
    },
    {
      "epoch": 0.4321683577602405,
      "grad_norm": 7.6082305908203125,
      "learning_rate": 3.1627093319489288e-06,
      "loss": 1.0535,
      "step": 8050
    },
    {
      "epoch": 0.43270521286304825,
      "grad_norm": 6.5823187828063965,
      "learning_rate": 3.1585169394574626e-06,
      "loss": 1.2032,
      "step": 8060
    },
    {
      "epoch": 0.433242067965856,
      "grad_norm": 13.519118309020996,
      "learning_rate": 3.1543225560798575e-06,
      "loss": 1.3118,
      "step": 8070
    },
    {
      "epoch": 0.4337789230686638,
      "grad_norm": 5.419600963592529,
      "learning_rate": 3.150126194496938e-06,
      "loss": 1.2894,
      "step": 8080
    },
    {
      "epoch": 0.43431577817147154,
      "grad_norm": 7.544774055480957,
      "learning_rate": 3.145927867395514e-06,
      "loss": 0.7806,
      "step": 8090
    },
    {
      "epoch": 0.43485263327427925,
      "grad_norm": 15.159143447875977,
      "learning_rate": 3.1417275874683357e-06,
      "loss": 1.0386,
      "step": 8100
    },
    {
      "epoch": 0.435389488377087,
      "grad_norm": 5.28191614151001,
      "learning_rate": 3.1375253674140583e-06,
      "loss": 1.3027,
      "step": 8110
    },
    {
      "epoch": 0.4359263434798948,
      "grad_norm": 6.061928749084473,
      "learning_rate": 3.1333212199372006e-06,
      "loss": 1.0102,
      "step": 8120
    },
    {
      "epoch": 0.43646319858270255,
      "grad_norm": 9.4578857421875,
      "learning_rate": 3.1291151577481106e-06,
      "loss": 1.3013,
      "step": 8130
    },
    {
      "epoch": 0.43700005368551026,
      "grad_norm": 6.47383451461792,
      "learning_rate": 3.124907193562925e-06,
      "loss": 1.1411,
      "step": 8140
    },
    {
      "epoch": 0.437536908788318,
      "grad_norm": 8.019689559936523,
      "learning_rate": 3.120697340103528e-06,
      "loss": 1.1994,
      "step": 8150
    },
    {
      "epoch": 0.4380737638911258,
      "grad_norm": 7.016213893890381,
      "learning_rate": 3.1164856100975196e-06,
      "loss": 1.0763,
      "step": 8160
    },
    {
      "epoch": 0.43861061899393355,
      "grad_norm": 7.428996562957764,
      "learning_rate": 3.1122720162781694e-06,
      "loss": 0.9407,
      "step": 8170
    },
    {
      "epoch": 0.43914747409674126,
      "grad_norm": 14.086064338684082,
      "learning_rate": 3.1080565713843857e-06,
      "loss": 0.9885,
      "step": 8180
    },
    {
      "epoch": 0.43968432919954903,
      "grad_norm": 13.369497299194336,
      "learning_rate": 3.1038392881606695e-06,
      "loss": 1.0636,
      "step": 8190
    },
    {
      "epoch": 0.4402211843023568,
      "grad_norm": 7.1421895027160645,
      "learning_rate": 3.0996201793570807e-06,
      "loss": 0.8163,
      "step": 8200
    },
    {
      "epoch": 0.44075803940516456,
      "grad_norm": 6.2819695472717285,
      "learning_rate": 3.0953992577292003e-06,
      "loss": 1.0462,
      "step": 8210
    },
    {
      "epoch": 0.4412948945079723,
      "grad_norm": 4.765186786651611,
      "learning_rate": 3.0911765360380887e-06,
      "loss": 1.0379,
      "step": 8220
    },
    {
      "epoch": 0.44183174961078003,
      "grad_norm": 9.777909278869629,
      "learning_rate": 3.0869520270502473e-06,
      "loss": 1.0819,
      "step": 8230
    },
    {
      "epoch": 0.4423686047135878,
      "grad_norm": 4.489352226257324,
      "learning_rate": 3.0827257435375818e-06,
      "loss": 0.8826,
      "step": 8240
    },
    {
      "epoch": 0.44290545981639556,
      "grad_norm": 9.617182731628418,
      "learning_rate": 3.078497698277364e-06,
      "loss": 1.4524,
      "step": 8250
    },
    {
      "epoch": 0.44344231491920333,
      "grad_norm": 7.790722370147705,
      "learning_rate": 3.0742679040521908e-06,
      "loss": 1.2826,
      "step": 8260
    },
    {
      "epoch": 0.44397917002201104,
      "grad_norm": 13.48586654663086,
      "learning_rate": 3.0700363736499465e-06,
      "loss": 1.2471,
      "step": 8270
    },
    {
      "epoch": 0.4445160251248188,
      "grad_norm": 15.685275077819824,
      "learning_rate": 3.065803119863765e-06,
      "loss": 1.1114,
      "step": 8280
    },
    {
      "epoch": 0.44505288022762657,
      "grad_norm": 4.37431526184082,
      "learning_rate": 3.06156815549199e-06,
      "loss": 0.6111,
      "step": 8290
    },
    {
      "epoch": 0.44558973533043433,
      "grad_norm": 4.879363536834717,
      "learning_rate": 3.057331493338138e-06,
      "loss": 0.8288,
      "step": 8300
    },
    {
      "epoch": 0.44612659043324204,
      "grad_norm": 6.886136054992676,
      "learning_rate": 3.053093146210856e-06,
      "loss": 1.117,
      "step": 8310
    },
    {
      "epoch": 0.4466634455360498,
      "grad_norm": 13.912681579589844,
      "learning_rate": 3.0488531269238874e-06,
      "loss": 0.9849,
      "step": 8320
    },
    {
      "epoch": 0.4472003006388576,
      "grad_norm": 5.316504955291748,
      "learning_rate": 3.04461144829603e-06,
      "loss": 1.0391,
      "step": 8330
    },
    {
      "epoch": 0.44773715574166534,
      "grad_norm": 5.916789531707764,
      "learning_rate": 3.040368123151098e-06,
      "loss": 1.62,
      "step": 8340
    },
    {
      "epoch": 0.4482740108444731,
      "grad_norm": 5.085758209228516,
      "learning_rate": 3.036123164317886e-06,
      "loss": 0.8633,
      "step": 8350
    },
    {
      "epoch": 0.4488108659472808,
      "grad_norm": 5.440527439117432,
      "learning_rate": 3.0318765846301234e-06,
      "loss": 0.764,
      "step": 8360
    },
    {
      "epoch": 0.4493477210500886,
      "grad_norm": 16.20299530029297,
      "learning_rate": 3.0276283969264424e-06,
      "loss": 0.9262,
      "step": 8370
    },
    {
      "epoch": 0.44988457615289634,
      "grad_norm": 8.615994453430176,
      "learning_rate": 3.0233786140503386e-06,
      "loss": 0.6383,
      "step": 8380
    },
    {
      "epoch": 0.4504214312557041,
      "grad_norm": 5.990074634552002,
      "learning_rate": 3.0191272488501257e-06,
      "loss": 0.9084,
      "step": 8390
    },
    {
      "epoch": 0.4509582863585118,
      "grad_norm": 8.876919746398926,
      "learning_rate": 3.0148743141789043e-06,
      "loss": 0.9203,
      "step": 8400
    },
    {
      "epoch": 0.4514951414613196,
      "grad_norm": 14.23562240600586,
      "learning_rate": 3.01061982289452e-06,
      "loss": 1.0704,
      "step": 8410
    },
    {
      "epoch": 0.45203199656412735,
      "grad_norm": 4.706974983215332,
      "learning_rate": 3.0063637878595235e-06,
      "loss": 0.6396,
      "step": 8420
    },
    {
      "epoch": 0.4525688516669351,
      "grad_norm": 7.573390483856201,
      "learning_rate": 3.0021062219411333e-06,
      "loss": 0.8722,
      "step": 8430
    },
    {
      "epoch": 0.4531057067697428,
      "grad_norm": 6.700966835021973,
      "learning_rate": 2.9978471380111952e-06,
      "loss": 1.2815,
      "step": 8440
    },
    {
      "epoch": 0.4536425618725506,
      "grad_norm": 6.776036739349365,
      "learning_rate": 2.993586548946146e-06,
      "loss": 1.112,
      "step": 8450
    },
    {
      "epoch": 0.45417941697535835,
      "grad_norm": 8.76427173614502,
      "learning_rate": 2.989324467626971e-06,
      "loss": 0.8624,
      "step": 8460
    },
    {
      "epoch": 0.4547162720781661,
      "grad_norm": 7.917606353759766,
      "learning_rate": 2.985060906939169e-06,
      "loss": 0.7923,
      "step": 8470
    },
    {
      "epoch": 0.4552531271809739,
      "grad_norm": 5.39255428314209,
      "learning_rate": 2.98079587977271e-06,
      "loss": 0.8665,
      "step": 8480
    },
    {
      "epoch": 0.4557899822837816,
      "grad_norm": 5.295314788818359,
      "learning_rate": 2.9765293990219977e-06,
      "loss": 1.0379,
      "step": 8490
    },
    {
      "epoch": 0.45632683738658936,
      "grad_norm": 6.090220928192139,
      "learning_rate": 2.9722614775858328e-06,
      "loss": 1.0946,
      "step": 8500
    },
    {
      "epoch": 0.4568636924893971,
      "grad_norm": 13.752340316772461,
      "learning_rate": 2.9679921283673663e-06,
      "loss": 1.4609,
      "step": 8510
    },
    {
      "epoch": 0.4574005475922049,
      "grad_norm": 5.422159194946289,
      "learning_rate": 2.9637213642740707e-06,
      "loss": 1.2311,
      "step": 8520
    },
    {
      "epoch": 0.4579374026950126,
      "grad_norm": 5.118160247802734,
      "learning_rate": 2.959449198217695e-06,
      "loss": 1.4937,
      "step": 8530
    },
    {
      "epoch": 0.45847425779782036,
      "grad_norm": 7.013083457946777,
      "learning_rate": 2.9551756431142253e-06,
      "loss": 1.455,
      "step": 8540
    },
    {
      "epoch": 0.4590111129006281,
      "grad_norm": 4.98638916015625,
      "learning_rate": 2.9509007118838485e-06,
      "loss": 0.7972,
      "step": 8550
    },
    {
      "epoch": 0.4595479680034359,
      "grad_norm": 4.9389543533325195,
      "learning_rate": 2.9466244174509106e-06,
      "loss": 1.0719,
      "step": 8560
    },
    {
      "epoch": 0.4600848231062436,
      "grad_norm": 14.276679039001465,
      "learning_rate": 2.9423467727438805e-06,
      "loss": 1.0612,
      "step": 8570
    },
    {
      "epoch": 0.46062167820905137,
      "grad_norm": 10.387727737426758,
      "learning_rate": 2.9380677906953087e-06,
      "loss": 1.0483,
      "step": 8580
    },
    {
      "epoch": 0.46115853331185913,
      "grad_norm": 8.601621627807617,
      "learning_rate": 2.933787484241789e-06,
      "loss": 1.7042,
      "step": 8590
    },
    {
      "epoch": 0.4616953884146669,
      "grad_norm": 8.846522331237793,
      "learning_rate": 2.9295058663239185e-06,
      "loss": 1.2368,
      "step": 8600
    },
    {
      "epoch": 0.46223224351747466,
      "grad_norm": 7.508945465087891,
      "learning_rate": 2.92522294988626e-06,
      "loss": 1.017,
      "step": 8610
    },
    {
      "epoch": 0.46276909862028237,
      "grad_norm": 14.352953910827637,
      "learning_rate": 2.920938747877302e-06,
      "loss": 1.7619,
      "step": 8620
    },
    {
      "epoch": 0.46330595372309014,
      "grad_norm": 15.417244911193848,
      "learning_rate": 2.91665327324942e-06,
      "loss": 1.4116,
      "step": 8630
    },
    {
      "epoch": 0.4638428088258979,
      "grad_norm": 5.905208587646484,
      "learning_rate": 2.9123665389588364e-06,
      "loss": 1.2536,
      "step": 8640
    },
    {
      "epoch": 0.46437966392870567,
      "grad_norm": 5.763683795928955,
      "learning_rate": 2.9080785579655813e-06,
      "loss": 0.9175,
      "step": 8650
    },
    {
      "epoch": 0.4649165190315134,
      "grad_norm": 6.967824935913086,
      "learning_rate": 2.9037893432334563e-06,
      "loss": 0.8137,
      "step": 8660
    },
    {
      "epoch": 0.46545337413432114,
      "grad_norm": 6.0751118659973145,
      "learning_rate": 2.8994989077299902e-06,
      "loss": 1.3179,
      "step": 8670
    },
    {
      "epoch": 0.4659902292371289,
      "grad_norm": 13.38095474243164,
      "learning_rate": 2.8952072644264035e-06,
      "loss": 2.2477,
      "step": 8680
    },
    {
      "epoch": 0.46652708433993667,
      "grad_norm": 13.883898735046387,
      "learning_rate": 2.8909144262975696e-06,
      "loss": 1.5232,
      "step": 8690
    },
    {
      "epoch": 0.4670639394427444,
      "grad_norm": 13.699193954467773,
      "learning_rate": 2.8866204063219726e-06,
      "loss": 0.8771,
      "step": 8700
    },
    {
      "epoch": 0.46760079454555215,
      "grad_norm": 6.6185526847839355,
      "learning_rate": 2.8823252174816697e-06,
      "loss": 1.4851,
      "step": 8710
    },
    {
      "epoch": 0.4681376496483599,
      "grad_norm": 5.803572654724121,
      "learning_rate": 2.8780288727622525e-06,
      "loss": 1.2309,
      "step": 8720
    },
    {
      "epoch": 0.4686745047511677,
      "grad_norm": 15.1759033203125,
      "learning_rate": 2.8737313851528072e-06,
      "loss": 0.8749,
      "step": 8730
    },
    {
      "epoch": 0.4692113598539754,
      "grad_norm": 5.809321880340576,
      "learning_rate": 2.8694327676458757e-06,
      "loss": 1.6008,
      "step": 8740
    },
    {
      "epoch": 0.46974821495678315,
      "grad_norm": 5.8425421714782715,
      "learning_rate": 2.8651330332374138e-06,
      "loss": 0.8292,
      "step": 8750
    },
    {
      "epoch": 0.4702850700595909,
      "grad_norm": 6.753209114074707,
      "learning_rate": 2.8608321949267564e-06,
      "loss": 1.2469,
      "step": 8760
    },
    {
      "epoch": 0.4708219251623987,
      "grad_norm": 16.480758666992188,
      "learning_rate": 2.856530265716575e-06,
      "loss": 1.2593,
      "step": 8770
    },
    {
      "epoch": 0.47135878026520645,
      "grad_norm": 5.503910064697266,
      "learning_rate": 2.8522272586128397e-06,
      "loss": 0.8623,
      "step": 8780
    },
    {
      "epoch": 0.47189563536801415,
      "grad_norm": 6.338234901428223,
      "learning_rate": 2.847923186624777e-06,
      "loss": 0.8724,
      "step": 8790
    },
    {
      "epoch": 0.4724324904708219,
      "grad_norm": 7.611451148986816,
      "learning_rate": 2.843618062764836e-06,
      "loss": 1.2689,
      "step": 8800
    },
    {
      "epoch": 0.4729693455736297,
      "grad_norm": 6.86128044128418,
      "learning_rate": 2.8393119000486443e-06,
      "loss": 1.174,
      "step": 8810
    },
    {
      "epoch": 0.47350620067643745,
      "grad_norm": 5.686656951904297,
      "learning_rate": 2.8350047114949703e-06,
      "loss": 1.0469,
      "step": 8820
    },
    {
      "epoch": 0.47404305577924516,
      "grad_norm": 4.990889072418213,
      "learning_rate": 2.8306965101256846e-06,
      "loss": 1.1025,
      "step": 8830
    },
    {
      "epoch": 0.4745799108820529,
      "grad_norm": 6.497244834899902,
      "learning_rate": 2.8263873089657186e-06,
      "loss": 1.012,
      "step": 8840
    },
    {
      "epoch": 0.4751167659848607,
      "grad_norm": 6.101356029510498,
      "learning_rate": 2.822077121043027e-06,
      "loss": 1.1054,
      "step": 8850
    },
    {
      "epoch": 0.47565362108766845,
      "grad_norm": 7.093038558959961,
      "learning_rate": 2.8177659593885486e-06,
      "loss": 1.433,
      "step": 8860
    },
    {
      "epoch": 0.47619047619047616,
      "grad_norm": 4.789013862609863,
      "learning_rate": 2.813453837036164e-06,
      "loss": 1.3347,
      "step": 8870
    },
    {
      "epoch": 0.47672733129328393,
      "grad_norm": 8.099120140075684,
      "learning_rate": 2.809140767022661e-06,
      "loss": 1.0866,
      "step": 8880
    },
    {
      "epoch": 0.4772641863960917,
      "grad_norm": 13.19403076171875,
      "learning_rate": 2.8048267623876896e-06,
      "loss": 1.8879,
      "step": 8890
    },
    {
      "epoch": 0.47780104149889946,
      "grad_norm": 5.094458103179932,
      "learning_rate": 2.8005118361737276e-06,
      "loss": 1.0088,
      "step": 8900
    },
    {
      "epoch": 0.4783378966017072,
      "grad_norm": 14.920623779296875,
      "learning_rate": 2.796196001426038e-06,
      "loss": 1.0877,
      "step": 8910
    },
    {
      "epoch": 0.47887475170451493,
      "grad_norm": 14.863776206970215,
      "learning_rate": 2.7918792711926307e-06,
      "loss": 1.0661,
      "step": 8920
    },
    {
      "epoch": 0.4794116068073227,
      "grad_norm": 5.470398902893066,
      "learning_rate": 2.787561658524223e-06,
      "loss": 1.2764,
      "step": 8930
    },
    {
      "epoch": 0.47994846191013046,
      "grad_norm": 5.766788959503174,
      "learning_rate": 2.7832431764742015e-06,
      "loss": 0.8507,
      "step": 8940
    },
    {
      "epoch": 0.48048531701293823,
      "grad_norm": 25.32003402709961,
      "learning_rate": 2.7789238380985765e-06,
      "loss": 1.1618,
      "step": 8950
    },
    {
      "epoch": 0.48102217211574594,
      "grad_norm": 6.201385974884033,
      "learning_rate": 2.7746036564559525e-06,
      "loss": 1.5303,
      "step": 8960
    },
    {
      "epoch": 0.4815590272185537,
      "grad_norm": 7.039514064788818,
      "learning_rate": 2.7702826446074816e-06,
      "loss": 1.1878,
      "step": 8970
    },
    {
      "epoch": 0.48209588232136147,
      "grad_norm": 5.601466655731201,
      "learning_rate": 2.7659608156168248e-06,
      "loss": 0.8973,
      "step": 8980
    },
    {
      "epoch": 0.48263273742416923,
      "grad_norm": 8.506794929504395,
      "learning_rate": 2.761638182550115e-06,
      "loss": 1.3379,
      "step": 8990
    },
    {
      "epoch": 0.48316959252697694,
      "grad_norm": 4.990551948547363,
      "learning_rate": 2.7573147584759146e-06,
      "loss": 1.4305,
      "step": 9000
    },
    {
      "epoch": 0.4837064476297847,
      "grad_norm": 16.103673934936523,
      "learning_rate": 2.7529905564651793e-06,
      "loss": 1.0015,
      "step": 9010
    },
    {
      "epoch": 0.4842433027325925,
      "grad_norm": 5.858074188232422,
      "learning_rate": 2.748665589591215e-06,
      "loss": 1.336,
      "step": 9020
    },
    {
      "epoch": 0.48478015783540024,
      "grad_norm": 6.731173992156982,
      "learning_rate": 2.744339870929641e-06,
      "loss": 0.8517,
      "step": 9030
    },
    {
      "epoch": 0.485317012938208,
      "grad_norm": 7.600436210632324,
      "learning_rate": 2.7400134135583487e-06,
      "loss": 1.2465,
      "step": 9040
    },
    {
      "epoch": 0.4858538680410157,
      "grad_norm": 6.899526119232178,
      "learning_rate": 2.735686230557464e-06,
      "loss": 0.9554,
      "step": 9050
    },
    {
      "epoch": 0.4863907231438235,
      "grad_norm": 8.104578971862793,
      "learning_rate": 2.731358335009306e-06,
      "loss": 0.8911,
      "step": 9060
    },
    {
      "epoch": 0.48692757824663124,
      "grad_norm": 3.9029831886291504,
      "learning_rate": 2.727029739998347e-06,
      "loss": 1.2492,
      "step": 9070
    },
    {
      "epoch": 0.487464433349439,
      "grad_norm": 7.332837104797363,
      "learning_rate": 2.722700458611175e-06,
      "loss": 1.0941,
      "step": 9080
    },
    {
      "epoch": 0.4880012884522467,
      "grad_norm": 5.136799335479736,
      "learning_rate": 2.7183705039364537e-06,
      "loss": 1.4759,
      "step": 9090
    },
    {
      "epoch": 0.4885381435550545,
      "grad_norm": 5.5781331062316895,
      "learning_rate": 2.714039889064882e-06,
      "loss": 0.6203,
      "step": 9100
    },
    {
      "epoch": 0.48907499865786225,
      "grad_norm": 13.513917922973633,
      "learning_rate": 2.7097086270891533e-06,
      "loss": 0.9831,
      "step": 9110
    },
    {
      "epoch": 0.48961185376067,
      "grad_norm": 6.96962308883667,
      "learning_rate": 2.7053767311039186e-06,
      "loss": 1.1232,
      "step": 9120
    },
    {
      "epoch": 0.4901487088634777,
      "grad_norm": 5.68367338180542,
      "learning_rate": 2.7010442142057466e-06,
      "loss": 1.0134,
      "step": 9130
    },
    {
      "epoch": 0.4906855639662855,
      "grad_norm": 4.899198532104492,
      "learning_rate": 2.6967110894930814e-06,
      "loss": 1.0194,
      "step": 9140
    },
    {
      "epoch": 0.49122241906909325,
      "grad_norm": 14.799491882324219,
      "learning_rate": 2.692377370066206e-06,
      "loss": 1.2725,
      "step": 9150
    },
    {
      "epoch": 0.491759274171901,
      "grad_norm": 5.474850654602051,
      "learning_rate": 2.6880430690271997e-06,
      "loss": 0.6603,
      "step": 9160
    },
    {
      "epoch": 0.4922961292747088,
      "grad_norm": 14.193864822387695,
      "learning_rate": 2.683708199479903e-06,
      "loss": 0.7897,
      "step": 9170
    },
    {
      "epoch": 0.4928329843775165,
      "grad_norm": 5.760058403015137,
      "learning_rate": 2.6793727745298727e-06,
      "loss": 1.1785,
      "step": 9180
    },
    {
      "epoch": 0.49336983948032426,
      "grad_norm": 6.650750160217285,
      "learning_rate": 2.675036807284345e-06,
      "loss": 0.8139,
      "step": 9190
    },
    {
      "epoch": 0.493906694583132,
      "grad_norm": 4.7902607917785645,
      "learning_rate": 2.670700310852198e-06,
      "loss": 1.002,
      "step": 9200
    },
    {
      "epoch": 0.4944435496859398,
      "grad_norm": 6.029917240142822,
      "learning_rate": 2.6663632983439064e-06,
      "loss": 1.3122,
      "step": 9210
    },
    {
      "epoch": 0.4949804047887475,
      "grad_norm": 7.9476637840271,
      "learning_rate": 2.662025782871506e-06,
      "loss": 1.3819,
      "step": 9220
    },
    {
      "epoch": 0.49551725989155526,
      "grad_norm": 7.198119640350342,
      "learning_rate": 2.657687777548556e-06,
      "loss": 1.1977,
      "step": 9230
    },
    {
      "epoch": 0.496054114994363,
      "grad_norm": 7.108974933624268,
      "learning_rate": 2.6533492954900923e-06,
      "loss": 0.8688,
      "step": 9240
    },
    {
      "epoch": 0.4965909700971708,
      "grad_norm": 5.569089889526367,
      "learning_rate": 2.6490103498125953e-06,
      "loss": 0.9967,
      "step": 9250
    },
    {
      "epoch": 0.4971278251999785,
      "grad_norm": 5.307986736297607,
      "learning_rate": 2.6446709536339464e-06,
      "loss": 1.2604,
      "step": 9260
    },
    {
      "epoch": 0.49766468030278627,
      "grad_norm": 5.181243419647217,
      "learning_rate": 2.640331120073387e-06,
      "loss": 1.4797,
      "step": 9270
    },
    {
      "epoch": 0.49820153540559403,
      "grad_norm": 6.483076572418213,
      "learning_rate": 2.6359908622514836e-06,
      "loss": 0.6561,
      "step": 9280
    },
    {
      "epoch": 0.4987383905084018,
      "grad_norm": 5.926863193511963,
      "learning_rate": 2.6316501932900836e-06,
      "loss": 0.7234,
      "step": 9290
    },
    {
      "epoch": 0.49927524561120956,
      "grad_norm": 4.85757303237915,
      "learning_rate": 2.6273091263122787e-06,
      "loss": 1.1774,
      "step": 9300
    },
    {
      "epoch": 0.49981210071401727,
      "grad_norm": 14.247601509094238,
      "learning_rate": 2.6229676744423617e-06,
      "loss": 1.5183,
      "step": 9310
    },
    {
      "epoch": 0.500348955816825,
      "grad_norm": 8.299243927001953,
      "learning_rate": 2.6186258508057915e-06,
      "loss": 1.0759,
      "step": 9320
    },
    {
      "epoch": 0.5008858109196328,
      "grad_norm": 13.628233909606934,
      "learning_rate": 2.61428366852915e-06,
      "loss": 1.3081,
      "step": 9330
    },
    {
      "epoch": 0.5014226660224406,
      "grad_norm": 6.781693458557129,
      "learning_rate": 2.609941140740103e-06,
      "loss": 1.0562,
      "step": 9340
    },
    {
      "epoch": 0.5019595211252483,
      "grad_norm": 6.7921552658081055,
      "learning_rate": 2.6055982805673615e-06,
      "loss": 0.8365,
      "step": 9350
    },
    {
      "epoch": 0.5024963762280561,
      "grad_norm": 5.792592525482178,
      "learning_rate": 2.6012551011406406e-06,
      "loss": 0.8947,
      "step": 9360
    },
    {
      "epoch": 0.5030332313308638,
      "grad_norm": 4.700439453125,
      "learning_rate": 2.5969116155906216e-06,
      "loss": 0.8611,
      "step": 9370
    },
    {
      "epoch": 0.5035700864336715,
      "grad_norm": 10.391050338745117,
      "learning_rate": 2.5925678370489103e-06,
      "loss": 0.8875,
      "step": 9380
    },
    {
      "epoch": 0.5041069415364793,
      "grad_norm": 6.474730968475342,
      "learning_rate": 2.588223778647999e-06,
      "loss": 0.8481,
      "step": 9390
    },
    {
      "epoch": 0.504643796639287,
      "grad_norm": 11.23221492767334,
      "learning_rate": 2.5838794535212262e-06,
      "loss": 1.0298,
      "step": 9400
    },
    {
      "epoch": 0.5051806517420948,
      "grad_norm": 13.635296821594238,
      "learning_rate": 2.579534874802736e-06,
      "loss": 1.0182,
      "step": 9410
    },
    {
      "epoch": 0.5057175068449026,
      "grad_norm": 5.06599235534668,
      "learning_rate": 2.57519005562744e-06,
      "loss": 0.8952,
      "step": 9420
    },
    {
      "epoch": 0.5062543619477103,
      "grad_norm": 5.236781597137451,
      "learning_rate": 2.570845009130976e-06,
      "loss": 0.9294,
      "step": 9430
    },
    {
      "epoch": 0.5067912170505181,
      "grad_norm": 10.683716773986816,
      "learning_rate": 2.5664997484496697e-06,
      "loss": 1.0637,
      "step": 9440
    },
    {
      "epoch": 0.5073280721533259,
      "grad_norm": 4.415614604949951,
      "learning_rate": 2.5621542867204945e-06,
      "loss": 0.6422,
      "step": 9450
    },
    {
      "epoch": 0.5078649272561335,
      "grad_norm": 5.092986583709717,
      "learning_rate": 2.5578086370810308e-06,
      "loss": 0.8282,
      "step": 9460
    },
    {
      "epoch": 0.5084017823589413,
      "grad_norm": 5.710644721984863,
      "learning_rate": 2.553462812669428e-06,
      "loss": 1.2372,
      "step": 9470
    },
    {
      "epoch": 0.508938637461749,
      "grad_norm": 6.715170383453369,
      "learning_rate": 2.549116826624362e-06,
      "loss": 1.0339,
      "step": 9480
    },
    {
      "epoch": 0.5094754925645568,
      "grad_norm": 9.063458442687988,
      "learning_rate": 2.544770692085001e-06,
      "loss": 0.8563,
      "step": 9490
    },
    {
      "epoch": 0.5100123476673646,
      "grad_norm": 6.3464860916137695,
      "learning_rate": 2.5404244221909586e-06,
      "loss": 1.0873,
      "step": 9500
    },
    {
      "epoch": 0.5105492027701724,
      "grad_norm": 5.757299423217773,
      "learning_rate": 2.536078030082259e-06,
      "loss": 0.8599,
      "step": 9510
    },
    {
      "epoch": 0.5110860578729801,
      "grad_norm": 6.454016208648682,
      "learning_rate": 2.531731528899296e-06,
      "loss": 0.7956,
      "step": 9520
    },
    {
      "epoch": 0.5116229129757879,
      "grad_norm": 6.02449369430542,
      "learning_rate": 2.5273849317827938e-06,
      "loss": 1.093,
      "step": 9530
    },
    {
      "epoch": 0.5121597680785955,
      "grad_norm": 15.334836959838867,
      "learning_rate": 2.5230382518737646e-06,
      "loss": 1.2814,
      "step": 9540
    },
    {
      "epoch": 0.5126966231814033,
      "grad_norm": 5.372366428375244,
      "learning_rate": 2.518691502313472e-06,
      "loss": 1.1824,
      "step": 9550
    },
    {
      "epoch": 0.5132334782842111,
      "grad_norm": 7.319975852966309,
      "learning_rate": 2.514344696243392e-06,
      "loss": 1.1357,
      "step": 9560
    },
    {
      "epoch": 0.5137703333870188,
      "grad_norm": 6.582172870635986,
      "learning_rate": 2.509997846805169e-06,
      "loss": 1.2812,
      "step": 9570
    },
    {
      "epoch": 0.5143071884898266,
      "grad_norm": 4.069430351257324,
      "learning_rate": 2.5056509671405784e-06,
      "loss": 0.9976,
      "step": 9580
    },
    {
      "epoch": 0.5148440435926344,
      "grad_norm": 6.521454334259033,
      "learning_rate": 2.5013040703914883e-06,
      "loss": 1.0681,
      "step": 9590
    },
    {
      "epoch": 0.5153808986954421,
      "grad_norm": 9.823486328125,
      "learning_rate": 2.4969571696998186e-06,
      "loss": 0.8507,
      "step": 9600
    },
    {
      "epoch": 0.5159177537982499,
      "grad_norm": 4.95862340927124,
      "learning_rate": 2.4926102782075e-06,
      "loss": 1.7461,
      "step": 9610
    },
    {
      "epoch": 0.5164546089010577,
      "grad_norm": 14.465418815612793,
      "learning_rate": 2.488263409056437e-06,
      "loss": 1.0893,
      "step": 9620
    },
    {
      "epoch": 0.5169914640038653,
      "grad_norm": 13.26216983795166,
      "learning_rate": 2.4839165753884634e-06,
      "loss": 1.1536,
      "step": 9630
    },
    {
      "epoch": 0.5175283191066731,
      "grad_norm": 6.435298442840576,
      "learning_rate": 2.47956979034531e-06,
      "loss": 0.9448,
      "step": 9640
    },
    {
      "epoch": 0.5180651742094808,
      "grad_norm": 14.428033828735352,
      "learning_rate": 2.475223067068557e-06,
      "loss": 0.926,
      "step": 9650
    },
    {
      "epoch": 0.5186020293122886,
      "grad_norm": 17.441368103027344,
      "learning_rate": 2.4708764186996002e-06,
      "loss": 1.389,
      "step": 9660
    },
    {
      "epoch": 0.5191388844150964,
      "grad_norm": 4.637203693389893,
      "learning_rate": 2.4665298583796064e-06,
      "loss": 1.4342,
      "step": 9670
    },
    {
      "epoch": 0.5196757395179041,
      "grad_norm": 15.724663734436035,
      "learning_rate": 2.4621833992494804e-06,
      "loss": 0.8645,
      "step": 9680
    },
    {
      "epoch": 0.5202125946207119,
      "grad_norm": 9.625112533569336,
      "learning_rate": 2.4578370544498164e-06,
      "loss": 0.864,
      "step": 9690
    },
    {
      "epoch": 0.5207494497235197,
      "grad_norm": 6.728978633880615,
      "learning_rate": 2.4534908371208657e-06,
      "loss": 1.2308,
      "step": 9700
    },
    {
      "epoch": 0.5212863048263274,
      "grad_norm": 6.635209560394287,
      "learning_rate": 2.4491447604024924e-06,
      "loss": 0.8617,
      "step": 9710
    },
    {
      "epoch": 0.5218231599291351,
      "grad_norm": 5.731926918029785,
      "learning_rate": 2.4447988374341393e-06,
      "loss": 1.1633,
      "step": 9720
    },
    {
      "epoch": 0.5223600150319428,
      "grad_norm": 14.250811576843262,
      "learning_rate": 2.440453081354779e-06,
      "loss": 1.0918,
      "step": 9730
    },
    {
      "epoch": 0.5228968701347506,
      "grad_norm": 6.089971542358398,
      "learning_rate": 2.4361075053028844e-06,
      "loss": 0.9989,
      "step": 9740
    },
    {
      "epoch": 0.5234337252375584,
      "grad_norm": 5.811034679412842,
      "learning_rate": 2.431762122416379e-06,
      "loss": 0.81,
      "step": 9750
    },
    {
      "epoch": 0.5239705803403661,
      "grad_norm": 7.727179050445557,
      "learning_rate": 2.427416945832609e-06,
      "loss": 1.3868,
      "step": 9760
    },
    {
      "epoch": 0.5245074354431739,
      "grad_norm": 7.159693717956543,
      "learning_rate": 2.4230719886882907e-06,
      "loss": 0.5712,
      "step": 9770
    },
    {
      "epoch": 0.5250442905459817,
      "grad_norm": 11.708717346191406,
      "learning_rate": 2.418727264119481e-06,
      "loss": 0.9225,
      "step": 9780
    },
    {
      "epoch": 0.5255811456487894,
      "grad_norm": 5.6728596687316895,
      "learning_rate": 2.4143827852615297e-06,
      "loss": 1.2411,
      "step": 9790
    },
    {
      "epoch": 0.5261180007515971,
      "grad_norm": 7.466031074523926,
      "learning_rate": 2.4100385652490487e-06,
      "loss": 0.8303,
      "step": 9800
    },
    {
      "epoch": 0.5266548558544049,
      "grad_norm": 8.255541801452637,
      "learning_rate": 2.4056946172158633e-06,
      "loss": 0.8249,
      "step": 9810
    },
    {
      "epoch": 0.5271917109572126,
      "grad_norm": 14.389863967895508,
      "learning_rate": 2.401350954294979e-06,
      "loss": 1.4965,
      "step": 9820
    },
    {
      "epoch": 0.5277285660600204,
      "grad_norm": 6.184101581573486,
      "learning_rate": 2.3970075896185365e-06,
      "loss": 0.9726,
      "step": 9830
    },
    {
      "epoch": 0.5282654211628282,
      "grad_norm": 5.858983516693115,
      "learning_rate": 2.392664536317779e-06,
      "loss": 0.8279,
      "step": 9840
    },
    {
      "epoch": 0.5288022762656359,
      "grad_norm": 5.418534755706787,
      "learning_rate": 2.388321807523005e-06,
      "loss": 0.6051,
      "step": 9850
    },
    {
      "epoch": 0.5293391313684437,
      "grad_norm": 15.70361614227295,
      "learning_rate": 2.383979416363532e-06,
      "loss": 1.1741,
      "step": 9860
    },
    {
      "epoch": 0.5298759864712514,
      "grad_norm": 8.53310775756836,
      "learning_rate": 2.379637375967659e-06,
      "loss": 1.6329,
      "step": 9870
    },
    {
      "epoch": 0.5304128415740592,
      "grad_norm": 5.116423606872559,
      "learning_rate": 2.3752956994626224e-06,
      "loss": 1.2267,
      "step": 9880
    },
    {
      "epoch": 0.5309496966768669,
      "grad_norm": 5.640902996063232,
      "learning_rate": 2.3709543999745602e-06,
      "loss": 0.8655,
      "step": 9890
    },
    {
      "epoch": 0.5314865517796746,
      "grad_norm": 7.251457691192627,
      "learning_rate": 2.3666134906284675e-06,
      "loss": 0.9526,
      "step": 9900
    },
    {
      "epoch": 0.5320234068824824,
      "grad_norm": 6.20330810546875,
      "learning_rate": 2.3622729845481642e-06,
      "loss": 1.048,
      "step": 9910
    },
    {
      "epoch": 0.5325602619852902,
      "grad_norm": 7.451358318328857,
      "learning_rate": 2.357932894856247e-06,
      "loss": 1.0077,
      "step": 9920
    },
    {
      "epoch": 0.5330971170880979,
      "grad_norm": 5.705124378204346,
      "learning_rate": 2.3535932346740583e-06,
      "loss": 0.9203,
      "step": 9930
    },
    {
      "epoch": 0.5336339721909057,
      "grad_norm": 5.153573513031006,
      "learning_rate": 2.3492540171216357e-06,
      "loss": 0.9224,
      "step": 9940
    },
    {
      "epoch": 0.5341708272937135,
      "grad_norm": 4.571021556854248,
      "learning_rate": 2.3449152553176845e-06,
      "loss": 0.9732,
      "step": 9950
    },
    {
      "epoch": 0.5347076823965212,
      "grad_norm": 7.386058330535889,
      "learning_rate": 2.3405769623795277e-06,
      "loss": 0.7306,
      "step": 9960
    },
    {
      "epoch": 0.5352445374993289,
      "grad_norm": 5.412652015686035,
      "learning_rate": 2.3362391514230756e-06,
      "loss": 0.8398,
      "step": 9970
    },
    {
      "epoch": 0.5357813926021366,
      "grad_norm": 5.483310222625732,
      "learning_rate": 2.3319018355627764e-06,
      "loss": 1.0431,
      "step": 9980
    },
    {
      "epoch": 0.5363182477049444,
      "grad_norm": 6.073135852813721,
      "learning_rate": 2.3275650279115847e-06,
      "loss": 1.4188,
      "step": 9990
    },
    {
      "epoch": 0.5368551028077522,
      "grad_norm": 6.084858417510986,
      "learning_rate": 2.3232287415809164e-06,
      "loss": 0.8893,
      "step": 10000
    },
    {
      "epoch": 0.5373919579105599,
      "grad_norm": 8.443501472473145,
      "learning_rate": 2.318892989680614e-06,
      "loss": 1.0515,
      "step": 10010
    },
    {
      "epoch": 0.5379288130133677,
      "grad_norm": 13.431053161621094,
      "learning_rate": 2.3145577853189015e-06,
      "loss": 1.2015,
      "step": 10020
    },
    {
      "epoch": 0.5384656681161755,
      "grad_norm": 4.795833110809326,
      "learning_rate": 2.31022314160235e-06,
      "loss": 1.0493,
      "step": 10030
    },
    {
      "epoch": 0.5390025232189832,
      "grad_norm": 8.388006210327148,
      "learning_rate": 2.305889071635833e-06,
      "loss": 0.792,
      "step": 10040
    },
    {
      "epoch": 0.539539378321791,
      "grad_norm": 7.47572660446167,
      "learning_rate": 2.301555588522492e-06,
      "loss": 0.9103,
      "step": 10050
    },
    {
      "epoch": 0.5400762334245987,
      "grad_norm": 6.261465549468994,
      "learning_rate": 2.297222705363692e-06,
      "loss": 0.8275,
      "step": 10060
    },
    {
      "epoch": 0.5406130885274064,
      "grad_norm": 6.299708366394043,
      "learning_rate": 2.292890435258986e-06,
      "loss": 0.9062,
      "step": 10070
    },
    {
      "epoch": 0.5411499436302142,
      "grad_norm": 5.449352264404297,
      "learning_rate": 2.2885587913060717e-06,
      "loss": 1.7082,
      "step": 10080
    },
    {
      "epoch": 0.541686798733022,
      "grad_norm": 15.12488842010498,
      "learning_rate": 2.2842277866007563e-06,
      "loss": 1.2874,
      "step": 10090
    },
    {
      "epoch": 0.5422236538358297,
      "grad_norm": 5.560448169708252,
      "learning_rate": 2.279897434236912e-06,
      "loss": 0.8503,
      "step": 10100
    },
    {
      "epoch": 0.5427605089386375,
      "grad_norm": 6.321796894073486,
      "learning_rate": 2.2755677473064394e-06,
      "loss": 0.892,
      "step": 10110
    },
    {
      "epoch": 0.5432973640414452,
      "grad_norm": 12.117864608764648,
      "learning_rate": 2.271238738899229e-06,
      "loss": 1.1939,
      "step": 10120
    },
    {
      "epoch": 0.543834219144253,
      "grad_norm": 13.869786262512207,
      "learning_rate": 2.266910422103117e-06,
      "loss": 1.0969,
      "step": 10130
    },
    {
      "epoch": 0.5443710742470608,
      "grad_norm": 14.814237594604492,
      "learning_rate": 2.2625828100038513e-06,
      "loss": 1.5467,
      "step": 10140
    },
    {
      "epoch": 0.5449079293498684,
      "grad_norm": 6.513592720031738,
      "learning_rate": 2.2582559156850467e-06,
      "loss": 1.1127,
      "step": 10150
    },
    {
      "epoch": 0.5454447844526762,
      "grad_norm": 7.026455879211426,
      "learning_rate": 2.2539297522281514e-06,
      "loss": 1.0628,
      "step": 10160
    },
    {
      "epoch": 0.545981639555484,
      "grad_norm": 13.22324275970459,
      "learning_rate": 2.2496043327124005e-06,
      "loss": 1.2139,
      "step": 10170
    },
    {
      "epoch": 0.5465184946582917,
      "grad_norm": 12.20676040649414,
      "learning_rate": 2.2452796702147826e-06,
      "loss": 0.6608,
      "step": 10180
    },
    {
      "epoch": 0.5470553497610995,
      "grad_norm": 15.429813385009766,
      "learning_rate": 2.2409557778099945e-06,
      "loss": 1.2997,
      "step": 10190
    },
    {
      "epoch": 0.5475922048639073,
      "grad_norm": 11.621594429016113,
      "learning_rate": 2.236632668570409e-06,
      "loss": 0.8456,
      "step": 10200
    },
    {
      "epoch": 0.548129059966715,
      "grad_norm": 14.709559440612793,
      "learning_rate": 2.232310355566028e-06,
      "loss": 1.2392,
      "step": 10210
    },
    {
      "epoch": 0.5486659150695228,
      "grad_norm": 12.997626304626465,
      "learning_rate": 2.2279888518644475e-06,
      "loss": 1.5849,
      "step": 10220
    },
    {
      "epoch": 0.5492027701723304,
      "grad_norm": 14.621047973632812,
      "learning_rate": 2.223668170530815e-06,
      "loss": 1.318,
      "step": 10230
    },
    {
      "epoch": 0.5497396252751382,
      "grad_norm": 6.242947578430176,
      "learning_rate": 2.2193483246277954e-06,
      "loss": 1.522,
      "step": 10240
    },
    {
      "epoch": 0.550276480377946,
      "grad_norm": 6.177704811096191,
      "learning_rate": 2.215029327215523e-06,
      "loss": 0.8574,
      "step": 10250
    },
    {
      "epoch": 0.5508133354807537,
      "grad_norm": 8.040987968444824,
      "learning_rate": 2.2107111913515715e-06,
      "loss": 1.1515,
      "step": 10260
    },
    {
      "epoch": 0.5513501905835615,
      "grad_norm": 6.494222164154053,
      "learning_rate": 2.206393930090906e-06,
      "loss": 0.8214,
      "step": 10270
    },
    {
      "epoch": 0.5518870456863693,
      "grad_norm": 5.411732196807861,
      "learning_rate": 2.202077556485851e-06,
      "loss": 0.5926,
      "step": 10280
    },
    {
      "epoch": 0.552423900789177,
      "grad_norm": 6.2157721519470215,
      "learning_rate": 2.197762083586044e-06,
      "loss": 1.0182,
      "step": 10290
    },
    {
      "epoch": 0.5529607558919848,
      "grad_norm": 4.849903106689453,
      "learning_rate": 2.1934475244384027e-06,
      "loss": 1.0658,
      "step": 10300
    },
    {
      "epoch": 0.5534976109947926,
      "grad_norm": 8.549363136291504,
      "learning_rate": 2.189133892087078e-06,
      "loss": 1.2559,
      "step": 10310
    },
    {
      "epoch": 0.5540344660976002,
      "grad_norm": 5.534502983093262,
      "learning_rate": 2.184821199573424e-06,
      "loss": 0.6197,
      "step": 10320
    },
    {
      "epoch": 0.554571321200408,
      "grad_norm": 7.615298271179199,
      "learning_rate": 2.1805094599359496e-06,
      "loss": 1.2455,
      "step": 10330
    },
    {
      "epoch": 0.5551081763032157,
      "grad_norm": 8.008660316467285,
      "learning_rate": 2.1761986862102845e-06,
      "loss": 0.8249,
      "step": 10340
    },
    {
      "epoch": 0.5556450314060235,
      "grad_norm": 5.708700656890869,
      "learning_rate": 2.1718888914291365e-06,
      "loss": 1.1307,
      "step": 10350
    },
    {
      "epoch": 0.5561818865088313,
      "grad_norm": 7.5365095138549805,
      "learning_rate": 2.1675800886222566e-06,
      "loss": 0.8307,
      "step": 10360
    },
    {
      "epoch": 0.556718741611639,
      "grad_norm": 5.835506916046143,
      "learning_rate": 2.163272290816394e-06,
      "loss": 1.0264,
      "step": 10370
    },
    {
      "epoch": 0.5572555967144468,
      "grad_norm": 5.597250938415527,
      "learning_rate": 2.158965511035261e-06,
      "loss": 0.795,
      "step": 10380
    },
    {
      "epoch": 0.5577924518172546,
      "grad_norm": 5.079643726348877,
      "learning_rate": 2.1546597622994925e-06,
      "loss": 0.8265,
      "step": 10390
    },
    {
      "epoch": 0.5583293069200623,
      "grad_norm": 5.55297327041626,
      "learning_rate": 2.1503550576266046e-06,
      "loss": 1.0734,
      "step": 10400
    },
    {
      "epoch": 0.55886616202287,
      "grad_norm": 14.704153060913086,
      "learning_rate": 2.146051410030959e-06,
      "loss": 0.8254,
      "step": 10410
    },
    {
      "epoch": 0.5594030171256777,
      "grad_norm": 5.373778343200684,
      "learning_rate": 2.1417488325237182e-06,
      "loss": 1.113,
      "step": 10420
    },
    {
      "epoch": 0.5599398722284855,
      "grad_norm": 16.62543487548828,
      "learning_rate": 2.1374473381128143e-06,
      "loss": 1.0807,
      "step": 10430
    },
    {
      "epoch": 0.5604767273312933,
      "grad_norm": 5.273484706878662,
      "learning_rate": 2.133146939802901e-06,
      "loss": 1.4459,
      "step": 10440
    },
    {
      "epoch": 0.561013582434101,
      "grad_norm": 5.3604230880737305,
      "learning_rate": 2.1288476505953194e-06,
      "loss": 1.2444,
      "step": 10450
    },
    {
      "epoch": 0.5615504375369088,
      "grad_norm": 17.38876724243164,
      "learning_rate": 2.124549483488057e-06,
      "loss": 1.7088,
      "step": 10460
    },
    {
      "epoch": 0.5620872926397166,
      "grad_norm": 6.306502342224121,
      "learning_rate": 2.1202524514757113e-06,
      "loss": 0.9625,
      "step": 10470
    },
    {
      "epoch": 0.5626241477425243,
      "grad_norm": 14.838828086853027,
      "learning_rate": 2.1159565675494443e-06,
      "loss": 1.4251,
      "step": 10480
    },
    {
      "epoch": 0.563161002845332,
      "grad_norm": 6.216856479644775,
      "learning_rate": 2.11166184469695e-06,
      "loss": 0.8733,
      "step": 10490
    },
    {
      "epoch": 0.5636978579481398,
      "grad_norm": 13.25187873840332,
      "learning_rate": 2.10736829590241e-06,
      "loss": 0.8504,
      "step": 10500
    },
    {
      "epoch": 0.5642347130509475,
      "grad_norm": 6.080920219421387,
      "learning_rate": 2.1030759341464583e-06,
      "loss": 1.1758,
      "step": 10510
    },
    {
      "epoch": 0.5647715681537553,
      "grad_norm": 5.644862651824951,
      "learning_rate": 2.098784772406139e-06,
      "loss": 0.8829,
      "step": 10520
    },
    {
      "epoch": 0.565308423256563,
      "grad_norm": 6.920463562011719,
      "learning_rate": 2.094494823654869e-06,
      "loss": 1.1413,
      "step": 10530
    },
    {
      "epoch": 0.5658452783593708,
      "grad_norm": 7.158686637878418,
      "learning_rate": 2.090206100862396e-06,
      "loss": 0.791,
      "step": 10540
    },
    {
      "epoch": 0.5663821334621786,
      "grad_norm": 13.96137809753418,
      "learning_rate": 2.085918616994764e-06,
      "loss": 1.2963,
      "step": 10550
    },
    {
      "epoch": 0.5669189885649863,
      "grad_norm": 14.418923377990723,
      "learning_rate": 2.0816323850142693e-06,
      "loss": 1.4476,
      "step": 10560
    },
    {
      "epoch": 0.5674558436677941,
      "grad_norm": 6.3950514793396,
      "learning_rate": 2.0773474178794256e-06,
      "loss": 0.6471,
      "step": 10570
    },
    {
      "epoch": 0.5679926987706018,
      "grad_norm": 7.1297688484191895,
      "learning_rate": 2.0730637285449186e-06,
      "loss": 1.4947,
      "step": 10580
    },
    {
      "epoch": 0.5685295538734095,
      "grad_norm": 5.737969875335693,
      "learning_rate": 2.0687813299615763e-06,
      "loss": 0.8903,
      "step": 10590
    },
    {
      "epoch": 0.5690664089762173,
      "grad_norm": 5.430782794952393,
      "learning_rate": 2.0645002350763194e-06,
      "loss": 1.207,
      "step": 10600
    },
    {
      "epoch": 0.5696032640790251,
      "grad_norm": 8.414691925048828,
      "learning_rate": 2.0602204568321305e-06,
      "loss": 0.8719,
      "step": 10610
    },
    {
      "epoch": 0.5701401191818328,
      "grad_norm": 6.2966532707214355,
      "learning_rate": 2.0559420081680094e-06,
      "loss": 0.8664,
      "step": 10620
    },
    {
      "epoch": 0.5706769742846406,
      "grad_norm": 13.461386680603027,
      "learning_rate": 2.051664902018938e-06,
      "loss": 0.8102,
      "step": 10630
    },
    {
      "epoch": 0.5712138293874484,
      "grad_norm": 11.95564079284668,
      "learning_rate": 2.0473891513158376e-06,
      "loss": 1.1167,
      "step": 10640
    },
    {
      "epoch": 0.5717506844902561,
      "grad_norm": 8.70516300201416,
      "learning_rate": 2.043114768985534e-06,
      "loss": 1.0362,
      "step": 10650
    },
    {
      "epoch": 0.5722875395930639,
      "grad_norm": 15.42365837097168,
      "learning_rate": 2.0388417679507143e-06,
      "loss": 0.88,
      "step": 10660
    },
    {
      "epoch": 0.5728243946958715,
      "grad_norm": 8.55978775024414,
      "learning_rate": 2.034570161129888e-06,
      "loss": 1.084,
      "step": 10670
    },
    {
      "epoch": 0.5733612497986793,
      "grad_norm": 14.646122932434082,
      "learning_rate": 2.030299961437354e-06,
      "loss": 1.1979,
      "step": 10680
    },
    {
      "epoch": 0.5738981049014871,
      "grad_norm": 6.80014181137085,
      "learning_rate": 2.026031181783152e-06,
      "loss": 0.7672,
      "step": 10690
    },
    {
      "epoch": 0.5744349600042948,
      "grad_norm": 5.60532808303833,
      "learning_rate": 2.0217638350730325e-06,
      "loss": 1.0441,
      "step": 10700
    },
    {
      "epoch": 0.5749718151071026,
      "grad_norm": 14.101771354675293,
      "learning_rate": 2.017497934208411e-06,
      "loss": 0.808,
      "step": 10710
    },
    {
      "epoch": 0.5755086702099104,
      "grad_norm": 5.3644280433654785,
      "learning_rate": 2.013233492086334e-06,
      "loss": 1.0105,
      "step": 10720
    },
    {
      "epoch": 0.5760455253127181,
      "grad_norm": 7.483492851257324,
      "learning_rate": 2.0089705215994357e-06,
      "loss": 1.4253,
      "step": 10730
    },
    {
      "epoch": 0.5765823804155259,
      "grad_norm": 5.044948577880859,
      "learning_rate": 2.0047090356359035e-06,
      "loss": 0.6524,
      "step": 10740
    },
    {
      "epoch": 0.5771192355183336,
      "grad_norm": 15.486733436584473,
      "learning_rate": 2.0004490470794336e-06,
      "loss": 1.0946,
      "step": 10750
    },
    {
      "epoch": 0.5776560906211413,
      "grad_norm": 6.127838134765625,
      "learning_rate": 1.996190568809199e-06,
      "loss": 1.0529,
      "step": 10760
    },
    {
      "epoch": 0.5781929457239491,
      "grad_norm": 5.216161727905273,
      "learning_rate": 1.9919336136998027e-06,
      "loss": 0.5933,
      "step": 10770
    },
    {
      "epoch": 0.5787298008267568,
      "grad_norm": 7.068978786468506,
      "learning_rate": 1.987678194621246e-06,
      "loss": 0.8161,
      "step": 10780
    },
    {
      "epoch": 0.5792666559295646,
      "grad_norm": 5.224578380584717,
      "learning_rate": 1.983424324438883e-06,
      "loss": 1.3624,
      "step": 10790
    },
    {
      "epoch": 0.5798035110323724,
      "grad_norm": 4.429221153259277,
      "learning_rate": 1.979172016013389e-06,
      "loss": 0.6227,
      "step": 10800
    },
    {
      "epoch": 0.5803403661351801,
      "grad_norm": 7.947227478027344,
      "learning_rate": 1.974921282200714e-06,
      "loss": 0.942,
      "step": 10810
    },
    {
      "epoch": 0.5808772212379879,
      "grad_norm": 5.985844135284424,
      "learning_rate": 1.970672135852051e-06,
      "loss": 0.8019,
      "step": 10820
    },
    {
      "epoch": 0.5814140763407957,
      "grad_norm": 9.886977195739746,
      "learning_rate": 1.966424589813789e-06,
      "loss": 1.098,
      "step": 10830
    },
    {
      "epoch": 0.5819509314436033,
      "grad_norm": 6.056911468505859,
      "learning_rate": 1.9621786569274847e-06,
      "loss": 0.772,
      "step": 10840
    },
    {
      "epoch": 0.5824877865464111,
      "grad_norm": 8.438995361328125,
      "learning_rate": 1.9579343500298124e-06,
      "loss": 1.2413,
      "step": 10850
    },
    {
      "epoch": 0.5830246416492189,
      "grad_norm": 6.900991439819336,
      "learning_rate": 1.953691681952534e-06,
      "loss": 1.3006,
      "step": 10860
    },
    {
      "epoch": 0.5835614967520266,
      "grad_norm": 5.983034133911133,
      "learning_rate": 1.949450665522454e-06,
      "loss": 0.7948,
      "step": 10870
    },
    {
      "epoch": 0.5840983518548344,
      "grad_norm": 5.9684929847717285,
      "learning_rate": 1.9452113135613866e-06,
      "loss": 1.0413,
      "step": 10880
    },
    {
      "epoch": 0.5846352069576422,
      "grad_norm": 5.101035118103027,
      "learning_rate": 1.9409736388861117e-06,
      "loss": 1.129,
      "step": 10890
    },
    {
      "epoch": 0.5851720620604499,
      "grad_norm": 7.34773063659668,
      "learning_rate": 1.9367376543083395e-06,
      "loss": 1.0269,
      "step": 10900
    },
    {
      "epoch": 0.5857089171632577,
      "grad_norm": 9.493403434753418,
      "learning_rate": 1.9325033726346683e-06,
      "loss": 0.9482,
      "step": 10910
    },
    {
      "epoch": 0.5862457722660653,
      "grad_norm": 5.407600402832031,
      "learning_rate": 1.9282708066665514e-06,
      "loss": 1.3943,
      "step": 10920
    },
    {
      "epoch": 0.5867826273688731,
      "grad_norm": 8.104090690612793,
      "learning_rate": 1.9240399692002527e-06,
      "loss": 0.8727,
      "step": 10930
    },
    {
      "epoch": 0.5873194824716809,
      "grad_norm": 6.991078853607178,
      "learning_rate": 1.91981087302681e-06,
      "loss": 1.7202,
      "step": 10940
    },
    {
      "epoch": 0.5878563375744886,
      "grad_norm": 13.489028930664062,
      "learning_rate": 1.9155835309319993e-06,
      "loss": 1.3177,
      "step": 10950
    },
    {
      "epoch": 0.5883931926772964,
      "grad_norm": 6.147861957550049,
      "learning_rate": 1.911357955696291e-06,
      "loss": 1.2789,
      "step": 10960
    },
    {
      "epoch": 0.5889300477801042,
      "grad_norm": 15.281700134277344,
      "learning_rate": 1.9071341600948145e-06,
      "loss": 1.3204,
      "step": 10970
    },
    {
      "epoch": 0.5894669028829119,
      "grad_norm": 5.820309162139893,
      "learning_rate": 1.902912156897318e-06,
      "loss": 1.0566,
      "step": 10980
    },
    {
      "epoch": 0.5900037579857197,
      "grad_norm": 4.329510688781738,
      "learning_rate": 1.8986919588681338e-06,
      "loss": 1.1956,
      "step": 10990
    },
    {
      "epoch": 0.5905406130885275,
      "grad_norm": 5.589778423309326,
      "learning_rate": 1.8944735787661332e-06,
      "loss": 1.0226,
      "step": 11000
    },
    {
      "epoch": 0.5910774681913351,
      "grad_norm": 14.254372596740723,
      "learning_rate": 1.890257029344693e-06,
      "loss": 1.078,
      "step": 11010
    },
    {
      "epoch": 0.5916143232941429,
      "grad_norm": 13.991432189941406,
      "learning_rate": 1.8860423233516541e-06,
      "loss": 0.8113,
      "step": 11020
    },
    {
      "epoch": 0.5921511783969506,
      "grad_norm": 13.541278839111328,
      "learning_rate": 1.8818294735292872e-06,
      "loss": 1.0958,
      "step": 11030
    },
    {
      "epoch": 0.5926880334997584,
      "grad_norm": 11.014192581176758,
      "learning_rate": 1.8776184926142473e-06,
      "loss": 1.4131,
      "step": 11040
    },
    {
      "epoch": 0.5932248886025662,
      "grad_norm": 4.97433614730835,
      "learning_rate": 1.8734093933375426e-06,
      "loss": 1.2723,
      "step": 11050
    },
    {
      "epoch": 0.5937617437053739,
      "grad_norm": 6.137520790100098,
      "learning_rate": 1.8692021884244892e-06,
      "loss": 1.2023,
      "step": 11060
    },
    {
      "epoch": 0.5942985988081817,
      "grad_norm": 7.318007469177246,
      "learning_rate": 1.8649968905946797e-06,
      "loss": 0.6247,
      "step": 11070
    },
    {
      "epoch": 0.5948354539109895,
      "grad_norm": 4.679361820220947,
      "learning_rate": 1.860793512561938e-06,
      "loss": 0.8565,
      "step": 11080
    },
    {
      "epoch": 0.5953723090137972,
      "grad_norm": 11.175265312194824,
      "learning_rate": 1.8565920670342858e-06,
      "loss": 1.0128,
      "step": 11090
    },
    {
      "epoch": 0.5959091641166049,
      "grad_norm": 12.925514221191406,
      "learning_rate": 1.8523925667138999e-06,
      "loss": 1.0265,
      "step": 11100
    },
    {
      "epoch": 0.5964460192194126,
      "grad_norm": 5.809579372406006,
      "learning_rate": 1.8481950242970804e-06,
      "loss": 0.9675,
      "step": 11110
    },
    {
      "epoch": 0.5969828743222204,
      "grad_norm": 5.329052448272705,
      "learning_rate": 1.8439994524742033e-06,
      "loss": 0.9069,
      "step": 11120
    },
    {
      "epoch": 0.5975197294250282,
      "grad_norm": 7.286038875579834,
      "learning_rate": 1.8398058639296906e-06,
      "loss": 0.8376,
      "step": 11130
    },
    {
      "epoch": 0.5980565845278359,
      "grad_norm": 5.797366619110107,
      "learning_rate": 1.8356142713419652e-06,
      "loss": 1.3797,
      "step": 11140
    },
    {
      "epoch": 0.5985934396306437,
      "grad_norm": 5.068739891052246,
      "learning_rate": 1.8314246873834192e-06,
      "loss": 1.5248,
      "step": 11150
    },
    {
      "epoch": 0.5991302947334515,
      "grad_norm": 11.302424430847168,
      "learning_rate": 1.8272371247203692e-06,
      "loss": 1.6425,
      "step": 11160
    },
    {
      "epoch": 0.5996671498362592,
      "grad_norm": 7.196443557739258,
      "learning_rate": 1.8230515960130205e-06,
      "loss": 0.9262,
      "step": 11170
    },
    {
      "epoch": 0.6002040049390669,
      "grad_norm": 13.76465892791748,
      "learning_rate": 1.8188681139154329e-06,
      "loss": 1.1582,
      "step": 11180
    },
    {
      "epoch": 0.6007408600418747,
      "grad_norm": 6.795285701751709,
      "learning_rate": 1.8146866910754745e-06,
      "loss": 0.8825,
      "step": 11190
    },
    {
      "epoch": 0.6012777151446824,
      "grad_norm": 8.118711471557617,
      "learning_rate": 1.8105073401347905e-06,
      "loss": 1.1124,
      "step": 11200
    },
    {
      "epoch": 0.6018145702474902,
      "grad_norm": 9.030536651611328,
      "learning_rate": 1.8063300737287593e-06,
      "loss": 0.6077,
      "step": 11210
    },
    {
      "epoch": 0.602351425350298,
      "grad_norm": 4.620079517364502,
      "learning_rate": 1.802154904486461e-06,
      "loss": 1.4065,
      "step": 11220
    },
    {
      "epoch": 0.6028882804531057,
      "grad_norm": 5.752904415130615,
      "learning_rate": 1.7979818450306315e-06,
      "loss": 0.819,
      "step": 11230
    },
    {
      "epoch": 0.6034251355559135,
      "grad_norm": 6.991641044616699,
      "learning_rate": 1.7938109079776316e-06,
      "loss": 1.331,
      "step": 11240
    },
    {
      "epoch": 0.6039619906587212,
      "grad_norm": 5.738000869750977,
      "learning_rate": 1.789642105937402e-06,
      "loss": 1.1412,
      "step": 11250
    },
    {
      "epoch": 0.604498845761529,
      "grad_norm": 5.73778772354126,
      "learning_rate": 1.7854754515134323e-06,
      "loss": 0.7904,
      "step": 11260
    },
    {
      "epoch": 0.6050357008643367,
      "grad_norm": 6.561555862426758,
      "learning_rate": 1.781310957302716e-06,
      "loss": 0.9814,
      "step": 11270
    },
    {
      "epoch": 0.6055725559671444,
      "grad_norm": 5.684905052185059,
      "learning_rate": 1.7771486358957185e-06,
      "loss": 1.1191,
      "step": 11280
    },
    {
      "epoch": 0.6061094110699522,
      "grad_norm": 5.973367214202881,
      "learning_rate": 1.772988499876333e-06,
      "loss": 0.9746,
      "step": 11290
    },
    {
      "epoch": 0.60664626617276,
      "grad_norm": 3.9681596755981445,
      "learning_rate": 1.7688305618218493e-06,
      "loss": 1.3901,
      "step": 11300
    },
    {
      "epoch": 0.6071831212755677,
      "grad_norm": 6.46362829208374,
      "learning_rate": 1.764674834302908e-06,
      "loss": 0.9453,
      "step": 11310
    },
    {
      "epoch": 0.6077199763783755,
      "grad_norm": 5.062347412109375,
      "learning_rate": 1.760521329883472e-06,
      "loss": 0.9932,
      "step": 11320
    },
    {
      "epoch": 0.6082568314811833,
      "grad_norm": 4.840379238128662,
      "learning_rate": 1.7563700611207765e-06,
      "loss": 1.1005,
      "step": 11330
    },
    {
      "epoch": 0.608793686583991,
      "grad_norm": 13.419822692871094,
      "learning_rate": 1.7522210405653039e-06,
      "loss": 0.7855,
      "step": 11340
    },
    {
      "epoch": 0.6093305416867988,
      "grad_norm": 6.106851577758789,
      "learning_rate": 1.7480742807607346e-06,
      "loss": 1.2715,
      "step": 11350
    },
    {
      "epoch": 0.6098673967896064,
      "grad_norm": 4.46468448638916,
      "learning_rate": 1.7439297942439193e-06,
      "loss": 1.1329,
      "step": 11360
    },
    {
      "epoch": 0.6104042518924142,
      "grad_norm": 7.136985778808594,
      "learning_rate": 1.7397875935448294e-06,
      "loss": 0.6192,
      "step": 11370
    },
    {
      "epoch": 0.610941106995222,
      "grad_norm": 6.187252998352051,
      "learning_rate": 1.735647691186532e-06,
      "loss": 1.4043,
      "step": 11380
    },
    {
      "epoch": 0.6114779620980297,
      "grad_norm": 5.264932155609131,
      "learning_rate": 1.7315100996851408e-06,
      "loss": 1.1557,
      "step": 11390
    },
    {
      "epoch": 0.6120148172008375,
      "grad_norm": 5.118681907653809,
      "learning_rate": 1.7273748315497873e-06,
      "loss": 0.6418,
      "step": 11400
    },
    {
      "epoch": 0.6125516723036453,
      "grad_norm": 15.431357383728027,
      "learning_rate": 1.7232418992825748e-06,
      "loss": 1.4859,
      "step": 11410
    },
    {
      "epoch": 0.613088527406453,
      "grad_norm": 5.591870307922363,
      "learning_rate": 1.7191113153785478e-06,
      "loss": 1.1158,
      "step": 11420
    },
    {
      "epoch": 0.6136253825092608,
      "grad_norm": 9.556784629821777,
      "learning_rate": 1.7149830923256477e-06,
      "loss": 1.1269,
      "step": 11430
    },
    {
      "epoch": 0.6141622376120685,
      "grad_norm": 10.16091537475586,
      "learning_rate": 1.7108572426046827e-06,
      "loss": 1.2005,
      "step": 11440
    },
    {
      "epoch": 0.6146990927148762,
      "grad_norm": 11.08980655670166,
      "learning_rate": 1.7067337786892824e-06,
      "loss": 1.2756,
      "step": 11450
    },
    {
      "epoch": 0.615235947817684,
      "grad_norm": 5.239773273468018,
      "learning_rate": 1.7026127130458633e-06,
      "loss": 1.0098,
      "step": 11460
    },
    {
      "epoch": 0.6157728029204917,
      "grad_norm": 6.5852952003479,
      "learning_rate": 1.6984940581335946e-06,
      "loss": 0.9719,
      "step": 11470
    },
    {
      "epoch": 0.6163096580232995,
      "grad_norm": 6.731161117553711,
      "learning_rate": 1.694377826404353e-06,
      "loss": 1.4538,
      "step": 11480
    },
    {
      "epoch": 0.6168465131261073,
      "grad_norm": 4.720456600189209,
      "learning_rate": 1.6902640303026924e-06,
      "loss": 0.9764,
      "step": 11490
    },
    {
      "epoch": 0.617383368228915,
      "grad_norm": 6.29373025894165,
      "learning_rate": 1.6861526822657997e-06,
      "loss": 1.2718,
      "step": 11500
    },
    {
      "epoch": 0.6179202233317228,
      "grad_norm": 12.66163158416748,
      "learning_rate": 1.6820437947234654e-06,
      "loss": 1.0908,
      "step": 11510
    },
    {
      "epoch": 0.6184570784345306,
      "grad_norm": 7.9806952476501465,
      "learning_rate": 1.677937380098037e-06,
      "loss": 0.6365,
      "step": 11520
    },
    {
      "epoch": 0.6189939335373382,
      "grad_norm": 5.7943525314331055,
      "learning_rate": 1.673833450804388e-06,
      "loss": 0.6515,
      "step": 11530
    },
    {
      "epoch": 0.619530788640146,
      "grad_norm": 5.5296196937561035,
      "learning_rate": 1.6697320192498752e-06,
      "loss": 1.2059,
      "step": 11540
    },
    {
      "epoch": 0.6200676437429538,
      "grad_norm": 6.310262680053711,
      "learning_rate": 1.665633097834309e-06,
      "loss": 1.0391,
      "step": 11550
    },
    {
      "epoch": 0.6206044988457615,
      "grad_norm": 5.145846843719482,
      "learning_rate": 1.661536698949906e-06,
      "loss": 0.766,
      "step": 11560
    },
    {
      "epoch": 0.6211413539485693,
      "grad_norm": 14.83370590209961,
      "learning_rate": 1.657442834981259e-06,
      "loss": 1.393,
      "step": 11570
    },
    {
      "epoch": 0.621678209051377,
      "grad_norm": 5.297738552093506,
      "learning_rate": 1.6533515183052957e-06,
      "loss": 0.8196,
      "step": 11580
    },
    {
      "epoch": 0.6222150641541848,
      "grad_norm": 15.334333419799805,
      "learning_rate": 1.6492627612912448e-06,
      "loss": 1.7275,
      "step": 11590
    },
    {
      "epoch": 0.6227519192569926,
      "grad_norm": 13.57276725769043,
      "learning_rate": 1.6451765763005937e-06,
      "loss": 1.5967,
      "step": 11600
    },
    {
      "epoch": 0.6232887743598002,
      "grad_norm": 12.05284595489502,
      "learning_rate": 1.6410929756870558e-06,
      "loss": 0.6083,
      "step": 11610
    },
    {
      "epoch": 0.623825629462608,
      "grad_norm": 7.541871070861816,
      "learning_rate": 1.6370119717965293e-06,
      "loss": 1.1741,
      "step": 11620
    },
    {
      "epoch": 0.6243624845654158,
      "grad_norm": 6.844648361206055,
      "learning_rate": 1.6329335769670648e-06,
      "loss": 0.7808,
      "step": 11630
    },
    {
      "epoch": 0.6248993396682235,
      "grad_norm": 5.326478004455566,
      "learning_rate": 1.6288578035288216e-06,
      "loss": 0.8447,
      "step": 11640
    },
    {
      "epoch": 0.6254361947710313,
      "grad_norm": 5.819628715515137,
      "learning_rate": 1.6247846638040365e-06,
      "loss": 0.5825,
      "step": 11650
    },
    {
      "epoch": 0.6259730498738391,
      "grad_norm": 12.920642852783203,
      "learning_rate": 1.6207141701069804e-06,
      "loss": 0.8654,
      "step": 11660
    },
    {
      "epoch": 0.6265099049766468,
      "grad_norm": 4.553651332855225,
      "learning_rate": 1.6166463347439292e-06,
      "loss": 1.0986,
      "step": 11670
    },
    {
      "epoch": 0.6270467600794546,
      "grad_norm": 13.6753568649292,
      "learning_rate": 1.6125811700131177e-06,
      "loss": 1.1029,
      "step": 11680
    },
    {
      "epoch": 0.6275836151822624,
      "grad_norm": 7.241902828216553,
      "learning_rate": 1.6085186882047098e-06,
      "loss": 1.0844,
      "step": 11690
    },
    {
      "epoch": 0.62812047028507,
      "grad_norm": 10.133666038513184,
      "learning_rate": 1.6044589016007545e-06,
      "loss": 1.061,
      "step": 11700
    },
    {
      "epoch": 0.6286573253878778,
      "grad_norm": 4.733138084411621,
      "learning_rate": 1.600401822475156e-06,
      "loss": 0.9622,
      "step": 11710
    },
    {
      "epoch": 0.6291941804906855,
      "grad_norm": 14.35534954071045,
      "learning_rate": 1.5963474630936323e-06,
      "loss": 1.3745,
      "step": 11720
    },
    {
      "epoch": 0.6297310355934933,
      "grad_norm": 15.016681671142578,
      "learning_rate": 1.5922958357136759e-06,
      "loss": 1.0341,
      "step": 11730
    },
    {
      "epoch": 0.6302678906963011,
      "grad_norm": 9.442488670349121,
      "learning_rate": 1.5882469525845245e-06,
      "loss": 1.4679,
      "step": 11740
    },
    {
      "epoch": 0.6308047457991088,
      "grad_norm": 13.3638334274292,
      "learning_rate": 1.5842008259471143e-06,
      "loss": 1.2613,
      "step": 11750
    },
    {
      "epoch": 0.6313416009019166,
      "grad_norm": 12.684722900390625,
      "learning_rate": 1.5801574680340514e-06,
      "loss": 1.1518,
      "step": 11760
    },
    {
      "epoch": 0.6318784560047244,
      "grad_norm": 4.621258735656738,
      "learning_rate": 1.5761168910695687e-06,
      "loss": 1.4765,
      "step": 11770
    },
    {
      "epoch": 0.6324153111075321,
      "grad_norm": 5.238236427307129,
      "learning_rate": 1.5720791072694944e-06,
      "loss": 1.1081,
      "step": 11780
    },
    {
      "epoch": 0.6329521662103398,
      "grad_norm": 12.762290000915527,
      "learning_rate": 1.568044128841209e-06,
      "loss": 0.7553,
      "step": 11790
    },
    {
      "epoch": 0.6334890213131475,
      "grad_norm": 5.382036209106445,
      "learning_rate": 1.564011967983614e-06,
      "loss": 0.9661,
      "step": 11800
    },
    {
      "epoch": 0.6340258764159553,
      "grad_norm": 5.524689197540283,
      "learning_rate": 1.55998263688709e-06,
      "loss": 1.0668,
      "step": 11810
    },
    {
      "epoch": 0.6345627315187631,
      "grad_norm": 13.151959419250488,
      "learning_rate": 1.5559561477334662e-06,
      "loss": 1.2007,
      "step": 11820
    },
    {
      "epoch": 0.6350995866215708,
      "grad_norm": 13.853103637695312,
      "learning_rate": 1.5519325126959755e-06,
      "loss": 1.6128,
      "step": 11830
    },
    {
      "epoch": 0.6356364417243786,
      "grad_norm": 6.439970970153809,
      "learning_rate": 1.547911743939226e-06,
      "loss": 1.3734,
      "step": 11840
    },
    {
      "epoch": 0.6361732968271864,
      "grad_norm": 8.293355941772461,
      "learning_rate": 1.5438938536191567e-06,
      "loss": 1.3217,
      "step": 11850
    },
    {
      "epoch": 0.6367101519299941,
      "grad_norm": 6.184427261352539,
      "learning_rate": 1.5398788538830068e-06,
      "loss": 1.2256,
      "step": 11860
    },
    {
      "epoch": 0.6372470070328018,
      "grad_norm": 5.882351398468018,
      "learning_rate": 1.535866756869275e-06,
      "loss": 0.7971,
      "step": 11870
    },
    {
      "epoch": 0.6377838621356096,
      "grad_norm": 15.61098575592041,
      "learning_rate": 1.5318575747076856e-06,
      "loss": 0.9899,
      "step": 11880
    },
    {
      "epoch": 0.6383207172384173,
      "grad_norm": 7.176275730133057,
      "learning_rate": 1.5278513195191475e-06,
      "loss": 0.8131,
      "step": 11890
    },
    {
      "epoch": 0.6388575723412251,
      "grad_norm": 7.056249141693115,
      "learning_rate": 1.5238480034157251e-06,
      "loss": 1.0058,
      "step": 11900
    },
    {
      "epoch": 0.6393944274440329,
      "grad_norm": 8.19430160522461,
      "learning_rate": 1.5198476385005931e-06,
      "loss": 1.4441,
      "step": 11910
    },
    {
      "epoch": 0.6399312825468406,
      "grad_norm": 7.751392364501953,
      "learning_rate": 1.5158502368680062e-06,
      "loss": 0.735,
      "step": 11920
    },
    {
      "epoch": 0.6404681376496484,
      "grad_norm": 5.088303089141846,
      "learning_rate": 1.5118558106032578e-06,
      "loss": 1.1384,
      "step": 11930
    },
    {
      "epoch": 0.6410049927524561,
      "grad_norm": 4.272446632385254,
      "learning_rate": 1.50786437178265e-06,
      "loss": 0.5929,
      "step": 11940
    },
    {
      "epoch": 0.6415418478552639,
      "grad_norm": 6.062412261962891,
      "learning_rate": 1.5038759324734487e-06,
      "loss": 1.0265,
      "step": 11950
    },
    {
      "epoch": 0.6420787029580716,
      "grad_norm": 14.457653045654297,
      "learning_rate": 1.4998905047338546e-06,
      "loss": 1.3603,
      "step": 11960
    },
    {
      "epoch": 0.6426155580608793,
      "grad_norm": 9.13637638092041,
      "learning_rate": 1.4959081006129604e-06,
      "loss": 1.229,
      "step": 11970
    },
    {
      "epoch": 0.6431524131636871,
      "grad_norm": 8.49454402923584,
      "learning_rate": 1.4919287321507203e-06,
      "loss": 0.6675,
      "step": 11980
    },
    {
      "epoch": 0.6436892682664949,
      "grad_norm": 6.687187194824219,
      "learning_rate": 1.4879524113779114e-06,
      "loss": 1.0696,
      "step": 11990
    },
    {
      "epoch": 0.6442261233693026,
      "grad_norm": 5.548574447631836,
      "learning_rate": 1.4839791503160926e-06,
      "loss": 1.2446,
      "step": 12000
    },
    {
      "epoch": 0.6447629784721104,
      "grad_norm": 5.2215352058410645,
      "learning_rate": 1.4800089609775778e-06,
      "loss": 0.6884,
      "step": 12010
    },
    {
      "epoch": 0.6452998335749182,
      "grad_norm": 7.686403274536133,
      "learning_rate": 1.4760418553653889e-06,
      "loss": 0.8507,
      "step": 12020
    },
    {
      "epoch": 0.6458366886777259,
      "grad_norm": 13.461560249328613,
      "learning_rate": 1.4720778454732295e-06,
      "loss": 0.976,
      "step": 12030
    },
    {
      "epoch": 0.6463735437805337,
      "grad_norm": 4.765686988830566,
      "learning_rate": 1.4681169432854396e-06,
      "loss": 1.0611,
      "step": 12040
    },
    {
      "epoch": 0.6469103988833413,
      "grad_norm": 8.464279174804688,
      "learning_rate": 1.464159160776968e-06,
      "loss": 0.8207,
      "step": 12050
    },
    {
      "epoch": 0.6474472539861491,
      "grad_norm": 6.104118347167969,
      "learning_rate": 1.460204509913328e-06,
      "loss": 0.9916,
      "step": 12060
    },
    {
      "epoch": 0.6479841090889569,
      "grad_norm": 9.488781929016113,
      "learning_rate": 1.4562530026505686e-06,
      "loss": 1.0822,
      "step": 12070
    },
    {
      "epoch": 0.6485209641917646,
      "grad_norm": 5.439093112945557,
      "learning_rate": 1.4523046509352317e-06,
      "loss": 0.8211,
      "step": 12080
    },
    {
      "epoch": 0.6490578192945724,
      "grad_norm": 12.87173080444336,
      "learning_rate": 1.4483594667043205e-06,
      "loss": 1.3982,
      "step": 12090
    },
    {
      "epoch": 0.6495946743973802,
      "grad_norm": 7.139204025268555,
      "learning_rate": 1.4444174618852619e-06,
      "loss": 0.7672,
      "step": 12100
    },
    {
      "epoch": 0.6501315295001879,
      "grad_norm": 6.558509349822998,
      "learning_rate": 1.4404786483958715e-06,
      "loss": 1.1895,
      "step": 12110
    },
    {
      "epoch": 0.6506683846029957,
      "grad_norm": 5.284725666046143,
      "learning_rate": 1.4365430381443124e-06,
      "loss": 1.1933,
      "step": 12120
    },
    {
      "epoch": 0.6512052397058034,
      "grad_norm": 7.213533878326416,
      "learning_rate": 1.432610643029071e-06,
      "loss": 0.9746,
      "step": 12130
    },
    {
      "epoch": 0.6517420948086111,
      "grad_norm": 5.383005619049072,
      "learning_rate": 1.4286814749389054e-06,
      "loss": 0.8389,
      "step": 12140
    },
    {
      "epoch": 0.6522789499114189,
      "grad_norm": 15.090075492858887,
      "learning_rate": 1.4247555457528229e-06,
      "loss": 1.2716,
      "step": 12150
    },
    {
      "epoch": 0.6528158050142266,
      "grad_norm": 7.002494812011719,
      "learning_rate": 1.4208328673400354e-06,
      "loss": 1.9856,
      "step": 12160
    },
    {
      "epoch": 0.6533526601170344,
      "grad_norm": 6.400459289550781,
      "learning_rate": 1.4169134515599291e-06,
      "loss": 1.071,
      "step": 12170
    },
    {
      "epoch": 0.6538895152198422,
      "grad_norm": 14.319091796875,
      "learning_rate": 1.4129973102620255e-06,
      "loss": 0.8464,
      "step": 12180
    },
    {
      "epoch": 0.6544263703226499,
      "grad_norm": 6.071603298187256,
      "learning_rate": 1.4090844552859465e-06,
      "loss": 1.0202,
      "step": 12190
    },
    {
      "epoch": 0.6549632254254577,
      "grad_norm": 13.050665855407715,
      "learning_rate": 1.4051748984613758e-06,
      "loss": 1.158,
      "step": 12200
    },
    {
      "epoch": 0.6555000805282655,
      "grad_norm": 5.212625026702881,
      "learning_rate": 1.4012686516080326e-06,
      "loss": 0.6297,
      "step": 12210
    },
    {
      "epoch": 0.6560369356310731,
      "grad_norm": 7.38482666015625,
      "learning_rate": 1.397365726535621e-06,
      "loss": 0.6037,
      "step": 12220
    },
    {
      "epoch": 0.6565737907338809,
      "grad_norm": 6.944662094116211,
      "learning_rate": 1.3934661350438083e-06,
      "loss": 0.8631,
      "step": 12230
    },
    {
      "epoch": 0.6571106458366887,
      "grad_norm": 5.195940971374512,
      "learning_rate": 1.3895698889221803e-06,
      "loss": 0.8346,
      "step": 12240
    },
    {
      "epoch": 0.6576475009394964,
      "grad_norm": 10.415426254272461,
      "learning_rate": 1.3856769999502107e-06,
      "loss": 0.8541,
      "step": 12250
    },
    {
      "epoch": 0.6581843560423042,
      "grad_norm": 7.565029621124268,
      "learning_rate": 1.381787479897222e-06,
      "loss": 0.8138,
      "step": 12260
    },
    {
      "epoch": 0.658721211145112,
      "grad_norm": 14.056329727172852,
      "learning_rate": 1.377901340522353e-06,
      "loss": 1.324,
      "step": 12270
    },
    {
      "epoch": 0.6592580662479197,
      "grad_norm": 14.161043167114258,
      "learning_rate": 1.37401859357452e-06,
      "loss": 1.4309,
      "step": 12280
    },
    {
      "epoch": 0.6597949213507275,
      "grad_norm": 5.29819917678833,
      "learning_rate": 1.370139250792385e-06,
      "loss": 0.6536,
      "step": 12290
    },
    {
      "epoch": 0.6603317764535352,
      "grad_norm": 14.71925163269043,
      "learning_rate": 1.3662633239043171e-06,
      "loss": 1.1378,
      "step": 12300
    },
    {
      "epoch": 0.6608686315563429,
      "grad_norm": 4.825445652008057,
      "learning_rate": 1.362390824628356e-06,
      "loss": 0.8647,
      "step": 12310
    },
    {
      "epoch": 0.6614054866591507,
      "grad_norm": 13.501662254333496,
      "learning_rate": 1.3585217646721846e-06,
      "loss": 1.4306,
      "step": 12320
    },
    {
      "epoch": 0.6619423417619584,
      "grad_norm": 5.907223701477051,
      "learning_rate": 1.3546561557330814e-06,
      "loss": 1.0999,
      "step": 12330
    },
    {
      "epoch": 0.6624791968647662,
      "grad_norm": 13.576423645019531,
      "learning_rate": 1.3507940094978953e-06,
      "loss": 1.1726,
      "step": 12340
    },
    {
      "epoch": 0.663016051967574,
      "grad_norm": 5.912051677703857,
      "learning_rate": 1.346935337643005e-06,
      "loss": 0.5968,
      "step": 12350
    },
    {
      "epoch": 0.6635529070703817,
      "grad_norm": 5.733963966369629,
      "learning_rate": 1.3430801518342857e-06,
      "loss": 0.6175,
      "step": 12360
    },
    {
      "epoch": 0.6640897621731895,
      "grad_norm": 6.955583095550537,
      "learning_rate": 1.3392284637270732e-06,
      "loss": 0.8163,
      "step": 12370
    },
    {
      "epoch": 0.6646266172759973,
      "grad_norm": 5.295736789703369,
      "learning_rate": 1.3353802849661296e-06,
      "loss": 0.6663,
      "step": 12380
    },
    {
      "epoch": 0.6651634723788049,
      "grad_norm": 15.844099044799805,
      "learning_rate": 1.3315356271856034e-06,
      "loss": 1.3306,
      "step": 12390
    },
    {
      "epoch": 0.6657003274816127,
      "grad_norm": 5.621187210083008,
      "learning_rate": 1.3276945020090048e-06,
      "loss": 1.0683,
      "step": 12400
    },
    {
      "epoch": 0.6662371825844204,
      "grad_norm": 6.563199996948242,
      "learning_rate": 1.3238569210491572e-06,
      "loss": 1.2167,
      "step": 12410
    },
    {
      "epoch": 0.6667740376872282,
      "grad_norm": 6.345821380615234,
      "learning_rate": 1.3200228959081728e-06,
      "loss": 1.0659,
      "step": 12420
    },
    {
      "epoch": 0.667310892790036,
      "grad_norm": 8.502699851989746,
      "learning_rate": 1.3161924381774126e-06,
      "loss": 0.6355,
      "step": 12430
    },
    {
      "epoch": 0.6678477478928437,
      "grad_norm": 5.2658162117004395,
      "learning_rate": 1.3123655594374523e-06,
      "loss": 0.8689,
      "step": 12440
    },
    {
      "epoch": 0.6683846029956515,
      "grad_norm": 5.402364253997803,
      "learning_rate": 1.3085422712580473e-06,
      "loss": 1.0245,
      "step": 12450
    },
    {
      "epoch": 0.6689214580984593,
      "grad_norm": 6.024814605712891,
      "learning_rate": 1.304722585198098e-06,
      "loss": 0.8626,
      "step": 12460
    },
    {
      "epoch": 0.669458313201267,
      "grad_norm": 13.036808013916016,
      "learning_rate": 1.3009065128056118e-06,
      "loss": 1.2364,
      "step": 12470
    },
    {
      "epoch": 0.6699951683040747,
      "grad_norm": 5.381145000457764,
      "learning_rate": 1.2970940656176768e-06,
      "loss": 0.6918,
      "step": 12480
    },
    {
      "epoch": 0.6705320234068824,
      "grad_norm": 5.74312162399292,
      "learning_rate": 1.2932852551604146e-06,
      "loss": 0.6644,
      "step": 12490
    },
    {
      "epoch": 0.6710688785096902,
      "grad_norm": 5.068548679351807,
      "learning_rate": 1.2894800929489563e-06,
      "loss": 0.82,
      "step": 12500
    },
    {
      "epoch": 0.671605733612498,
      "grad_norm": 6.714485168457031,
      "learning_rate": 1.2856785904874015e-06,
      "loss": 0.7554,
      "step": 12510
    },
    {
      "epoch": 0.6721425887153057,
      "grad_norm": 13.879148483276367,
      "learning_rate": 1.281880759268786e-06,
      "loss": 0.7694,
      "step": 12520
    },
    {
      "epoch": 0.6726794438181135,
      "grad_norm": 13.900629997253418,
      "learning_rate": 1.2780866107750455e-06,
      "loss": 1.2169,
      "step": 12530
    },
    {
      "epoch": 0.6732162989209213,
      "grad_norm": 6.083281993865967,
      "learning_rate": 1.2742961564769832e-06,
      "loss": 0.8179,
      "step": 12540
    },
    {
      "epoch": 0.673753154023729,
      "grad_norm": 6.199990749359131,
      "learning_rate": 1.2705094078342322e-06,
      "loss": 0.5999,
      "step": 12550
    },
    {
      "epoch": 0.6742900091265367,
      "grad_norm": 7.213817119598389,
      "learning_rate": 1.266726376295223e-06,
      "loss": 1.2233,
      "step": 12560
    },
    {
      "epoch": 0.6748268642293445,
      "grad_norm": 6.948901653289795,
      "learning_rate": 1.2629470732971497e-06,
      "loss": 0.606,
      "step": 12570
    },
    {
      "epoch": 0.6753637193321522,
      "grad_norm": 14.305567741394043,
      "learning_rate": 1.259171510265929e-06,
      "loss": 0.7826,
      "step": 12580
    },
    {
      "epoch": 0.67590057443496,
      "grad_norm": 6.2015700340271,
      "learning_rate": 1.2553996986161776e-06,
      "loss": 1.245,
      "step": 12590
    },
    {
      "epoch": 0.6764374295377678,
      "grad_norm": 5.527857780456543,
      "learning_rate": 1.2516316497511648e-06,
      "loss": 1.0458,
      "step": 12600
    },
    {
      "epoch": 0.6769742846405755,
      "grad_norm": 12.794093132019043,
      "learning_rate": 1.2478673750627872e-06,
      "loss": 1.4125,
      "step": 12610
    },
    {
      "epoch": 0.6775111397433833,
      "grad_norm": 12.989511489868164,
      "learning_rate": 1.2441068859315304e-06,
      "loss": 1.0473,
      "step": 12620
    },
    {
      "epoch": 0.678047994846191,
      "grad_norm": 12.967605590820312,
      "learning_rate": 1.2403501937264337e-06,
      "loss": 1.155,
      "step": 12630
    },
    {
      "epoch": 0.6785848499489988,
      "grad_norm": 8.510704040527344,
      "learning_rate": 1.2365973098050593e-06,
      "loss": 0.959,
      "step": 12640
    },
    {
      "epoch": 0.6791217050518065,
      "grad_norm": 5.085075378417969,
      "learning_rate": 1.2328482455134551e-06,
      "loss": 1.2159,
      "step": 12650
    },
    {
      "epoch": 0.6796585601546142,
      "grad_norm": 9.209644317626953,
      "learning_rate": 1.229103012186119e-06,
      "loss": 1.088,
      "step": 12660
    },
    {
      "epoch": 0.680195415257422,
      "grad_norm": 5.466617584228516,
      "learning_rate": 1.2253616211459715e-06,
      "loss": 1.0747,
      "step": 12670
    },
    {
      "epoch": 0.6807322703602298,
      "grad_norm": 7.370673656463623,
      "learning_rate": 1.2216240837043116e-06,
      "loss": 0.862,
      "step": 12680
    },
    {
      "epoch": 0.6812691254630375,
      "grad_norm": 5.093696117401123,
      "learning_rate": 1.217890411160791e-06,
      "loss": 0.7911,
      "step": 12690
    },
    {
      "epoch": 0.6818059805658453,
      "grad_norm": 13.383155822753906,
      "learning_rate": 1.2141606148033763e-06,
      "loss": 0.9875,
      "step": 12700
    },
    {
      "epoch": 0.6823428356686531,
      "grad_norm": 7.08881139755249,
      "learning_rate": 1.2104347059083144e-06,
      "loss": 1.1963,
      "step": 12710
    },
    {
      "epoch": 0.6828796907714608,
      "grad_norm": 13.814618110656738,
      "learning_rate": 1.2067126957400995e-06,
      "loss": 0.8238,
      "step": 12720
    },
    {
      "epoch": 0.6834165458742686,
      "grad_norm": 5.581386566162109,
      "learning_rate": 1.2029945955514404e-06,
      "loss": 1.1849,
      "step": 12730
    },
    {
      "epoch": 0.6839534009770762,
      "grad_norm": 6.0861124992370605,
      "learning_rate": 1.1992804165832205e-06,
      "loss": 0.5964,
      "step": 12740
    },
    {
      "epoch": 0.684490256079884,
      "grad_norm": 6.717843532562256,
      "learning_rate": 1.1955701700644753e-06,
      "loss": 1.4122,
      "step": 12750
    },
    {
      "epoch": 0.6850271111826918,
      "grad_norm": 15.057908058166504,
      "learning_rate": 1.1918638672123432e-06,
      "loss": 1.2364,
      "step": 12760
    },
    {
      "epoch": 0.6855639662854995,
      "grad_norm": 14.847865104675293,
      "learning_rate": 1.1881615192320478e-06,
      "loss": 1.2568,
      "step": 12770
    },
    {
      "epoch": 0.6861008213883073,
      "grad_norm": 7.931560039520264,
      "learning_rate": 1.1844631373168492e-06,
      "loss": 1.3847,
      "step": 12780
    },
    {
      "epoch": 0.6866376764911151,
      "grad_norm": 6.195589542388916,
      "learning_rate": 1.1807687326480199e-06,
      "loss": 0.7664,
      "step": 12790
    },
    {
      "epoch": 0.6871745315939228,
      "grad_norm": 9.557731628417969,
      "learning_rate": 1.177078316394811e-06,
      "loss": 0.7734,
      "step": 12800
    },
    {
      "epoch": 0.6877113866967306,
      "grad_norm": 5.904951095581055,
      "learning_rate": 1.1733918997144092e-06,
      "loss": 1.5835,
      "step": 12810
    },
    {
      "epoch": 0.6882482417995383,
      "grad_norm": 16.28277015686035,
      "learning_rate": 1.1697094937519137e-06,
      "loss": 1.1999,
      "step": 12820
    },
    {
      "epoch": 0.688785096902346,
      "grad_norm": 7.681834697723389,
      "learning_rate": 1.166031109640297e-06,
      "loss": 0.8195,
      "step": 12830
    },
    {
      "epoch": 0.6893219520051538,
      "grad_norm": 5.995288372039795,
      "learning_rate": 1.1623567585003726e-06,
      "loss": 0.8131,
      "step": 12840
    },
    {
      "epoch": 0.6898588071079615,
      "grad_norm": 5.842278003692627,
      "learning_rate": 1.158686451440761e-06,
      "loss": 0.6382,
      "step": 12850
    },
    {
      "epoch": 0.6903956622107693,
      "grad_norm": 16.892210006713867,
      "learning_rate": 1.1550201995578566e-06,
      "loss": 1.4128,
      "step": 12860
    },
    {
      "epoch": 0.6909325173135771,
      "grad_norm": 6.019161701202393,
      "learning_rate": 1.1513580139357914e-06,
      "loss": 0.9018,
      "step": 12870
    },
    {
      "epoch": 0.6914693724163848,
      "grad_norm": 5.544093608856201,
      "learning_rate": 1.1476999056464094e-06,
      "loss": 1.2916,
      "step": 12880
    },
    {
      "epoch": 0.6920062275191926,
      "grad_norm": 5.346205711364746,
      "learning_rate": 1.1440458857492217e-06,
      "loss": 1.2564,
      "step": 12890
    },
    {
      "epoch": 0.6925430826220004,
      "grad_norm": 6.091616630554199,
      "learning_rate": 1.140395965291382e-06,
      "loss": 0.9416,
      "step": 12900
    },
    {
      "epoch": 0.693079937724808,
      "grad_norm": 11.857626914978027,
      "learning_rate": 1.1367501553076506e-06,
      "loss": 1.2828,
      "step": 12910
    },
    {
      "epoch": 0.6936167928276158,
      "grad_norm": 6.510515213012695,
      "learning_rate": 1.1331084668203593e-06,
      "loss": 1.1319,
      "step": 12920
    },
    {
      "epoch": 0.6941536479304236,
      "grad_norm": 10.920360565185547,
      "learning_rate": 1.12947091083938e-06,
      "loss": 1.0008,
      "step": 12930
    },
    {
      "epoch": 0.6946905030332313,
      "grad_norm": 7.122239589691162,
      "learning_rate": 1.1258374983620917e-06,
      "loss": 1.2193,
      "step": 12940
    },
    {
      "epoch": 0.6952273581360391,
      "grad_norm": 5.291947841644287,
      "learning_rate": 1.1222082403733434e-06,
      "loss": 0.9559,
      "step": 12950
    },
    {
      "epoch": 0.6957642132388469,
      "grad_norm": 7.448797702789307,
      "learning_rate": 1.1185831478454288e-06,
      "loss": 0.8432,
      "step": 12960
    },
    {
      "epoch": 0.6963010683416546,
      "grad_norm": 6.0150017738342285,
      "learning_rate": 1.1149622317380431e-06,
      "loss": 1.2043,
      "step": 12970
    },
    {
      "epoch": 0.6968379234444624,
      "grad_norm": 6.135735988616943,
      "learning_rate": 1.1113455029982583e-06,
      "loss": 1.0074,
      "step": 12980
    },
    {
      "epoch": 0.6973747785472701,
      "grad_norm": 5.808218002319336,
      "learning_rate": 1.1077329725604857e-06,
      "loss": 1.2463,
      "step": 12990
    },
    {
      "epoch": 0.6979116336500778,
      "grad_norm": 10.02350902557373,
      "learning_rate": 1.104124651346444e-06,
      "loss": 1.0582,
      "step": 13000
    },
    {
      "epoch": 0.6984484887528856,
      "grad_norm": 13.884827613830566,
      "learning_rate": 1.100520550265126e-06,
      "loss": 1.1559,
      "step": 13010
    },
    {
      "epoch": 0.6989853438556933,
      "grad_norm": 5.233762741088867,
      "learning_rate": 1.0969206802127678e-06,
      "loss": 1.0167,
      "step": 13020
    },
    {
      "epoch": 0.6995221989585011,
      "grad_norm": 6.094729423522949,
      "learning_rate": 1.093325052072808e-06,
      "loss": 0.9959,
      "step": 13030
    },
    {
      "epoch": 0.7000590540613089,
      "grad_norm": 14.690372467041016,
      "learning_rate": 1.0897336767158698e-06,
      "loss": 1.4729,
      "step": 13040
    },
    {
      "epoch": 0.7005959091641166,
      "grad_norm": 5.273060321807861,
      "learning_rate": 1.0861465649997104e-06,
      "loss": 1.2979,
      "step": 13050
    },
    {
      "epoch": 0.7011327642669244,
      "grad_norm": 4.609137535095215,
      "learning_rate": 1.0825637277692008e-06,
      "loss": 0.7733,
      "step": 13060
    },
    {
      "epoch": 0.7016696193697322,
      "grad_norm": 10.123322486877441,
      "learning_rate": 1.0789851758562913e-06,
      "loss": 0.7755,
      "step": 13070
    },
    {
      "epoch": 0.7022064744725398,
      "grad_norm": 9.748332023620605,
      "learning_rate": 1.0754109200799708e-06,
      "loss": 1.058,
      "step": 13080
    },
    {
      "epoch": 0.7027433295753476,
      "grad_norm": 6.044355869293213,
      "learning_rate": 1.0718409712462438e-06,
      "loss": 1.204,
      "step": 13090
    },
    {
      "epoch": 0.7032801846781553,
      "grad_norm": 7.637211322784424,
      "learning_rate": 1.0682753401480928e-06,
      "loss": 0.7768,
      "step": 13100
    },
    {
      "epoch": 0.7038170397809631,
      "grad_norm": 4.9901041984558105,
      "learning_rate": 1.064714037565446e-06,
      "loss": 0.7998,
      "step": 13110
    },
    {
      "epoch": 0.7043538948837709,
      "grad_norm": 12.343181610107422,
      "learning_rate": 1.0611570742651455e-06,
      "loss": 1.2348,
      "step": 13120
    },
    {
      "epoch": 0.7048907499865786,
      "grad_norm": 6.451207637786865,
      "learning_rate": 1.0576044610009157e-06,
      "loss": 1.0921,
      "step": 13130
    },
    {
      "epoch": 0.7054276050893864,
      "grad_norm": 8.132711410522461,
      "learning_rate": 1.0540562085133256e-06,
      "loss": 1.2067,
      "step": 13140
    },
    {
      "epoch": 0.7059644601921942,
      "grad_norm": 6.082878112792969,
      "learning_rate": 1.050512327529767e-06,
      "loss": 1.0526,
      "step": 13150
    },
    {
      "epoch": 0.7065013152950019,
      "grad_norm": 8.207721710205078,
      "learning_rate": 1.0469728287644087e-06,
      "loss": 1.4003,
      "step": 13160
    },
    {
      "epoch": 0.7070381703978096,
      "grad_norm": 14.246277809143066,
      "learning_rate": 1.0434377229181745e-06,
      "loss": 1.18,
      "step": 13170
    },
    {
      "epoch": 0.7075750255006173,
      "grad_norm": 14.038466453552246,
      "learning_rate": 1.0399070206787066e-06,
      "loss": 1.0161,
      "step": 13180
    },
    {
      "epoch": 0.7081118806034251,
      "grad_norm": 7.873215198516846,
      "learning_rate": 1.0363807327203328e-06,
      "loss": 0.7694,
      "step": 13190
    },
    {
      "epoch": 0.7086487357062329,
      "grad_norm": 4.880125999450684,
      "learning_rate": 1.0328588697040371e-06,
      "loss": 1.1258,
      "step": 13200
    },
    {
      "epoch": 0.7091855908090406,
      "grad_norm": 4.894803524017334,
      "learning_rate": 1.0293414422774245e-06,
      "loss": 0.8678,
      "step": 13210
    },
    {
      "epoch": 0.7097224459118484,
      "grad_norm": 5.978727340698242,
      "learning_rate": 1.0258284610746878e-06,
      "loss": 0.8609,
      "step": 13220
    },
    {
      "epoch": 0.7102593010146562,
      "grad_norm": 6.270735263824463,
      "learning_rate": 1.022319936716583e-06,
      "loss": 0.6022,
      "step": 13230
    },
    {
      "epoch": 0.7107961561174639,
      "grad_norm": 6.012319087982178,
      "learning_rate": 1.0188158798103856e-06,
      "loss": 0.83,
      "step": 13240
    },
    {
      "epoch": 0.7113330112202716,
      "grad_norm": 12.503332138061523,
      "learning_rate": 1.0153163009498692e-06,
      "loss": 1.005,
      "step": 13250
    },
    {
      "epoch": 0.7118698663230794,
      "grad_norm": 5.174591064453125,
      "learning_rate": 1.0118212107152669e-06,
      "loss": 0.6914,
      "step": 13260
    },
    {
      "epoch": 0.7124067214258871,
      "grad_norm": 5.366012096405029,
      "learning_rate": 1.0083306196732423e-06,
      "loss": 0.8076,
      "step": 13270
    },
    {
      "epoch": 0.7129435765286949,
      "grad_norm": 7.188870906829834,
      "learning_rate": 1.0048445383768556e-06,
      "loss": 1.0181,
      "step": 13280
    },
    {
      "epoch": 0.7134804316315027,
      "grad_norm": 12.918221473693848,
      "learning_rate": 1.001362977365535e-06,
      "loss": 0.9993,
      "step": 13290
    },
    {
      "epoch": 0.7140172867343104,
      "grad_norm": 6.333808422088623,
      "learning_rate": 9.97885947165038e-07,
      "loss": 0.8346,
      "step": 13300
    },
    {
      "epoch": 0.7145541418371182,
      "grad_norm": 6.002601146697998,
      "learning_rate": 9.944134582874295e-07,
      "loss": 1.0502,
      "step": 13310
    },
    {
      "epoch": 0.715090996939926,
      "grad_norm": 5.675798416137695,
      "learning_rate": 9.909455212310426e-07,
      "loss": 0.5749,
      "step": 13320
    },
    {
      "epoch": 0.7156278520427337,
      "grad_norm": 15.930012702941895,
      "learning_rate": 9.87482146480446e-07,
      "loss": 0.9684,
      "step": 13330
    },
    {
      "epoch": 0.7161647071455414,
      "grad_norm": 8.702911376953125,
      "learning_rate": 9.840233445064213e-07,
      "loss": 0.9736,
      "step": 13340
    },
    {
      "epoch": 0.7167015622483491,
      "grad_norm": 6.550909519195557,
      "learning_rate": 9.805691257659188e-07,
      "loss": 0.827,
      "step": 13350
    },
    {
      "epoch": 0.7172384173511569,
      "grad_norm": 6.246655464172363,
      "learning_rate": 9.771195007020374e-07,
      "loss": 1.3828,
      "step": 13360
    },
    {
      "epoch": 0.7177752724539647,
      "grad_norm": 11.373625755310059,
      "learning_rate": 9.736744797439848e-07,
      "loss": 1.0558,
      "step": 13370
    },
    {
      "epoch": 0.7183121275567724,
      "grad_norm": 5.425200939178467,
      "learning_rate": 9.702340733070508e-07,
      "loss": 1.4408,
      "step": 13380
    },
    {
      "epoch": 0.7188489826595802,
      "grad_norm": 8.932554244995117,
      "learning_rate": 9.66798291792574e-07,
      "loss": 1.1383,
      "step": 13390
    },
    {
      "epoch": 0.719385837762388,
      "grad_norm": 4.846194744110107,
      "learning_rate": 9.633671455879108e-07,
      "loss": 1.7011,
      "step": 13400
    },
    {
      "epoch": 0.7199226928651957,
      "grad_norm": 15.551196098327637,
      "learning_rate": 9.59940645066401e-07,
      "loss": 1.4429,
      "step": 13410
    },
    {
      "epoch": 0.7204595479680035,
      "grad_norm": 13.631563186645508,
      "learning_rate": 9.565188005873446e-07,
      "loss": 1.0166,
      "step": 13420
    },
    {
      "epoch": 0.7209964030708111,
      "grad_norm": 6.831662654876709,
      "learning_rate": 9.531016224959583e-07,
      "loss": 1.0459,
      "step": 13430
    },
    {
      "epoch": 0.7215332581736189,
      "grad_norm": 7.9199604988098145,
      "learning_rate": 9.496891211233578e-07,
      "loss": 1.1572,
      "step": 13440
    },
    {
      "epoch": 0.7220701132764267,
      "grad_norm": 5.234699249267578,
      "learning_rate": 9.462813067865139e-07,
      "loss": 0.8202,
      "step": 13450
    },
    {
      "epoch": 0.7226069683792344,
      "grad_norm": 7.433228492736816,
      "learning_rate": 9.428781897882308e-07,
      "loss": 1.2445,
      "step": 13460
    },
    {
      "epoch": 0.7231438234820422,
      "grad_norm": 11.753120422363281,
      "learning_rate": 9.394797804171096e-07,
      "loss": 1.1995,
      "step": 13470
    },
    {
      "epoch": 0.72368067858485,
      "grad_norm": 5.27089786529541,
      "learning_rate": 9.360860889475201e-07,
      "loss": 0.9782,
      "step": 13480
    },
    {
      "epoch": 0.7242175336876577,
      "grad_norm": 4.742655277252197,
      "learning_rate": 9.326971256395675e-07,
      "loss": 0.7745,
      "step": 13490
    },
    {
      "epoch": 0.7247543887904655,
      "grad_norm": 6.478848934173584,
      "learning_rate": 9.293129007390636e-07,
      "loss": 1.0859,
      "step": 13500
    },
    {
      "epoch": 0.7252912438932732,
      "grad_norm": 14.039655685424805,
      "learning_rate": 9.259334244774912e-07,
      "loss": 1.2472,
      "step": 13510
    },
    {
      "epoch": 0.7258280989960809,
      "grad_norm": 17.909832000732422,
      "learning_rate": 9.225587070719827e-07,
      "loss": 0.9831,
      "step": 13520
    },
    {
      "epoch": 0.7263649540988887,
      "grad_norm": 7.570437908172607,
      "learning_rate": 9.19188758725277e-07,
      "loss": 1.3642,
      "step": 13530
    },
    {
      "epoch": 0.7269018092016964,
      "grad_norm": 5.7500529289245605,
      "learning_rate": 9.158235896256984e-07,
      "loss": 1.2551,
      "step": 13540
    },
    {
      "epoch": 0.7274386643045042,
      "grad_norm": 4.77415657043457,
      "learning_rate": 9.124632099471209e-07,
      "loss": 1.0367,
      "step": 13550
    },
    {
      "epoch": 0.727975519407312,
      "grad_norm": 5.784523010253906,
      "learning_rate": 9.09107629848939e-07,
      "loss": 1.0457,
      "step": 13560
    },
    {
      "epoch": 0.7285123745101197,
      "grad_norm": 6.024356365203857,
      "learning_rate": 9.057568594760366e-07,
      "loss": 1.026,
      "step": 13570
    },
    {
      "epoch": 0.7290492296129275,
      "grad_norm": 7.0575151443481445,
      "learning_rate": 9.024109089587566e-07,
      "loss": 1.4477,
      "step": 13580
    },
    {
      "epoch": 0.7295860847157353,
      "grad_norm": 14.014503479003906,
      "learning_rate": 8.990697884128696e-07,
      "loss": 1.0003,
      "step": 13590
    },
    {
      "epoch": 0.7301229398185429,
      "grad_norm": 7.102997303009033,
      "learning_rate": 8.957335079395446e-07,
      "loss": 1.3623,
      "step": 13600
    },
    {
      "epoch": 0.7306597949213507,
      "grad_norm": 6.331473350524902,
      "learning_rate": 8.924020776253179e-07,
      "loss": 1.0217,
      "step": 13610
    },
    {
      "epoch": 0.7311966500241585,
      "grad_norm": 6.434484481811523,
      "learning_rate": 8.890755075420587e-07,
      "loss": 1.2782,
      "step": 13620
    },
    {
      "epoch": 0.7317335051269662,
      "grad_norm": 5.173196792602539,
      "learning_rate": 8.857538077469491e-07,
      "loss": 1.0443,
      "step": 13630
    },
    {
      "epoch": 0.732270360229774,
      "grad_norm": 14.574265480041504,
      "learning_rate": 8.824369882824404e-07,
      "loss": 1.2161,
      "step": 13640
    },
    {
      "epoch": 0.7328072153325818,
      "grad_norm": 5.777060031890869,
      "learning_rate": 8.791250591762332e-07,
      "loss": 1.2387,
      "step": 13650
    },
    {
      "epoch": 0.7333440704353895,
      "grad_norm": 6.020390510559082,
      "learning_rate": 8.758180304412414e-07,
      "loss": 0.8216,
      "step": 13660
    },
    {
      "epoch": 0.7338809255381973,
      "grad_norm": 8.963418960571289,
      "learning_rate": 8.725159120755647e-07,
      "loss": 0.7959,
      "step": 13670
    },
    {
      "epoch": 0.734417780641005,
      "grad_norm": 9.20470905303955,
      "learning_rate": 8.692187140624564e-07,
      "loss": 1.3077,
      "step": 13680
    },
    {
      "epoch": 0.7349546357438127,
      "grad_norm": 7.884720802307129,
      "learning_rate": 8.659264463702954e-07,
      "loss": 0.7919,
      "step": 13690
    },
    {
      "epoch": 0.7354914908466205,
      "grad_norm": 14.757783889770508,
      "learning_rate": 8.626391189525518e-07,
      "loss": 1.2739,
      "step": 13700
    },
    {
      "epoch": 0.7360283459494282,
      "grad_norm": 7.042613983154297,
      "learning_rate": 8.593567417477652e-07,
      "loss": 1.0657,
      "step": 13710
    },
    {
      "epoch": 0.736565201052236,
      "grad_norm": 6.077809810638428,
      "learning_rate": 8.560793246795038e-07,
      "loss": 1.8561,
      "step": 13720
    },
    {
      "epoch": 0.7371020561550438,
      "grad_norm": 6.676036357879639,
      "learning_rate": 8.528068776563425e-07,
      "loss": 0.9913,
      "step": 13730
    },
    {
      "epoch": 0.7376389112578515,
      "grad_norm": 6.183002471923828,
      "learning_rate": 8.495394105718308e-07,
      "loss": 0.6377,
      "step": 13740
    },
    {
      "epoch": 0.7381757663606593,
      "grad_norm": 6.67649507522583,
      "learning_rate": 8.462769333044609e-07,
      "loss": 0.9959,
      "step": 13750
    },
    {
      "epoch": 0.7387126214634671,
      "grad_norm": 6.807796001434326,
      "learning_rate": 8.430194557176408e-07,
      "loss": 0.9962,
      "step": 13760
    },
    {
      "epoch": 0.7392494765662747,
      "grad_norm": 8.101261138916016,
      "learning_rate": 8.397669876596623e-07,
      "loss": 0.8119,
      "step": 13770
    },
    {
      "epoch": 0.7397863316690825,
      "grad_norm": 13.647658348083496,
      "learning_rate": 8.365195389636701e-07,
      "loss": 1.583,
      "step": 13780
    },
    {
      "epoch": 0.7403231867718902,
      "grad_norm": 14.911652565002441,
      "learning_rate": 8.332771194476392e-07,
      "loss": 0.9692,
      "step": 13790
    },
    {
      "epoch": 0.740860041874698,
      "grad_norm": 4.764041423797607,
      "learning_rate": 8.300397389143334e-07,
      "loss": 1.1375,
      "step": 13800
    },
    {
      "epoch": 0.7413968969775058,
      "grad_norm": 5.9352192878723145,
      "learning_rate": 8.268074071512866e-07,
      "loss": 1.2378,
      "step": 13810
    },
    {
      "epoch": 0.7419337520803135,
      "grad_norm": 9.963265419006348,
      "learning_rate": 8.235801339307675e-07,
      "loss": 1.0275,
      "step": 13820
    },
    {
      "epoch": 0.7424706071831213,
      "grad_norm": 7.309261322021484,
      "learning_rate": 8.203579290097515e-07,
      "loss": 0.8908,
      "step": 13830
    },
    {
      "epoch": 0.7430074622859291,
      "grad_norm": 7.186404705047607,
      "learning_rate": 8.171408021298907e-07,
      "loss": 1.0339,
      "step": 13840
    },
    {
      "epoch": 0.7435443173887368,
      "grad_norm": 5.620382785797119,
      "learning_rate": 8.139287630174855e-07,
      "loss": 0.9473,
      "step": 13850
    },
    {
      "epoch": 0.7440811724915445,
      "grad_norm": 5.416253566741943,
      "learning_rate": 8.107218213834539e-07,
      "loss": 1.1954,
      "step": 13860
    },
    {
      "epoch": 0.7446180275943522,
      "grad_norm": 5.165733814239502,
      "learning_rate": 8.07519986923303e-07,
      "loss": 1.2095,
      "step": 13870
    },
    {
      "epoch": 0.74515488269716,
      "grad_norm": 5.741585731506348,
      "learning_rate": 8.043232693171002e-07,
      "loss": 1.0998,
      "step": 13880
    },
    {
      "epoch": 0.7456917377999678,
      "grad_norm": 5.891583442687988,
      "learning_rate": 8.011316782294401e-07,
      "loss": 0.7956,
      "step": 13890
    },
    {
      "epoch": 0.7462285929027755,
      "grad_norm": 11.913064956665039,
      "learning_rate": 7.979452233094239e-07,
      "loss": 1.1818,
      "step": 13900
    },
    {
      "epoch": 0.7467654480055833,
      "grad_norm": 13.882493019104004,
      "learning_rate": 7.947639141906188e-07,
      "loss": 1.0865,
      "step": 13910
    },
    {
      "epoch": 0.7473023031083911,
      "grad_norm": 9.192534446716309,
      "learning_rate": 7.915877604910385e-07,
      "loss": 1.1859,
      "step": 13920
    },
    {
      "epoch": 0.7478391582111988,
      "grad_norm": 8.157587051391602,
      "learning_rate": 7.884167718131092e-07,
      "loss": 0.6067,
      "step": 13930
    },
    {
      "epoch": 0.7483760133140066,
      "grad_norm": 5.338557720184326,
      "learning_rate": 7.852509577436421e-07,
      "loss": 1.0229,
      "step": 13940
    },
    {
      "epoch": 0.7489128684168143,
      "grad_norm": 5.674491882324219,
      "learning_rate": 7.820903278538033e-07,
      "loss": 1.0202,
      "step": 13950
    },
    {
      "epoch": 0.749449723519622,
      "grad_norm": 5.5500874519348145,
      "learning_rate": 7.789348916990871e-07,
      "loss": 1.2064,
      "step": 13960
    },
    {
      "epoch": 0.7499865786224298,
      "grad_norm": 5.895168781280518,
      "learning_rate": 7.757846588192828e-07,
      "loss": 1.2032,
      "step": 13970
    },
    {
      "epoch": 0.7505234337252376,
      "grad_norm": 5.002926826477051,
      "learning_rate": 7.726396387384533e-07,
      "loss": 1.1921,
      "step": 13980
    },
    {
      "epoch": 0.7510602888280453,
      "grad_norm": 5.821321964263916,
      "learning_rate": 7.694998409648968e-07,
      "loss": 1.2462,
      "step": 13990
    },
    {
      "epoch": 0.7515971439308531,
      "grad_norm": 13.609045028686523,
      "learning_rate": 7.66365274991126e-07,
      "loss": 1.2219,
      "step": 14000
    },
    {
      "epoch": 0.7521339990336608,
      "grad_norm": 3.8345649242401123,
      "learning_rate": 7.632359502938355e-07,
      "loss": 1.2953,
      "step": 14010
    },
    {
      "epoch": 0.7526708541364686,
      "grad_norm": 12.086748123168945,
      "learning_rate": 7.601118763338742e-07,
      "loss": 0.8663,
      "step": 14020
    },
    {
      "epoch": 0.7532077092392763,
      "grad_norm": 5.392734527587891,
      "learning_rate": 7.569930625562158e-07,
      "loss": 1.0412,
      "step": 14030
    },
    {
      "epoch": 0.753744564342084,
      "grad_norm": 5.756548881530762,
      "learning_rate": 7.53879518389933e-07,
      "loss": 0.6565,
      "step": 14040
    },
    {
      "epoch": 0.7542814194448918,
      "grad_norm": 7.041780948638916,
      "learning_rate": 7.507712532481632e-07,
      "loss": 1.0,
      "step": 14050
    },
    {
      "epoch": 0.7548182745476996,
      "grad_norm": 12.369487762451172,
      "learning_rate": 7.476682765280882e-07,
      "loss": 0.7847,
      "step": 14060
    },
    {
      "epoch": 0.7553551296505073,
      "grad_norm": 5.834813594818115,
      "learning_rate": 7.445705976108981e-07,
      "loss": 1.2401,
      "step": 14070
    },
    {
      "epoch": 0.7558919847533151,
      "grad_norm": 7.147216796875,
      "learning_rate": 7.414782258617673e-07,
      "loss": 0.7987,
      "step": 14080
    },
    {
      "epoch": 0.7564288398561229,
      "grad_norm": 8.170140266418457,
      "learning_rate": 7.383911706298255e-07,
      "loss": 0.767,
      "step": 14090
    },
    {
      "epoch": 0.7569656949589306,
      "grad_norm": 7.053866386413574,
      "learning_rate": 7.353094412481288e-07,
      "loss": 0.8332,
      "step": 14100
    },
    {
      "epoch": 0.7575025500617384,
      "grad_norm": 5.854894161224365,
      "learning_rate": 7.322330470336314e-07,
      "loss": 0.943,
      "step": 14110
    },
    {
      "epoch": 0.758039405164546,
      "grad_norm": 12.823599815368652,
      "learning_rate": 7.291619972871581e-07,
      "loss": 1.1741,
      "step": 14120
    },
    {
      "epoch": 0.7585762602673538,
      "grad_norm": 5.5932722091674805,
      "learning_rate": 7.260963012933758e-07,
      "loss": 1.4166,
      "step": 14130
    },
    {
      "epoch": 0.7591131153701616,
      "grad_norm": 5.113554954528809,
      "learning_rate": 7.23035968320765e-07,
      "loss": 1.1078,
      "step": 14140
    },
    {
      "epoch": 0.7596499704729693,
      "grad_norm": 6.615344047546387,
      "learning_rate": 7.199810076215932e-07,
      "loss": 1.1827,
      "step": 14150
    },
    {
      "epoch": 0.7601868255757771,
      "grad_norm": 4.974226474761963,
      "learning_rate": 7.169314284318849e-07,
      "loss": 0.9641,
      "step": 14160
    },
    {
      "epoch": 0.7607236806785849,
      "grad_norm": 6.178246021270752,
      "learning_rate": 7.138872399713964e-07,
      "loss": 1.0313,
      "step": 14170
    },
    {
      "epoch": 0.7612605357813926,
      "grad_norm": 5.516609191894531,
      "learning_rate": 7.108484514435826e-07,
      "loss": 0.9752,
      "step": 14180
    },
    {
      "epoch": 0.7617973908842004,
      "grad_norm": 11.759650230407715,
      "learning_rate": 7.078150720355784e-07,
      "loss": 0.9818,
      "step": 14190
    },
    {
      "epoch": 0.762334245987008,
      "grad_norm": 14.698812484741211,
      "learning_rate": 7.047871109181604e-07,
      "loss": 1.2276,
      "step": 14200
    },
    {
      "epoch": 0.7628711010898158,
      "grad_norm": 4.3963470458984375,
      "learning_rate": 7.017645772457263e-07,
      "loss": 0.5772,
      "step": 14210
    },
    {
      "epoch": 0.7634079561926236,
      "grad_norm": 6.931156635284424,
      "learning_rate": 6.987474801562652e-07,
      "loss": 1.0245,
      "step": 14220
    },
    {
      "epoch": 0.7639448112954313,
      "grad_norm": 13.668159484863281,
      "learning_rate": 6.957358287713295e-07,
      "loss": 1.0339,
      "step": 14230
    },
    {
      "epoch": 0.7644816663982391,
      "grad_norm": 15.227300643920898,
      "learning_rate": 6.927296321960078e-07,
      "loss": 1.1539,
      "step": 14240
    },
    {
      "epoch": 0.7650185215010469,
      "grad_norm": 5.660309791564941,
      "learning_rate": 6.897288995188975e-07,
      "loss": 0.5936,
      "step": 14250
    },
    {
      "epoch": 0.7655553766038546,
      "grad_norm": 6.377910614013672,
      "learning_rate": 6.867336398120749e-07,
      "loss": 0.765,
      "step": 14260
    },
    {
      "epoch": 0.7660922317066624,
      "grad_norm": 5.526716232299805,
      "learning_rate": 6.837438621310746e-07,
      "loss": 0.876,
      "step": 14270
    },
    {
      "epoch": 0.7666290868094702,
      "grad_norm": 7.58469295501709,
      "learning_rate": 6.807595755148522e-07,
      "loss": 1.3837,
      "step": 14280
    },
    {
      "epoch": 0.7671659419122778,
      "grad_norm": 4.648711681365967,
      "learning_rate": 6.777807889857652e-07,
      "loss": 0.7417,
      "step": 14290
    },
    {
      "epoch": 0.7677027970150856,
      "grad_norm": 5.295623779296875,
      "learning_rate": 6.748075115495425e-07,
      "loss": 0.7826,
      "step": 14300
    },
    {
      "epoch": 0.7682396521178934,
      "grad_norm": 8.764469146728516,
      "learning_rate": 6.718397521952568e-07,
      "loss": 1.252,
      "step": 14310
    },
    {
      "epoch": 0.7687765072207011,
      "grad_norm": 6.6944756507873535,
      "learning_rate": 6.68877519895299e-07,
      "loss": 1.0122,
      "step": 14320
    },
    {
      "epoch": 0.7693133623235089,
      "grad_norm": 5.427619934082031,
      "learning_rate": 6.659208236053493e-07,
      "loss": 0.8202,
      "step": 14330
    },
    {
      "epoch": 0.7698502174263167,
      "grad_norm": 5.072685241699219,
      "learning_rate": 6.629696722643497e-07,
      "loss": 0.9939,
      "step": 14340
    },
    {
      "epoch": 0.7703870725291244,
      "grad_norm": 5.672342300415039,
      "learning_rate": 6.60024074794482e-07,
      "loss": 0.8259,
      "step": 14350
    },
    {
      "epoch": 0.7709239276319322,
      "grad_norm": 6.412765979766846,
      "learning_rate": 6.570840401011327e-07,
      "loss": 0.8182,
      "step": 14360
    },
    {
      "epoch": 0.77146078273474,
      "grad_norm": 6.315755844116211,
      "learning_rate": 6.541495770728734e-07,
      "loss": 0.8157,
      "step": 14370
    },
    {
      "epoch": 0.7719976378375476,
      "grad_norm": 13.682597160339355,
      "learning_rate": 6.512206945814298e-07,
      "loss": 0.963,
      "step": 14380
    },
    {
      "epoch": 0.7725344929403554,
      "grad_norm": 4.152225494384766,
      "learning_rate": 6.48297401481656e-07,
      "loss": 0.8044,
      "step": 14390
    },
    {
      "epoch": 0.7730713480431631,
      "grad_norm": 10.70572280883789,
      "learning_rate": 6.453797066115083e-07,
      "loss": 1.2063,
      "step": 14400
    },
    {
      "epoch": 0.7736082031459709,
      "grad_norm": 5.059110641479492,
      "learning_rate": 6.424676187920173e-07,
      "loss": 1.1985,
      "step": 14410
    },
    {
      "epoch": 0.7741450582487787,
      "grad_norm": 6.054802417755127,
      "learning_rate": 6.395611468272625e-07,
      "loss": 1.0035,
      "step": 14420
    },
    {
      "epoch": 0.7746819133515864,
      "grad_norm": 12.596382141113281,
      "learning_rate": 6.366602995043448e-07,
      "loss": 1.0446,
      "step": 14430
    },
    {
      "epoch": 0.7752187684543942,
      "grad_norm": 5.147801399230957,
      "learning_rate": 6.337650855933603e-07,
      "loss": 1.0262,
      "step": 14440
    },
    {
      "epoch": 0.775755623557202,
      "grad_norm": 5.56813383102417,
      "learning_rate": 6.308755138473718e-07,
      "loss": 1.0003,
      "step": 14450
    },
    {
      "epoch": 0.7762924786600096,
      "grad_norm": 9.963966369628906,
      "learning_rate": 6.279915930023886e-07,
      "loss": 0.5726,
      "step": 14460
    },
    {
      "epoch": 0.7768293337628174,
      "grad_norm": 5.578929424285889,
      "learning_rate": 6.251133317773312e-07,
      "loss": 0.8426,
      "step": 14470
    },
    {
      "epoch": 0.7773661888656251,
      "grad_norm": 14.503446578979492,
      "learning_rate": 6.222407388740115e-07,
      "loss": 1.036,
      "step": 14480
    },
    {
      "epoch": 0.7779030439684329,
      "grad_norm": 10.30693244934082,
      "learning_rate": 6.193738229771043e-07,
      "loss": 0.8269,
      "step": 14490
    },
    {
      "epoch": 0.7784398990712407,
      "grad_norm": 14.127557754516602,
      "learning_rate": 6.165125927541216e-07,
      "loss": 1.3341,
      "step": 14500
    },
    {
      "epoch": 0.7789767541740484,
      "grad_norm": 13.651432037353516,
      "learning_rate": 6.136570568553848e-07,
      "loss": 0.7898,
      "step": 14510
    },
    {
      "epoch": 0.7795136092768562,
      "grad_norm": 6.5036301612854,
      "learning_rate": 6.108072239140012e-07,
      "loss": 0.8168,
      "step": 14520
    },
    {
      "epoch": 0.780050464379664,
      "grad_norm": 9.995062828063965,
      "learning_rate": 6.079631025458338e-07,
      "loss": 0.8541,
      "step": 14530
    },
    {
      "epoch": 0.7805873194824717,
      "grad_norm": 4.836038112640381,
      "learning_rate": 6.05124701349482e-07,
      "loss": 0.7842,
      "step": 14540
    },
    {
      "epoch": 0.7811241745852794,
      "grad_norm": 6.333088397979736,
      "learning_rate": 6.022920289062472e-07,
      "loss": 1.0605,
      "step": 14550
    },
    {
      "epoch": 0.7816610296880872,
      "grad_norm": 6.1778435707092285,
      "learning_rate": 5.994650937801141e-07,
      "loss": 1.1092,
      "step": 14560
    },
    {
      "epoch": 0.7821978847908949,
      "grad_norm": 4.797774791717529,
      "learning_rate": 5.966439045177203e-07,
      "loss": 0.6042,
      "step": 14570
    },
    {
      "epoch": 0.7827347398937027,
      "grad_norm": 9.564352989196777,
      "learning_rate": 5.938284696483326e-07,
      "loss": 1.0258,
      "step": 14580
    },
    {
      "epoch": 0.7832715949965104,
      "grad_norm": 6.39804744720459,
      "learning_rate": 5.910187976838209e-07,
      "loss": 0.6038,
      "step": 14590
    },
    {
      "epoch": 0.7838084500993182,
      "grad_norm": 4.713473320007324,
      "learning_rate": 5.882148971186321e-07,
      "loss": 0.5362,
      "step": 14600
    },
    {
      "epoch": 0.784345305202126,
      "grad_norm": 6.555142402648926,
      "learning_rate": 5.854167764297622e-07,
      "loss": 0.6607,
      "step": 14610
    },
    {
      "epoch": 0.7848821603049337,
      "grad_norm": 6.934569835662842,
      "learning_rate": 5.826244440767376e-07,
      "loss": 1.5105,
      "step": 14620
    },
    {
      "epoch": 0.7854190154077415,
      "grad_norm": 5.729149341583252,
      "learning_rate": 5.798379085015804e-07,
      "loss": 1.0202,
      "step": 14630
    },
    {
      "epoch": 0.7859558705105492,
      "grad_norm": 6.059198379516602,
      "learning_rate": 5.770571781287901e-07,
      "loss": 0.6313,
      "step": 14640
    },
    {
      "epoch": 0.7864927256133569,
      "grad_norm": 7.271918296813965,
      "learning_rate": 5.742822613653141e-07,
      "loss": 0.655,
      "step": 14650
    },
    {
      "epoch": 0.7870295807161647,
      "grad_norm": 14.146615028381348,
      "learning_rate": 5.715131666005242e-07,
      "loss": 1.2268,
      "step": 14660
    },
    {
      "epoch": 0.7875664358189725,
      "grad_norm": 5.835081577301025,
      "learning_rate": 5.687499022061904e-07,
      "loss": 1.0907,
      "step": 14670
    },
    {
      "epoch": 0.7881032909217802,
      "grad_norm": 15.315571784973145,
      "learning_rate": 5.65992476536456e-07,
      "loss": 1.0633,
      "step": 14680
    },
    {
      "epoch": 0.788640146024588,
      "grad_norm": 4.5801920890808105,
      "learning_rate": 5.63240897927812e-07,
      "loss": 0.9281,
      "step": 14690
    },
    {
      "epoch": 0.7891770011273957,
      "grad_norm": 14.99062728881836,
      "learning_rate": 5.604951746990719e-07,
      "loss": 1.1764,
      "step": 14700
    },
    {
      "epoch": 0.7897138562302035,
      "grad_norm": 6.696996688842773,
      "learning_rate": 5.577553151513479e-07,
      "loss": 0.9656,
      "step": 14710
    },
    {
      "epoch": 0.7902507113330112,
      "grad_norm": 6.302368640899658,
      "learning_rate": 5.550213275680211e-07,
      "loss": 1.0242,
      "step": 14720
    },
    {
      "epoch": 0.7907875664358189,
      "grad_norm": 7.835973739624023,
      "learning_rate": 5.522932202147252e-07,
      "loss": 0.6361,
      "step": 14730
    },
    {
      "epoch": 0.7913244215386267,
      "grad_norm": 7.245169162750244,
      "learning_rate": 5.495710013393118e-07,
      "loss": 0.9753,
      "step": 14740
    },
    {
      "epoch": 0.7918612766414345,
      "grad_norm": 8.01725959777832,
      "learning_rate": 5.468546791718321e-07,
      "loss": 1.0695,
      "step": 14750
    },
    {
      "epoch": 0.7923981317442422,
      "grad_norm": 8.593071937561035,
      "learning_rate": 5.441442619245096e-07,
      "loss": 0.8281,
      "step": 14760
    },
    {
      "epoch": 0.79293498684705,
      "grad_norm": 7.386030197143555,
      "learning_rate": 5.414397577917152e-07,
      "loss": 0.9188,
      "step": 14770
    },
    {
      "epoch": 0.7934718419498578,
      "grad_norm": 4.840989589691162,
      "learning_rate": 5.387411749499427e-07,
      "loss": 1.1881,
      "step": 14780
    },
    {
      "epoch": 0.7940086970526655,
      "grad_norm": 6.873349666595459,
      "learning_rate": 5.360485215577851e-07,
      "loss": 1.2618,
      "step": 14790
    },
    {
      "epoch": 0.7945455521554733,
      "grad_norm": 13.610200881958008,
      "learning_rate": 5.333618057559065e-07,
      "loss": 1.2495,
      "step": 14800
    },
    {
      "epoch": 0.7950824072582809,
      "grad_norm": 7.336531639099121,
      "learning_rate": 5.306810356670239e-07,
      "loss": 0.6293,
      "step": 14810
    },
    {
      "epoch": 0.7956192623610887,
      "grad_norm": 14.844898223876953,
      "learning_rate": 5.280062193958737e-07,
      "loss": 1.3111,
      "step": 14820
    },
    {
      "epoch": 0.7961561174638965,
      "grad_norm": 5.348860740661621,
      "learning_rate": 5.253373650291982e-07,
      "loss": 0.8134,
      "step": 14830
    },
    {
      "epoch": 0.7966929725667042,
      "grad_norm": 5.151512622833252,
      "learning_rate": 5.226744806357095e-07,
      "loss": 1.1423,
      "step": 14840
    },
    {
      "epoch": 0.797229827669512,
      "grad_norm": 7.060750484466553,
      "learning_rate": 5.200175742660737e-07,
      "loss": 1.0752,
      "step": 14850
    },
    {
      "epoch": 0.7977666827723198,
      "grad_norm": 8.13083553314209,
      "learning_rate": 5.173666539528832e-07,
      "loss": 1.1241,
      "step": 14860
    },
    {
      "epoch": 0.7983035378751275,
      "grad_norm": 13.794220924377441,
      "learning_rate": 5.147217277106329e-07,
      "loss": 1.0418,
      "step": 14870
    },
    {
      "epoch": 0.7988403929779353,
      "grad_norm": 4.832096576690674,
      "learning_rate": 5.120828035356951e-07,
      "loss": 0.5808,
      "step": 14880
    },
    {
      "epoch": 0.799377248080743,
      "grad_norm": 14.462972640991211,
      "learning_rate": 5.09449889406298e-07,
      "loss": 1.4165,
      "step": 14890
    },
    {
      "epoch": 0.7999141031835507,
      "grad_norm": 9.341672897338867,
      "learning_rate": 5.068229932824961e-07,
      "loss": 1.1924,
      "step": 14900
    },
    {
      "epoch": 0.8004509582863585,
      "grad_norm": 5.1106414794921875,
      "learning_rate": 5.042021231061548e-07,
      "loss": 0.9598,
      "step": 14910
    },
    {
      "epoch": 0.8009878133891662,
      "grad_norm": 5.613618850708008,
      "learning_rate": 5.015872868009164e-07,
      "loss": 1.0166,
      "step": 14920
    },
    {
      "epoch": 0.801524668491974,
      "grad_norm": 6.023451805114746,
      "learning_rate": 4.989784922721833e-07,
      "loss": 0.5804,
      "step": 14930
    },
    {
      "epoch": 0.8020615235947818,
      "grad_norm": 7.043715476989746,
      "learning_rate": 4.96375747407094e-07,
      "loss": 1.4282,
      "step": 14940
    },
    {
      "epoch": 0.8025983786975895,
      "grad_norm": 7.3612518310546875,
      "learning_rate": 4.937790600744929e-07,
      "loss": 0.7644,
      "step": 14950
    },
    {
      "epoch": 0.8031352338003973,
      "grad_norm": 6.460395336151123,
      "learning_rate": 4.911884381249135e-07,
      "loss": 1.1767,
      "step": 14960
    },
    {
      "epoch": 0.8036720889032051,
      "grad_norm": 6.060906887054443,
      "learning_rate": 4.886038893905509e-07,
      "loss": 1.2538,
      "step": 14970
    },
    {
      "epoch": 0.8042089440060127,
      "grad_norm": 9.795743942260742,
      "learning_rate": 4.860254216852398e-07,
      "loss": 1.1308,
      "step": 14980
    },
    {
      "epoch": 0.8047457991088205,
      "grad_norm": 14.42686653137207,
      "learning_rate": 4.834530428044293e-07,
      "loss": 1.5139,
      "step": 14990
    },
    {
      "epoch": 0.8052826542116283,
      "grad_norm": 8.141839981079102,
      "learning_rate": 4.808867605251619e-07,
      "loss": 1.0009,
      "step": 15000
    },
    {
      "epoch": 0.805819509314436,
      "grad_norm": 4.5711212158203125,
      "learning_rate": 4.783265826060451e-07,
      "loss": 1.1594,
      "step": 15010
    },
    {
      "epoch": 0.8063563644172438,
      "grad_norm": 6.025176048278809,
      "learning_rate": 4.757725167872354e-07,
      "loss": 0.7674,
      "step": 15020
    },
    {
      "epoch": 0.8068932195200516,
      "grad_norm": 5.193818092346191,
      "learning_rate": 4.7322457079040705e-07,
      "loss": 0.8334,
      "step": 15030
    },
    {
      "epoch": 0.8074300746228593,
      "grad_norm": 13.285975456237793,
      "learning_rate": 4.706827523187343e-07,
      "loss": 1.1843,
      "step": 15040
    },
    {
      "epoch": 0.8079669297256671,
      "grad_norm": 5.036655902862549,
      "learning_rate": 4.6814706905686543e-07,
      "loss": 0.9891,
      "step": 15050
    },
    {
      "epoch": 0.8085037848284748,
      "grad_norm": 15.112704277038574,
      "learning_rate": 4.6561752867090053e-07,
      "loss": 1.0376,
      "step": 15060
    },
    {
      "epoch": 0.8090406399312825,
      "grad_norm": 9.768606185913086,
      "learning_rate": 4.630941388083676e-07,
      "loss": 0.9921,
      "step": 15070
    },
    {
      "epoch": 0.8095774950340903,
      "grad_norm": 5.773665904998779,
      "learning_rate": 4.605769070982011e-07,
      "loss": 1.0634,
      "step": 15080
    },
    {
      "epoch": 0.810114350136898,
      "grad_norm": 8.623888969421387,
      "learning_rate": 4.580658411507144e-07,
      "loss": 1.1296,
      "step": 15090
    },
    {
      "epoch": 0.8106512052397058,
      "grad_norm": 6.4867777824401855,
      "learning_rate": 4.555609485575849e-07,
      "loss": 1.1278,
      "step": 15100
    },
    {
      "epoch": 0.8111880603425136,
      "grad_norm": 8.722004890441895,
      "learning_rate": 4.5306223689182137e-07,
      "loss": 0.9839,
      "step": 15110
    },
    {
      "epoch": 0.8117249154453213,
      "grad_norm": 7.843863487243652,
      "learning_rate": 4.505697137077486e-07,
      "loss": 1.1946,
      "step": 15120
    },
    {
      "epoch": 0.8122617705481291,
      "grad_norm": 5.4133453369140625,
      "learning_rate": 4.480833865409817e-07,
      "loss": 1.1836,
      "step": 15130
    },
    {
      "epoch": 0.8127986256509369,
      "grad_norm": 7.0167436599731445,
      "learning_rate": 4.456032629084023e-07,
      "loss": 0.7816,
      "step": 15140
    },
    {
      "epoch": 0.8133354807537445,
      "grad_norm": 5.609349727630615,
      "learning_rate": 4.43129350308138e-07,
      "loss": 0.9915,
      "step": 15150
    },
    {
      "epoch": 0.8138723358565523,
      "grad_norm": 14.107940673828125,
      "learning_rate": 4.4066165621953845e-07,
      "loss": 0.7721,
      "step": 15160
    },
    {
      "epoch": 0.81440919095936,
      "grad_norm": 6.951791286468506,
      "learning_rate": 4.382001881031514e-07,
      "loss": 1.1434,
      "step": 15170
    },
    {
      "epoch": 0.8149460460621678,
      "grad_norm": 5.524426460266113,
      "learning_rate": 4.3574495340070437e-07,
      "loss": 0.7286,
      "step": 15180
    },
    {
      "epoch": 0.8154829011649756,
      "grad_norm": 7.650599956512451,
      "learning_rate": 4.3329595953507887e-07,
      "loss": 0.9624,
      "step": 15190
    },
    {
      "epoch": 0.8160197562677833,
      "grad_norm": 11.388049125671387,
      "learning_rate": 4.3085321391028527e-07,
      "loss": 0.8099,
      "step": 15200
    },
    {
      "epoch": 0.8165566113705911,
      "grad_norm": 7.6944756507873535,
      "learning_rate": 4.2841672391144895e-07,
      "loss": 1.1217,
      "step": 15210
    },
    {
      "epoch": 0.8170934664733989,
      "grad_norm": 5.906994342803955,
      "learning_rate": 4.2598649690477864e-07,
      "loss": 0.9964,
      "step": 15220
    },
    {
      "epoch": 0.8176303215762066,
      "grad_norm": 13.138711929321289,
      "learning_rate": 4.2356254023754956e-07,
      "loss": 0.94,
      "step": 15230
    },
    {
      "epoch": 0.8181671766790143,
      "grad_norm": 4.798529624938965,
      "learning_rate": 4.2114486123808087e-07,
      "loss": 0.5789,
      "step": 15240
    },
    {
      "epoch": 0.818704031781822,
      "grad_norm": 8.891928672790527,
      "learning_rate": 4.187334672157109e-07,
      "loss": 0.7818,
      "step": 15250
    },
    {
      "epoch": 0.8192408868846298,
      "grad_norm": 7.39893913269043,
      "learning_rate": 4.1632836546077774e-07,
      "loss": 0.7588,
      "step": 15260
    },
    {
      "epoch": 0.8197777419874376,
      "grad_norm": 8.864723205566406,
      "learning_rate": 4.1392956324459665e-07,
      "loss": 1.1293,
      "step": 15270
    },
    {
      "epoch": 0.8203145970902453,
      "grad_norm": 13.912874221801758,
      "learning_rate": 4.115370678194347e-07,
      "loss": 1.5874,
      "step": 15280
    },
    {
      "epoch": 0.8208514521930531,
      "grad_norm": 9.129638671875,
      "learning_rate": 4.091508864184962e-07,
      "loss": 1.219,
      "step": 15290
    },
    {
      "epoch": 0.8213883072958609,
      "grad_norm": 14.170513153076172,
      "learning_rate": 4.067710262558924e-07,
      "loss": 0.8687,
      "step": 15300
    },
    {
      "epoch": 0.8219251623986686,
      "grad_norm": 5.297933101654053,
      "learning_rate": 4.0439749452662575e-07,
      "loss": 0.8751,
      "step": 15310
    },
    {
      "epoch": 0.8224620175014764,
      "grad_norm": 14.562607765197754,
      "learning_rate": 4.020302984065652e-07,
      "loss": 0.8279,
      "step": 15320
    },
    {
      "epoch": 0.8229988726042841,
      "grad_norm": 6.039121150970459,
      "learning_rate": 3.9966944505242576e-07,
      "loss": 0.8008,
      "step": 15330
    },
    {
      "epoch": 0.8235357277070918,
      "grad_norm": 12.649148941040039,
      "learning_rate": 3.9731494160174605e-07,
      "loss": 1.3814,
      "step": 15340
    },
    {
      "epoch": 0.8240725828098996,
      "grad_norm": 5.797545433044434,
      "learning_rate": 3.9496679517286806e-07,
      "loss": 0.9989,
      "step": 15350
    },
    {
      "epoch": 0.8246094379127074,
      "grad_norm": 5.674045085906982,
      "learning_rate": 3.926250128649123e-07,
      "loss": 0.7848,
      "step": 15360
    },
    {
      "epoch": 0.8251462930155151,
      "grad_norm": 14.328997611999512,
      "learning_rate": 3.902896017577626e-07,
      "loss": 0.8516,
      "step": 15370
    },
    {
      "epoch": 0.8256831481183229,
      "grad_norm": 5.823481559753418,
      "learning_rate": 3.879605689120367e-07,
      "loss": 1.0528,
      "step": 15380
    },
    {
      "epoch": 0.8262200032211306,
      "grad_norm": 6.061529159545898,
      "learning_rate": 3.856379213690714e-07,
      "loss": 1.2037,
      "step": 15390
    },
    {
      "epoch": 0.8267568583239384,
      "grad_norm": 7.215034008026123,
      "learning_rate": 3.833216661508984e-07,
      "loss": 0.5627,
      "step": 15400
    },
    {
      "epoch": 0.8272937134267461,
      "grad_norm": 5.339962005615234,
      "learning_rate": 3.8101181026022366e-07,
      "loss": 1.3848,
      "step": 15410
    },
    {
      "epoch": 0.8278305685295538,
      "grad_norm": 5.421991348266602,
      "learning_rate": 3.7870836068040596e-07,
      "loss": 0.7389,
      "step": 15420
    },
    {
      "epoch": 0.8283674236323616,
      "grad_norm": 5.2090654373168945,
      "learning_rate": 3.7641132437543667e-07,
      "loss": 0.9713,
      "step": 15430
    },
    {
      "epoch": 0.8289042787351694,
      "grad_norm": 13.75158977508545,
      "learning_rate": 3.741207082899154e-07,
      "loss": 1.03,
      "step": 15440
    },
    {
      "epoch": 0.8294411338379771,
      "grad_norm": 7.3717360496521,
      "learning_rate": 3.71836519349035e-07,
      "loss": 0.8533,
      "step": 15450
    },
    {
      "epoch": 0.8299779889407849,
      "grad_norm": 7.324709892272949,
      "learning_rate": 3.695587644585566e-07,
      "loss": 0.73,
      "step": 15460
    },
    {
      "epoch": 0.8305148440435927,
      "grad_norm": 5.253335475921631,
      "learning_rate": 3.6728745050478575e-07,
      "loss": 1.0157,
      "step": 15470
    },
    {
      "epoch": 0.8310516991464004,
      "grad_norm": 6.440526962280273,
      "learning_rate": 3.650225843545607e-07,
      "loss": 1.1621,
      "step": 15480
    },
    {
      "epoch": 0.8315885542492082,
      "grad_norm": 14.390092849731445,
      "learning_rate": 3.6276417285522116e-07,
      "loss": 0.976,
      "step": 15490
    },
    {
      "epoch": 0.8321254093520158,
      "grad_norm": 15.713860511779785,
      "learning_rate": 3.605122228345967e-07,
      "loss": 0.9741,
      "step": 15500
    },
    {
      "epoch": 0.8326622644548236,
      "grad_norm": 14.099813461303711,
      "learning_rate": 3.5826674110097853e-07,
      "loss": 1.0934,
      "step": 15510
    },
    {
      "epoch": 0.8331991195576314,
      "grad_norm": 12.254583358764648,
      "learning_rate": 3.5602773444310457e-07,
      "loss": 1.5931,
      "step": 15520
    },
    {
      "epoch": 0.8337359746604391,
      "grad_norm": 16.35249137878418,
      "learning_rate": 3.537952096301356e-07,
      "loss": 1.2173,
      "step": 15530
    },
    {
      "epoch": 0.8342728297632469,
      "grad_norm": 8.78498649597168,
      "learning_rate": 3.5156917341163666e-07,
      "loss": 1.3068,
      "step": 15540
    },
    {
      "epoch": 0.8348096848660547,
      "grad_norm": 6.873340129852295,
      "learning_rate": 3.493496325175558e-07,
      "loss": 1.4443,
      "step": 15550
    },
    {
      "epoch": 0.8353465399688624,
      "grad_norm": 5.45495080947876,
      "learning_rate": 3.4713659365820384e-07,
      "loss": 1.092,
      "step": 15560
    },
    {
      "epoch": 0.8358833950716702,
      "grad_norm": 6.163602828979492,
      "learning_rate": 3.4493006352423226e-07,
      "loss": 0.9729,
      "step": 15570
    },
    {
      "epoch": 0.836420250174478,
      "grad_norm": 5.356025695800781,
      "learning_rate": 3.4273004878661865e-07,
      "loss": 0.596,
      "step": 15580
    },
    {
      "epoch": 0.8369571052772856,
      "grad_norm": 4.77785062789917,
      "learning_rate": 3.405365560966387e-07,
      "loss": 0.803,
      "step": 15590
    },
    {
      "epoch": 0.8374939603800934,
      "grad_norm": 14.395994186401367,
      "learning_rate": 3.383495920858526e-07,
      "loss": 1.3778,
      "step": 15600
    },
    {
      "epoch": 0.8380308154829011,
      "grad_norm": 5.17941427230835,
      "learning_rate": 3.361691633660813e-07,
      "loss": 0.9509,
      "step": 15610
    },
    {
      "epoch": 0.8385676705857089,
      "grad_norm": 12.606101036071777,
      "learning_rate": 3.339952765293883e-07,
      "loss": 0.7782,
      "step": 15620
    },
    {
      "epoch": 0.8391045256885167,
      "grad_norm": 11.191023826599121,
      "learning_rate": 3.318279381480591e-07,
      "loss": 1.035,
      "step": 15630
    },
    {
      "epoch": 0.8396413807913244,
      "grad_norm": 14.106231689453125,
      "learning_rate": 3.296671547745811e-07,
      "loss": 1.1408,
      "step": 15640
    },
    {
      "epoch": 0.8401782358941322,
      "grad_norm": 8.368142127990723,
      "learning_rate": 3.275129329416227e-07,
      "loss": 0.9647,
      "step": 15650
    },
    {
      "epoch": 0.84071509099694,
      "grad_norm": 6.511770248413086,
      "learning_rate": 3.253652791620182e-07,
      "loss": 1.0934,
      "step": 15660
    },
    {
      "epoch": 0.8412519460997476,
      "grad_norm": 6.934089660644531,
      "learning_rate": 3.232241999287414e-07,
      "loss": 1.5033,
      "step": 15670
    },
    {
      "epoch": 0.8417888012025554,
      "grad_norm": 5.094871997833252,
      "learning_rate": 3.210897017148909e-07,
      "loss": 0.9953,
      "step": 15680
    },
    {
      "epoch": 0.8423256563053632,
      "grad_norm": 6.364528179168701,
      "learning_rate": 3.189617909736695e-07,
      "loss": 1.2537,
      "step": 15690
    },
    {
      "epoch": 0.8428625114081709,
      "grad_norm": 13.439284324645996,
      "learning_rate": 3.1684047413836267e-07,
      "loss": 1.1445,
      "step": 15700
    },
    {
      "epoch": 0.8433993665109787,
      "grad_norm": 5.618056774139404,
      "learning_rate": 3.14725757622322e-07,
      "loss": 0.9951,
      "step": 15710
    },
    {
      "epoch": 0.8439362216137865,
      "grad_norm": 6.045928001403809,
      "learning_rate": 3.126176478189435e-07,
      "loss": 0.9883,
      "step": 15720
    },
    {
      "epoch": 0.8444730767165942,
      "grad_norm": 5.2609543800354,
      "learning_rate": 3.1051615110164995e-07,
      "loss": 0.8264,
      "step": 15730
    },
    {
      "epoch": 0.845009931819402,
      "grad_norm": 6.589994430541992,
      "learning_rate": 3.084212738238701e-07,
      "loss": 1.0839,
      "step": 15740
    },
    {
      "epoch": 0.8455467869222097,
      "grad_norm": 4.5332794189453125,
      "learning_rate": 3.063330223190211e-07,
      "loss": 0.6038,
      "step": 15750
    },
    {
      "epoch": 0.8460836420250174,
      "grad_norm": 13.612586975097656,
      "learning_rate": 3.042514029004867e-07,
      "loss": 1.1868,
      "step": 15760
    },
    {
      "epoch": 0.8466204971278252,
      "grad_norm": 5.75744104385376,
      "learning_rate": 3.02176421861603e-07,
      "loss": 0.9685,
      "step": 15770
    },
    {
      "epoch": 0.8471573522306329,
      "grad_norm": 6.578497409820557,
      "learning_rate": 3.001080854756333e-07,
      "loss": 0.8696,
      "step": 15780
    },
    {
      "epoch": 0.8476942073334407,
      "grad_norm": 14.110395431518555,
      "learning_rate": 2.9804639999575363e-07,
      "loss": 1.2128,
      "step": 15790
    },
    {
      "epoch": 0.8482310624362485,
      "grad_norm": 13.504613876342773,
      "learning_rate": 2.959913716550328e-07,
      "loss": 0.9753,
      "step": 15800
    },
    {
      "epoch": 0.8487679175390562,
      "grad_norm": 14.940838813781738,
      "learning_rate": 2.93943006666412e-07,
      "loss": 1.4672,
      "step": 15810
    },
    {
      "epoch": 0.849304772641864,
      "grad_norm": 5.956108093261719,
      "learning_rate": 2.9190131122268814e-07,
      "loss": 0.5676,
      "step": 15820
    },
    {
      "epoch": 0.8498416277446718,
      "grad_norm": 9.678078651428223,
      "learning_rate": 2.8986629149649475e-07,
      "loss": 0.8479,
      "step": 15830
    },
    {
      "epoch": 0.8503784828474794,
      "grad_norm": 6.044682502746582,
      "learning_rate": 2.878379536402798e-07,
      "loss": 0.554,
      "step": 15840
    },
    {
      "epoch": 0.8509153379502872,
      "grad_norm": 4.474028587341309,
      "learning_rate": 2.8581630378629405e-07,
      "loss": 1.0309,
      "step": 15850
    },
    {
      "epoch": 0.8514521930530949,
      "grad_norm": 4.543635845184326,
      "learning_rate": 2.838013480465657e-07,
      "loss": 0.8343,
      "step": 15860
    },
    {
      "epoch": 0.8519890481559027,
      "grad_norm": 6.112476825714111,
      "learning_rate": 2.8179309251288563e-07,
      "loss": 1.4364,
      "step": 15870
    },
    {
      "epoch": 0.8525259032587105,
      "grad_norm": 8.480449676513672,
      "learning_rate": 2.7979154325678797e-07,
      "loss": 1.1432,
      "step": 15880
    },
    {
      "epoch": 0.8530627583615182,
      "grad_norm": 5.434619426727295,
      "learning_rate": 2.7779670632953227e-07,
      "loss": 0.5996,
      "step": 15890
    },
    {
      "epoch": 0.853599613464326,
      "grad_norm": 5.943763732910156,
      "learning_rate": 2.758085877620839e-07,
      "loss": 0.9732,
      "step": 15900
    },
    {
      "epoch": 0.8541364685671338,
      "grad_norm": 6.547821998596191,
      "learning_rate": 2.7382719356509824e-07,
      "loss": 0.8181,
      "step": 15910
    },
    {
      "epoch": 0.8546733236699415,
      "grad_norm": 7.002528190612793,
      "learning_rate": 2.718525297288979e-07,
      "loss": 1.1697,
      "step": 15920
    },
    {
      "epoch": 0.8552101787727492,
      "grad_norm": 5.954078674316406,
      "learning_rate": 2.6988460222346216e-07,
      "loss": 1.2268,
      "step": 15930
    },
    {
      "epoch": 0.855747033875557,
      "grad_norm": 5.344947814941406,
      "learning_rate": 2.6792341699840047e-07,
      "loss": 0.8354,
      "step": 15940
    },
    {
      "epoch": 0.8562838889783647,
      "grad_norm": 9.842594146728516,
      "learning_rate": 2.6596897998293985e-07,
      "loss": 1.2116,
      "step": 15950
    },
    {
      "epoch": 0.8568207440811725,
      "grad_norm": 9.079874992370605,
      "learning_rate": 2.640212970859063e-07,
      "loss": 0.6259,
      "step": 15960
    },
    {
      "epoch": 0.8573575991839802,
      "grad_norm": 6.48187780380249,
      "learning_rate": 2.6208037419570506e-07,
      "loss": 0.9541,
      "step": 15970
    },
    {
      "epoch": 0.857894454286788,
      "grad_norm": 13.5930814743042,
      "learning_rate": 2.601462171803043e-07,
      "loss": 1.7646,
      "step": 15980
    },
    {
      "epoch": 0.8584313093895958,
      "grad_norm": 8.183120727539062,
      "learning_rate": 2.582188318872175e-07,
      "loss": 0.5917,
      "step": 15990
    },
    {
      "epoch": 0.8589681644924035,
      "grad_norm": 6.542034149169922,
      "learning_rate": 2.562982241434847e-07,
      "loss": 0.9252,
      "step": 16000
    },
    {
      "epoch": 0.8595050195952113,
      "grad_norm": 8.35802936553955,
      "learning_rate": 2.5438439975565556e-07,
      "loss": 1.297,
      "step": 16010
    },
    {
      "epoch": 0.860041874698019,
      "grad_norm": 13.594679832458496,
      "learning_rate": 2.5247736450977197e-07,
      "loss": 0.9472,
      "step": 16020
    },
    {
      "epoch": 0.8605787298008267,
      "grad_norm": 13.287386894226074,
      "learning_rate": 2.5057712417134895e-07,
      "loss": 1.3473,
      "step": 16030
    },
    {
      "epoch": 0.8611155849036345,
      "grad_norm": 6.062356948852539,
      "learning_rate": 2.486836844853616e-07,
      "loss": 1.1956,
      "step": 16040
    },
    {
      "epoch": 0.8616524400064423,
      "grad_norm": 4.892768383026123,
      "learning_rate": 2.4679705117622103e-07,
      "loss": 1.1925,
      "step": 16050
    },
    {
      "epoch": 0.86218929510925,
      "grad_norm": 13.090235710144043,
      "learning_rate": 2.449172299477634e-07,
      "loss": 1.1784,
      "step": 16060
    },
    {
      "epoch": 0.8627261502120578,
      "grad_norm": 17.01757049560547,
      "learning_rate": 2.430442264832286e-07,
      "loss": 1.2096,
      "step": 16070
    },
    {
      "epoch": 0.8632630053148656,
      "grad_norm": 13.214481353759766,
      "learning_rate": 2.411780464452448e-07,
      "loss": 1.4189,
      "step": 16080
    },
    {
      "epoch": 0.8637998604176733,
      "grad_norm": 5.731315612792969,
      "learning_rate": 2.3931869547581174e-07,
      "loss": 0.822,
      "step": 16090
    },
    {
      "epoch": 0.864336715520481,
      "grad_norm": 8.962678909301758,
      "learning_rate": 2.3746617919628234e-07,
      "loss": 1.1854,
      "step": 16100
    },
    {
      "epoch": 0.8648735706232887,
      "grad_norm": 6.1976118087768555,
      "learning_rate": 2.356205032073447e-07,
      "loss": 0.964,
      "step": 16110
    },
    {
      "epoch": 0.8654104257260965,
      "grad_norm": 13.814313888549805,
      "learning_rate": 2.3378167308901022e-07,
      "loss": 0.9556,
      "step": 16120
    },
    {
      "epoch": 0.8659472808289043,
      "grad_norm": 11.780576705932617,
      "learning_rate": 2.3194969440059e-07,
      "loss": 0.9886,
      "step": 16130
    },
    {
      "epoch": 0.866484135931712,
      "grad_norm": 5.901388168334961,
      "learning_rate": 2.3012457268068305e-07,
      "loss": 1.1546,
      "step": 16140
    },
    {
      "epoch": 0.8670209910345198,
      "grad_norm": 6.536515712738037,
      "learning_rate": 2.283063134471572e-07,
      "loss": 0.7811,
      "step": 16150
    },
    {
      "epoch": 0.8675578461373276,
      "grad_norm": 6.261140823364258,
      "learning_rate": 2.2649492219713355e-07,
      "loss": 1.4084,
      "step": 16160
    },
    {
      "epoch": 0.8680947012401353,
      "grad_norm": 7.525829315185547,
      "learning_rate": 2.2469040440696864e-07,
      "loss": 0.9362,
      "step": 16170
    },
    {
      "epoch": 0.8686315563429431,
      "grad_norm": 4.603457450866699,
      "learning_rate": 2.2289276553223938e-07,
      "loss": 1.8306,
      "step": 16180
    },
    {
      "epoch": 0.8691684114457507,
      "grad_norm": 4.993289947509766,
      "learning_rate": 2.211020110077236e-07,
      "loss": 0.5669,
      "step": 16190
    },
    {
      "epoch": 0.8697052665485585,
      "grad_norm": 4.902647972106934,
      "learning_rate": 2.193181462473895e-07,
      "loss": 0.8243,
      "step": 16200
    },
    {
      "epoch": 0.8702421216513663,
      "grad_norm": 5.600671291351318,
      "learning_rate": 2.1754117664437164e-07,
      "loss": 1.3468,
      "step": 16210
    },
    {
      "epoch": 0.870778976754174,
      "grad_norm": 4.874847412109375,
      "learning_rate": 2.1577110757096175e-07,
      "loss": 1.0147,
      "step": 16220
    },
    {
      "epoch": 0.8713158318569818,
      "grad_norm": 14.312248229980469,
      "learning_rate": 2.1400794437858696e-07,
      "loss": 1.2434,
      "step": 16230
    },
    {
      "epoch": 0.8718526869597896,
      "grad_norm": 13.422438621520996,
      "learning_rate": 2.122516923977966e-07,
      "loss": 0.8378,
      "step": 16240
    },
    {
      "epoch": 0.8723895420625973,
      "grad_norm": 5.652050018310547,
      "learning_rate": 2.1050235693824612e-07,
      "loss": 0.9621,
      "step": 16250
    },
    {
      "epoch": 0.8729263971654051,
      "grad_norm": 14.596415519714355,
      "learning_rate": 2.0875994328868e-07,
      "loss": 1.0221,
      "step": 16260
    },
    {
      "epoch": 0.8734632522682129,
      "grad_norm": 6.0642242431640625,
      "learning_rate": 2.0702445671691524e-07,
      "loss": 0.5787,
      "step": 16270
    },
    {
      "epoch": 0.8740001073710205,
      "grad_norm": 4.171885013580322,
      "learning_rate": 2.0529590246982755e-07,
      "loss": 1.0479,
      "step": 16280
    },
    {
      "epoch": 0.8745369624738283,
      "grad_norm": 14.880964279174805,
      "learning_rate": 2.035742857733336e-07,
      "loss": 1.0218,
      "step": 16290
    },
    {
      "epoch": 0.875073817576636,
      "grad_norm": 8.987008094787598,
      "learning_rate": 2.0185961183237568e-07,
      "loss": 1.2075,
      "step": 16300
    },
    {
      "epoch": 0.8756106726794438,
      "grad_norm": 5.683349609375,
      "learning_rate": 2.0015188583090707e-07,
      "loss": 0.6197,
      "step": 16310
    },
    {
      "epoch": 0.8761475277822516,
      "grad_norm": 4.731996059417725,
      "learning_rate": 1.9845111293187308e-07,
      "loss": 1.0677,
      "step": 16320
    },
    {
      "epoch": 0.8766843828850593,
      "grad_norm": 10.180704116821289,
      "learning_rate": 1.9675729827720147e-07,
      "loss": 0.9701,
      "step": 16330
    },
    {
      "epoch": 0.8772212379878671,
      "grad_norm": 6.053878307342529,
      "learning_rate": 1.9507044698777982e-07,
      "loss": 0.753,
      "step": 16340
    },
    {
      "epoch": 0.8777580930906749,
      "grad_norm": 4.995319366455078,
      "learning_rate": 1.9339056416344538e-07,
      "loss": 1.24,
      "step": 16350
    },
    {
      "epoch": 0.8782949481934825,
      "grad_norm": 5.0205912590026855,
      "learning_rate": 1.917176548829669e-07,
      "loss": 1.0195,
      "step": 16360
    },
    {
      "epoch": 0.8788318032962903,
      "grad_norm": 5.405204772949219,
      "learning_rate": 1.9005172420403035e-07,
      "loss": 1.0292,
      "step": 16370
    },
    {
      "epoch": 0.8793686583990981,
      "grad_norm": 6.854423999786377,
      "learning_rate": 1.883927771632238e-07,
      "loss": 1.6501,
      "step": 16380
    },
    {
      "epoch": 0.8799055135019058,
      "grad_norm": 9.502025604248047,
      "learning_rate": 1.867408187760214e-07,
      "loss": 1.1233,
      "step": 16390
    },
    {
      "epoch": 0.8804423686047136,
      "grad_norm": 6.623039722442627,
      "learning_rate": 1.8509585403676756e-07,
      "loss": 1.2102,
      "step": 16400
    },
    {
      "epoch": 0.8809792237075214,
      "grad_norm": 6.781182289123535,
      "learning_rate": 1.8345788791866548e-07,
      "loss": 0.563,
      "step": 16410
    },
    {
      "epoch": 0.8815160788103291,
      "grad_norm": 7.516454219818115,
      "learning_rate": 1.8182692537375673e-07,
      "loss": 0.7479,
      "step": 16420
    },
    {
      "epoch": 0.8820529339131369,
      "grad_norm": 5.878048896789551,
      "learning_rate": 1.8020297133291038e-07,
      "loss": 0.7812,
      "step": 16430
    },
    {
      "epoch": 0.8825897890159446,
      "grad_norm": 4.865749835968018,
      "learning_rate": 1.785860307058071e-07,
      "loss": 1.3748,
      "step": 16440
    },
    {
      "epoch": 0.8831266441187523,
      "grad_norm": 5.623748302459717,
      "learning_rate": 1.7697610838092321e-07,
      "loss": 1.1744,
      "step": 16450
    },
    {
      "epoch": 0.8836634992215601,
      "grad_norm": 7.565742015838623,
      "learning_rate": 1.7537320922551697e-07,
      "loss": 0.7838,
      "step": 16460
    },
    {
      "epoch": 0.8842003543243678,
      "grad_norm": 7.496234893798828,
      "learning_rate": 1.737773380856142e-07,
      "loss": 1.2782,
      "step": 16470
    },
    {
      "epoch": 0.8847372094271756,
      "grad_norm": 8.178376197814941,
      "learning_rate": 1.7218849978599127e-07,
      "loss": 0.7694,
      "step": 16480
    },
    {
      "epoch": 0.8852740645299834,
      "grad_norm": 7.756636142730713,
      "learning_rate": 1.70606699130165e-07,
      "loss": 1.4795,
      "step": 16490
    },
    {
      "epoch": 0.8858109196327911,
      "grad_norm": 6.48458194732666,
      "learning_rate": 1.6903194090037267e-07,
      "loss": 1.4667,
      "step": 16500
    },
    {
      "epoch": 0.8863477747355989,
      "grad_norm": 14.310111045837402,
      "learning_rate": 1.6746422985756155e-07,
      "loss": 1.3209,
      "step": 16510
    },
    {
      "epoch": 0.8868846298384067,
      "grad_norm": 7.360464096069336,
      "learning_rate": 1.6590357074137308e-07,
      "loss": 0.7611,
      "step": 16520
    },
    {
      "epoch": 0.8874214849412143,
      "grad_norm": 5.160701751708984,
      "learning_rate": 1.643499682701291e-07,
      "loss": 1.2491,
      "step": 16530
    },
    {
      "epoch": 0.8879583400440221,
      "grad_norm": 13.49434757232666,
      "learning_rate": 1.628034271408166e-07,
      "loss": 0.9898,
      "step": 16540
    },
    {
      "epoch": 0.8884951951468298,
      "grad_norm": 11.073318481445312,
      "learning_rate": 1.612639520290743e-07,
      "loss": 0.9863,
      "step": 16550
    },
    {
      "epoch": 0.8890320502496376,
      "grad_norm": 9.222113609313965,
      "learning_rate": 1.5973154758917808e-07,
      "loss": 1.1984,
      "step": 16560
    },
    {
      "epoch": 0.8895689053524454,
      "grad_norm": 4.880134582519531,
      "learning_rate": 1.5820621845402756e-07,
      "loss": 1.428,
      "step": 16570
    },
    {
      "epoch": 0.8901057604552531,
      "grad_norm": 6.376257419586182,
      "learning_rate": 1.5668796923513224e-07,
      "loss": 1.5253,
      "step": 16580
    },
    {
      "epoch": 0.8906426155580609,
      "grad_norm": 6.526719093322754,
      "learning_rate": 1.551768045225946e-07,
      "loss": 0.9487,
      "step": 16590
    },
    {
      "epoch": 0.8911794706608687,
      "grad_norm": 5.535374164581299,
      "learning_rate": 1.53672728885102e-07,
      "loss": 1.4526,
      "step": 16600
    },
    {
      "epoch": 0.8917163257636764,
      "grad_norm": 5.568209171295166,
      "learning_rate": 1.5217574686990644e-07,
      "loss": 1.1913,
      "step": 16610
    },
    {
      "epoch": 0.8922531808664841,
      "grad_norm": 5.700722694396973,
      "learning_rate": 1.5068586300281573e-07,
      "loss": 0.5533,
      "step": 16620
    },
    {
      "epoch": 0.8927900359692919,
      "grad_norm": 7.555735111236572,
      "learning_rate": 1.492030817881776e-07,
      "loss": 0.6182,
      "step": 16630
    },
    {
      "epoch": 0.8933268910720996,
      "grad_norm": 12.773331642150879,
      "learning_rate": 1.4772740770886584e-07,
      "loss": 1.2319,
      "step": 16640
    },
    {
      "epoch": 0.8938637461749074,
      "grad_norm": 4.722620964050293,
      "learning_rate": 1.462588452262681e-07,
      "loss": 0.9853,
      "step": 16650
    },
    {
      "epoch": 0.8944006012777151,
      "grad_norm": 5.152465343475342,
      "learning_rate": 1.4479739878027117e-07,
      "loss": 1.0407,
      "step": 16660
    },
    {
      "epoch": 0.8949374563805229,
      "grad_norm": 12.55517292022705,
      "learning_rate": 1.4334307278924707e-07,
      "loss": 1.3438,
      "step": 16670
    },
    {
      "epoch": 0.8954743114833307,
      "grad_norm": 6.65220308303833,
      "learning_rate": 1.418958716500432e-07,
      "loss": 0.9041,
      "step": 16680
    },
    {
      "epoch": 0.8960111665861384,
      "grad_norm": 13.85601806640625,
      "learning_rate": 1.404557997379638e-07,
      "loss": 0.9812,
      "step": 16690
    },
    {
      "epoch": 0.8965480216889462,
      "grad_norm": 5.970524311065674,
      "learning_rate": 1.390228614067607e-07,
      "loss": 0.7958,
      "step": 16700
    },
    {
      "epoch": 0.8970848767917539,
      "grad_norm": 7.2943339347839355,
      "learning_rate": 1.3759706098861902e-07,
      "loss": 1.0571,
      "step": 16710
    },
    {
      "epoch": 0.8976217318945616,
      "grad_norm": 9.76077938079834,
      "learning_rate": 1.3617840279414317e-07,
      "loss": 0.9845,
      "step": 16720
    },
    {
      "epoch": 0.8981585869973694,
      "grad_norm": 6.444101333618164,
      "learning_rate": 1.3476689111234537e-07,
      "loss": 1.0341,
      "step": 16730
    },
    {
      "epoch": 0.8986954421001772,
      "grad_norm": 5.951274394989014,
      "learning_rate": 1.3336253021063173e-07,
      "loss": 0.8978,
      "step": 16740
    },
    {
      "epoch": 0.8992322972029849,
      "grad_norm": 5.252955913543701,
      "learning_rate": 1.3196532433478793e-07,
      "loss": 0.7903,
      "step": 16750
    },
    {
      "epoch": 0.8997691523057927,
      "grad_norm": 5.719615936279297,
      "learning_rate": 1.3057527770897083e-07,
      "loss": 0.8711,
      "step": 16760
    },
    {
      "epoch": 0.9003060074086005,
      "grad_norm": 7.622428894042969,
      "learning_rate": 1.2919239453569043e-07,
      "loss": 1.0752,
      "step": 16770
    },
    {
      "epoch": 0.9008428625114082,
      "grad_norm": 9.959822654724121,
      "learning_rate": 1.2781667899580018e-07,
      "loss": 1.2476,
      "step": 16780
    },
    {
      "epoch": 0.9013797176142159,
      "grad_norm": 5.833847522735596,
      "learning_rate": 1.2644813524848448e-07,
      "loss": 0.7926,
      "step": 16790
    },
    {
      "epoch": 0.9019165727170236,
      "grad_norm": 7.675579071044922,
      "learning_rate": 1.2508676743124447e-07,
      "loss": 1.211,
      "step": 16800
    },
    {
      "epoch": 0.9024534278198314,
      "grad_norm": 5.106683731079102,
      "learning_rate": 1.237325796598865e-07,
      "loss": 1.1985,
      "step": 16810
    },
    {
      "epoch": 0.9029902829226392,
      "grad_norm": 9.795063972473145,
      "learning_rate": 1.223855760285103e-07,
      "loss": 1.1216,
      "step": 16820
    },
    {
      "epoch": 0.9035271380254469,
      "grad_norm": 10.792073249816895,
      "learning_rate": 1.2104576060949475e-07,
      "loss": 1.2195,
      "step": 16830
    },
    {
      "epoch": 0.9040639931282547,
      "grad_norm": 11.519238471984863,
      "learning_rate": 1.1971313745348768e-07,
      "loss": 0.7844,
      "step": 16840
    },
    {
      "epoch": 0.9046008482310625,
      "grad_norm": 6.861754894256592,
      "learning_rate": 1.1838771058939213e-07,
      "loss": 1.1751,
      "step": 16850
    },
    {
      "epoch": 0.9051377033338702,
      "grad_norm": 6.551353454589844,
      "learning_rate": 1.1706948402435442e-07,
      "loss": 0.9749,
      "step": 16860
    },
    {
      "epoch": 0.905674558436678,
      "grad_norm": 6.385002613067627,
      "learning_rate": 1.157584617437535e-07,
      "loss": 1.0021,
      "step": 16870
    },
    {
      "epoch": 0.9062114135394856,
      "grad_norm": 12.106761932373047,
      "learning_rate": 1.1445464771118576e-07,
      "loss": 0.5233,
      "step": 16880
    },
    {
      "epoch": 0.9067482686422934,
      "grad_norm": 5.353480339050293,
      "learning_rate": 1.1315804586845725e-07,
      "loss": 0.531,
      "step": 16890
    },
    {
      "epoch": 0.9072851237451012,
      "grad_norm": 15.447254180908203,
      "learning_rate": 1.1186866013556758e-07,
      "loss": 1.2271,
      "step": 16900
    },
    {
      "epoch": 0.9078219788479089,
      "grad_norm": 6.8522539138793945,
      "learning_rate": 1.1058649441070075e-07,
      "loss": 0.9594,
      "step": 16910
    },
    {
      "epoch": 0.9083588339507167,
      "grad_norm": 9.474037170410156,
      "learning_rate": 1.0931155257021291e-07,
      "loss": 0.8343,
      "step": 16920
    },
    {
      "epoch": 0.9088956890535245,
      "grad_norm": 12.29819107055664,
      "learning_rate": 1.0804383846861971e-07,
      "loss": 1.3551,
      "step": 16930
    },
    {
      "epoch": 0.9094325441563322,
      "grad_norm": 5.700606346130371,
      "learning_rate": 1.0678335593858558e-07,
      "loss": 0.9949,
      "step": 16940
    },
    {
      "epoch": 0.90996939925914,
      "grad_norm": 4.340880393981934,
      "learning_rate": 1.0553010879091246e-07,
      "loss": 1.0281,
      "step": 16950
    },
    {
      "epoch": 0.9105062543619478,
      "grad_norm": 9.698758125305176,
      "learning_rate": 1.0428410081452566e-07,
      "loss": 0.9805,
      "step": 16960
    },
    {
      "epoch": 0.9110431094647554,
      "grad_norm": 7.991297245025635,
      "learning_rate": 1.0304533577646714e-07,
      "loss": 0.7785,
      "step": 16970
    },
    {
      "epoch": 0.9115799645675632,
      "grad_norm": 5.511057376861572,
      "learning_rate": 1.0181381742187884e-07,
      "loss": 0.7986,
      "step": 16980
    },
    {
      "epoch": 0.912116819670371,
      "grad_norm": 6.462248802185059,
      "learning_rate": 1.0058954947399557e-07,
      "loss": 1.1077,
      "step": 16990
    },
    {
      "epoch": 0.9126536747731787,
      "grad_norm": 4.6878862380981445,
      "learning_rate": 9.937253563413157e-08,
      "loss": 1.3605,
      "step": 17000
    },
    {
      "epoch": 0.9131905298759865,
      "grad_norm": 5.9183220863342285,
      "learning_rate": 9.816277958166975e-08,
      "loss": 1.2065,
      "step": 17010
    },
    {
      "epoch": 0.9137273849787942,
      "grad_norm": 5.923278331756592,
      "learning_rate": 9.69602849740503e-08,
      "loss": 0.7966,
      "step": 17020
    },
    {
      "epoch": 0.914264240081602,
      "grad_norm": 7.234816074371338,
      "learning_rate": 9.576505544676096e-08,
      "loss": 1.004,
      "step": 17030
    },
    {
      "epoch": 0.9148010951844098,
      "grad_norm": 13.594586372375488,
      "learning_rate": 9.457709461332343e-08,
      "loss": 1.2928,
      "step": 17040
    },
    {
      "epoch": 0.9153379502872174,
      "grad_norm": 5.343970775604248,
      "learning_rate": 9.339640606528644e-08,
      "loss": 0.5624,
      "step": 17050
    },
    {
      "epoch": 0.9158748053900252,
      "grad_norm": 6.650351047515869,
      "learning_rate": 9.222299337220963e-08,
      "loss": 1.0104,
      "step": 17060
    },
    {
      "epoch": 0.916411660492833,
      "grad_norm": 5.70102596282959,
      "learning_rate": 9.105686008165776e-08,
      "loss": 0.6376,
      "step": 17070
    },
    {
      "epoch": 0.9169485155956407,
      "grad_norm": 6.201600551605225,
      "learning_rate": 8.989800971918789e-08,
      "loss": 0.9912,
      "step": 17080
    },
    {
      "epoch": 0.9174853706984485,
      "grad_norm": 6.082885265350342,
      "learning_rate": 8.874644578833718e-08,
      "loss": 1.2591,
      "step": 17090
    },
    {
      "epoch": 0.9180222258012563,
      "grad_norm": 7.98124885559082,
      "learning_rate": 8.760217177061492e-08,
      "loss": 0.7846,
      "step": 17100
    },
    {
      "epoch": 0.918559080904064,
      "grad_norm": 6.164219856262207,
      "learning_rate": 8.646519112549045e-08,
      "loss": 0.8477,
      "step": 17110
    },
    {
      "epoch": 0.9190959360068718,
      "grad_norm": 5.993988037109375,
      "learning_rate": 8.53355072903836e-08,
      "loss": 0.7715,
      "step": 17120
    },
    {
      "epoch": 0.9196327911096795,
      "grad_norm": 7.52805757522583,
      "learning_rate": 8.421312368065343e-08,
      "loss": 1.3903,
      "step": 17130
    },
    {
      "epoch": 0.9201696462124872,
      "grad_norm": 6.513925075531006,
      "learning_rate": 8.309804368958868e-08,
      "loss": 0.6142,
      "step": 17140
    },
    {
      "epoch": 0.920706501315295,
      "grad_norm": 13.960413932800293,
      "learning_rate": 8.199027068839627e-08,
      "loss": 0.9503,
      "step": 17150
    },
    {
      "epoch": 0.9212433564181027,
      "grad_norm": 6.463510513305664,
      "learning_rate": 8.088980802619384e-08,
      "loss": 1.4947,
      "step": 17160
    },
    {
      "epoch": 0.9217802115209105,
      "grad_norm": 12.759188652038574,
      "learning_rate": 7.979665902999533e-08,
      "loss": 1.0333,
      "step": 17170
    },
    {
      "epoch": 0.9223170666237183,
      "grad_norm": 6.954909801483154,
      "learning_rate": 7.871082700470544e-08,
      "loss": 1.3297,
      "step": 17180
    },
    {
      "epoch": 0.922853921726526,
      "grad_norm": 4.726139068603516,
      "learning_rate": 7.763231523310627e-08,
      "loss": 1.0929,
      "step": 17190
    },
    {
      "epoch": 0.9233907768293338,
      "grad_norm": 5.912487983703613,
      "learning_rate": 7.656112697584928e-08,
      "loss": 0.6638,
      "step": 17200
    },
    {
      "epoch": 0.9239276319321416,
      "grad_norm": 5.926290035247803,
      "learning_rate": 7.549726547144504e-08,
      "loss": 0.8416,
      "step": 17210
    },
    {
      "epoch": 0.9244644870349493,
      "grad_norm": 3.708927869796753,
      "learning_rate": 7.444073393625267e-08,
      "loss": 0.9909,
      "step": 17220
    },
    {
      "epoch": 0.925001342137757,
      "grad_norm": 6.825359344482422,
      "learning_rate": 7.33915355644707e-08,
      "loss": 1.2265,
      "step": 17230
    },
    {
      "epoch": 0.9255381972405647,
      "grad_norm": 8.064277648925781,
      "learning_rate": 7.234967352812817e-08,
      "loss": 0.9078,
      "step": 17240
    },
    {
      "epoch": 0.9260750523433725,
      "grad_norm": 11.741791725158691,
      "learning_rate": 7.131515097707293e-08,
      "loss": 1.0521,
      "step": 17250
    },
    {
      "epoch": 0.9266119074461803,
      "grad_norm": 6.170499324798584,
      "learning_rate": 7.028797103896507e-08,
      "loss": 1.1996,
      "step": 17260
    },
    {
      "epoch": 0.927148762548988,
      "grad_norm": 8.637792587280273,
      "learning_rate": 6.926813681926436e-08,
      "loss": 1.1815,
      "step": 17270
    },
    {
      "epoch": 0.9276856176517958,
      "grad_norm": 3.948028564453125,
      "learning_rate": 6.825565140122364e-08,
      "loss": 1.1182,
      "step": 17280
    },
    {
      "epoch": 0.9282224727546036,
      "grad_norm": 7.411802291870117,
      "learning_rate": 6.725051784587711e-08,
      "loss": 0.6015,
      "step": 17290
    },
    {
      "epoch": 0.9287593278574113,
      "grad_norm": 5.4390177726745605,
      "learning_rate": 6.625273919203313e-08,
      "loss": 1.0073,
      "step": 17300
    },
    {
      "epoch": 0.929296182960219,
      "grad_norm": 13.381534576416016,
      "learning_rate": 6.526231845626258e-08,
      "loss": 1.8452,
      "step": 17310
    },
    {
      "epoch": 0.9298330380630268,
      "grad_norm": 6.753538608551025,
      "learning_rate": 6.42792586328933e-08,
      "loss": 0.7463,
      "step": 17320
    },
    {
      "epoch": 0.9303698931658345,
      "grad_norm": 4.439027309417725,
      "learning_rate": 6.33035626939979e-08,
      "loss": 1.5172,
      "step": 17330
    },
    {
      "epoch": 0.9309067482686423,
      "grad_norm": 15.756540298461914,
      "learning_rate": 6.233523358938509e-08,
      "loss": 1.5219,
      "step": 17340
    },
    {
      "epoch": 0.93144360337145,
      "grad_norm": 7.112362861633301,
      "learning_rate": 6.13742742465931e-08,
      "loss": 1.0129,
      "step": 17350
    },
    {
      "epoch": 0.9319804584742578,
      "grad_norm": 4.9493608474731445,
      "learning_rate": 6.042068757087793e-08,
      "loss": 0.7846,
      "step": 17360
    },
    {
      "epoch": 0.9325173135770656,
      "grad_norm": 13.696374893188477,
      "learning_rate": 5.947447644520682e-08,
      "loss": 1.2661,
      "step": 17370
    },
    {
      "epoch": 0.9330541686798733,
      "grad_norm": 14.819186210632324,
      "learning_rate": 5.853564373024784e-08,
      "loss": 1.129,
      "step": 17380
    },
    {
      "epoch": 0.9335910237826811,
      "grad_norm": 7.000197887420654,
      "learning_rate": 5.760419226436248e-08,
      "loss": 1.2165,
      "step": 17390
    },
    {
      "epoch": 0.9341278788854888,
      "grad_norm": 7.475527286529541,
      "learning_rate": 5.668012486359675e-08,
      "loss": 1.2956,
      "step": 17400
    },
    {
      "epoch": 0.9346647339882965,
      "grad_norm": 7.747924327850342,
      "learning_rate": 5.5763444321671744e-08,
      "loss": 1.3628,
      "step": 17410
    },
    {
      "epoch": 0.9352015890911043,
      "grad_norm": 13.559918403625488,
      "learning_rate": 5.485415340997669e-08,
      "loss": 1.1587,
      "step": 17420
    },
    {
      "epoch": 0.9357384441939121,
      "grad_norm": 13.788677215576172,
      "learning_rate": 5.3952254877559784e-08,
      "loss": 1.0131,
      "step": 17430
    },
    {
      "epoch": 0.9362752992967198,
      "grad_norm": 4.872044086456299,
      "learning_rate": 5.3057751451119075e-08,
      "loss": 1.1329,
      "step": 17440
    },
    {
      "epoch": 0.9368121543995276,
      "grad_norm": 7.38754415512085,
      "learning_rate": 5.217064583499659e-08,
      "loss": 0.97,
      "step": 17450
    },
    {
      "epoch": 0.9373490095023354,
      "grad_norm": 14.111727714538574,
      "learning_rate": 5.129094071116725e-08,
      "loss": 1.4056,
      "step": 17460
    },
    {
      "epoch": 0.9378858646051431,
      "grad_norm": 7.140087604522705,
      "learning_rate": 5.041863873923275e-08,
      "loss": 0.8585,
      "step": 17470
    },
    {
      "epoch": 0.9384227197079508,
      "grad_norm": 9.142769813537598,
      "learning_rate": 4.9553742556412984e-08,
      "loss": 1.3199,
      "step": 17480
    },
    {
      "epoch": 0.9389595748107585,
      "grad_norm": 4.277403354644775,
      "learning_rate": 4.869625477753798e-08,
      "loss": 0.8005,
      "step": 17490
    },
    {
      "epoch": 0.9394964299135663,
      "grad_norm": 5.613715171813965,
      "learning_rate": 4.7846177995039544e-08,
      "loss": 0.7965,
      "step": 17500
    },
    {
      "epoch": 0.9400332850163741,
      "grad_norm": 5.491856098175049,
      "learning_rate": 4.7003514778945215e-08,
      "loss": 0.5945,
      "step": 17510
    },
    {
      "epoch": 0.9405701401191818,
      "grad_norm": 15.6240816116333,
      "learning_rate": 4.6168267676867386e-08,
      "loss": 1.4097,
      "step": 17520
    },
    {
      "epoch": 0.9411069952219896,
      "grad_norm": 5.749660491943359,
      "learning_rate": 4.534043921399889e-08,
      "loss": 0.8255,
      "step": 17530
    },
    {
      "epoch": 0.9416438503247974,
      "grad_norm": 5.679696559906006,
      "learning_rate": 4.4520031893103e-08,
      "loss": 0.9009,
      "step": 17540
    },
    {
      "epoch": 0.9421807054276051,
      "grad_norm": 6.706498622894287,
      "learning_rate": 4.370704819450705e-08,
      "loss": 0.7823,
      "step": 17550
    },
    {
      "epoch": 0.9427175605304129,
      "grad_norm": 8.199049949645996,
      "learning_rate": 4.290149057609494e-08,
      "loss": 0.8138,
      "step": 17560
    },
    {
      "epoch": 0.9432544156332205,
      "grad_norm": 5.3225860595703125,
      "learning_rate": 4.21033614732988e-08,
      "loss": 0.8175,
      "step": 17570
    },
    {
      "epoch": 0.9437912707360283,
      "grad_norm": 6.2288103103637695,
      "learning_rate": 4.131266329909234e-08,
      "loss": 1.0212,
      "step": 17580
    },
    {
      "epoch": 0.9443281258388361,
      "grad_norm": 5.749591827392578,
      "learning_rate": 4.0529398443983906e-08,
      "loss": 0.9169,
      "step": 17590
    },
    {
      "epoch": 0.9448649809416438,
      "grad_norm": 7.289031505584717,
      "learning_rate": 3.975356927600843e-08,
      "loss": 0.9355,
      "step": 17600
    },
    {
      "epoch": 0.9454018360444516,
      "grad_norm": 16.4879093170166,
      "learning_rate": 3.898517814072078e-08,
      "loss": 1.3169,
      "step": 17610
    },
    {
      "epoch": 0.9459386911472594,
      "grad_norm": 13.460213661193848,
      "learning_rate": 3.822422736118825e-08,
      "loss": 1.4638,
      "step": 17620
    },
    {
      "epoch": 0.9464755462500671,
      "grad_norm": 5.899116039276123,
      "learning_rate": 3.74707192379839e-08,
      "loss": 1.059,
      "step": 17630
    },
    {
      "epoch": 0.9470124013528749,
      "grad_norm": 15.04935359954834,
      "learning_rate": 3.672465604917963e-08,
      "loss": 1.658,
      "step": 17640
    },
    {
      "epoch": 0.9475492564556827,
      "grad_norm": 14.351346015930176,
      "learning_rate": 3.598604005033896e-08,
      "loss": 0.809,
      "step": 17650
    },
    {
      "epoch": 0.9480861115584903,
      "grad_norm": 5.682928562164307,
      "learning_rate": 3.525487347451062e-08,
      "loss": 0.5764,
      "step": 17660
    },
    {
      "epoch": 0.9486229666612981,
      "grad_norm": 6.558468341827393,
      "learning_rate": 3.4531158532221384e-08,
      "loss": 1.407,
      "step": 17670
    },
    {
      "epoch": 0.9491598217641058,
      "grad_norm": 5.316196918487549,
      "learning_rate": 3.3814897411469924e-08,
      "loss": 0.9844,
      "step": 17680
    },
    {
      "epoch": 0.9496966768669136,
      "grad_norm": 9.460941314697266,
      "learning_rate": 3.310609227771933e-08,
      "loss": 0.955,
      "step": 17690
    },
    {
      "epoch": 0.9502335319697214,
      "grad_norm": 5.473784446716309,
      "learning_rate": 3.240474527389209e-08,
      "loss": 1.0483,
      "step": 17700
    },
    {
      "epoch": 0.9507703870725291,
      "grad_norm": 6.276250839233398,
      "learning_rate": 3.1710858520360986e-08,
      "loss": 1.4778,
      "step": 17710
    },
    {
      "epoch": 0.9513072421753369,
      "grad_norm": 5.623984336853027,
      "learning_rate": 3.102443411494627e-08,
      "loss": 0.8079,
      "step": 17720
    },
    {
      "epoch": 0.9518440972781447,
      "grad_norm": 7.380721092224121,
      "learning_rate": 3.0345474132905396e-08,
      "loss": 1.1052,
      "step": 17730
    },
    {
      "epoch": 0.9523809523809523,
      "grad_norm": 4.901718616485596,
      "learning_rate": 2.9673980626930554e-08,
      "loss": 0.8392,
      "step": 17740
    },
    {
      "epoch": 0.9529178074837601,
      "grad_norm": 10.777400016784668,
      "learning_rate": 2.900995562713921e-08,
      "loss": 1.2105,
      "step": 17750
    },
    {
      "epoch": 0.9534546625865679,
      "grad_norm": 4.757724761962891,
      "learning_rate": 2.8353401141070215e-08,
      "loss": 0.9428,
      "step": 17760
    },
    {
      "epoch": 0.9539915176893756,
      "grad_norm": 9.286770820617676,
      "learning_rate": 2.7704319153676606e-08,
      "loss": 1.1475,
      "step": 17770
    },
    {
      "epoch": 0.9545283727921834,
      "grad_norm": 9.04317855834961,
      "learning_rate": 2.7062711627320048e-08,
      "loss": 0.9576,
      "step": 17780
    },
    {
      "epoch": 0.9550652278949912,
      "grad_norm": 10.708128929138184,
      "learning_rate": 2.6428580501764444e-08,
      "loss": 1.2747,
      "step": 17790
    },
    {
      "epoch": 0.9556020829977989,
      "grad_norm": 14.917594909667969,
      "learning_rate": 2.5801927694170948e-08,
      "loss": 1.181,
      "step": 17800
    },
    {
      "epoch": 0.9561389381006067,
      "grad_norm": 6.624945163726807,
      "learning_rate": 2.5182755099090738e-08,
      "loss": 0.8406,
      "step": 17810
    },
    {
      "epoch": 0.9566757932034144,
      "grad_norm": 8.624831199645996,
      "learning_rate": 2.4571064588461148e-08,
      "loss": 1.6098,
      "step": 17820
    },
    {
      "epoch": 0.9572126483062221,
      "grad_norm": 14.403082847595215,
      "learning_rate": 2.3966858011598425e-08,
      "loss": 1.3032,
      "step": 17830
    },
    {
      "epoch": 0.9577495034090299,
      "grad_norm": 5.488682746887207,
      "learning_rate": 2.3370137195192487e-08,
      "loss": 1.043,
      "step": 17840
    },
    {
      "epoch": 0.9582863585118376,
      "grad_norm": 13.663739204406738,
      "learning_rate": 2.2780903943302734e-08,
      "loss": 0.9343,
      "step": 17850
    },
    {
      "epoch": 0.9588232136146454,
      "grad_norm": 13.979706764221191,
      "learning_rate": 2.2199160037350286e-08,
      "loss": 0.9745,
      "step": 17860
    },
    {
      "epoch": 0.9593600687174532,
      "grad_norm": 13.1190185546875,
      "learning_rate": 2.1624907236114378e-08,
      "loss": 1.6075,
      "step": 17870
    },
    {
      "epoch": 0.9598969238202609,
      "grad_norm": 4.753443241119385,
      "learning_rate": 2.1058147275726525e-08,
      "loss": 0.8294,
      "step": 17880
    },
    {
      "epoch": 0.9604337789230687,
      "grad_norm": 14.212505340576172,
      "learning_rate": 2.0498881869665256e-08,
      "loss": 0.7947,
      "step": 17890
    },
    {
      "epoch": 0.9609706340258765,
      "grad_norm": 8.319589614868164,
      "learning_rate": 1.994711270875027e-08,
      "loss": 0.9522,
      "step": 17900
    },
    {
      "epoch": 0.9615074891286842,
      "grad_norm": 5.447271823883057,
      "learning_rate": 1.9402841461138856e-08,
      "loss": 1.1906,
      "step": 17910
    },
    {
      "epoch": 0.9620443442314919,
      "grad_norm": 7.951906204223633,
      "learning_rate": 1.886606977231975e-08,
      "loss": 0.7939,
      "step": 17920
    },
    {
      "epoch": 0.9625811993342996,
      "grad_norm": 7.3565592765808105,
      "learning_rate": 1.8336799265107617e-08,
      "loss": 0.7441,
      "step": 17930
    },
    {
      "epoch": 0.9631180544371074,
      "grad_norm": 14.070670127868652,
      "learning_rate": 1.7815031539639983e-08,
      "loss": 0.8173,
      "step": 17940
    },
    {
      "epoch": 0.9636549095399152,
      "grad_norm": 13.530094146728516,
      "learning_rate": 1.7300768173370564e-08,
      "loss": 1.1088,
      "step": 17950
    },
    {
      "epoch": 0.9641917646427229,
      "grad_norm": 5.844667911529541,
      "learning_rate": 1.67940107210654e-08,
      "loss": 0.6264,
      "step": 17960
    },
    {
      "epoch": 0.9647286197455307,
      "grad_norm": 5.290414333343506,
      "learning_rate": 1.629476071479813e-08,
      "loss": 1.2993,
      "step": 17970
    },
    {
      "epoch": 0.9652654748483385,
      "grad_norm": 5.902510643005371,
      "learning_rate": 1.5803019663944984e-08,
      "loss": 0.5587,
      "step": 17980
    },
    {
      "epoch": 0.9658023299511462,
      "grad_norm": 5.548087120056152,
      "learning_rate": 1.5318789055180916e-08,
      "loss": 1.0156,
      "step": 17990
    },
    {
      "epoch": 0.9663391850539539,
      "grad_norm": 4.363626956939697,
      "learning_rate": 1.4842070352474036e-08,
      "loss": 0.7524,
      "step": 18000
    },
    {
      "epoch": 0.9668760401567617,
      "grad_norm": 7.808571815490723,
      "learning_rate": 1.4372864997082291e-08,
      "loss": 1.0264,
      "step": 18010
    },
    {
      "epoch": 0.9674128952595694,
      "grad_norm": 7.280130386352539,
      "learning_rate": 1.3911174407548189e-08,
      "loss": 0.627,
      "step": 18020
    },
    {
      "epoch": 0.9679497503623772,
      "grad_norm": 4.784152984619141,
      "learning_rate": 1.345699997969574e-08,
      "loss": 0.5966,
      "step": 18030
    },
    {
      "epoch": 0.968486605465185,
      "grad_norm": 15.725899696350098,
      "learning_rate": 1.3010343086624354e-08,
      "loss": 1.6275,
      "step": 18040
    },
    {
      "epoch": 0.9690234605679927,
      "grad_norm": 5.423874855041504,
      "learning_rate": 1.2571205078706627e-08,
      "loss": 0.8308,
      "step": 18050
    },
    {
      "epoch": 0.9695603156708005,
      "grad_norm": 4.604104518890381,
      "learning_rate": 1.2139587283583055e-08,
      "loss": 0.7535,
      "step": 18060
    },
    {
      "epoch": 0.9700971707736082,
      "grad_norm": 16.02294158935547,
      "learning_rate": 1.1715491006158153e-08,
      "loss": 1.4176,
      "step": 18070
    },
    {
      "epoch": 0.970634025876416,
      "grad_norm": 8.886614799499512,
      "learning_rate": 1.129891752859713e-08,
      "loss": 0.7895,
      "step": 18080
    },
    {
      "epoch": 0.9711708809792237,
      "grad_norm": 14.897608757019043,
      "learning_rate": 1.0889868110321445e-08,
      "loss": 1.1152,
      "step": 18090
    },
    {
      "epoch": 0.9717077360820314,
      "grad_norm": 9.89549732208252,
      "learning_rate": 1.048834398800519e-08,
      "loss": 1.0447,
      "step": 18100
    },
    {
      "epoch": 0.9722445911848392,
      "grad_norm": 14.480354309082031,
      "learning_rate": 1.0094346375571218e-08,
      "loss": 1.559,
      "step": 18110
    },
    {
      "epoch": 0.972781446287647,
      "grad_norm": 5.865367412567139,
      "learning_rate": 9.707876464187805e-09,
      "loss": 0.7892,
      "step": 18120
    },
    {
      "epoch": 0.9733183013904547,
      "grad_norm": 7.592047214508057,
      "learning_rate": 9.328935422264208e-09,
      "loss": 0.9339,
      "step": 18130
    },
    {
      "epoch": 0.9738551564932625,
      "grad_norm": 10.322150230407715,
      "learning_rate": 8.957524395449002e-09,
      "loss": 0.9784,
      "step": 18140
    },
    {
      "epoch": 0.9743920115960703,
      "grad_norm": 5.849143028259277,
      "learning_rate": 8.5936445066237e-09,
      "loss": 0.8782,
      "step": 18150
    },
    {
      "epoch": 0.974928866698878,
      "grad_norm": 6.506575107574463,
      "learning_rate": 8.237296855902465e-09,
      "loss": 1.0272,
      "step": 18160
    },
    {
      "epoch": 0.9754657218016857,
      "grad_norm": 4.64422082901001,
      "learning_rate": 7.888482520626573e-09,
      "loss": 0.9683,
      "step": 18170
    },
    {
      "epoch": 0.9760025769044934,
      "grad_norm": 6.020284652709961,
      "learning_rate": 7.547202555362176e-09,
      "loss": 1.2065,
      "step": 18180
    },
    {
      "epoch": 0.9765394320073012,
      "grad_norm": 6.799893856048584,
      "learning_rate": 7.213457991896711e-09,
      "loss": 0.615,
      "step": 18190
    },
    {
      "epoch": 0.977076287110109,
      "grad_norm": 6.635623455047607,
      "learning_rate": 6.887249839235833e-09,
      "loss": 1.1637,
      "step": 18200
    },
    {
      "epoch": 0.9776131422129167,
      "grad_norm": 7.470244407653809,
      "learning_rate": 6.568579083600646e-09,
      "loss": 0.8002,
      "step": 18210
    },
    {
      "epoch": 0.9781499973157245,
      "grad_norm": 8.707500457763672,
      "learning_rate": 6.257446688424929e-09,
      "loss": 1.2426,
      "step": 18220
    },
    {
      "epoch": 0.9786868524185323,
      "grad_norm": 15.11837100982666,
      "learning_rate": 5.953853594351244e-09,
      "loss": 1.3893,
      "step": 18230
    },
    {
      "epoch": 0.97922370752134,
      "grad_norm": 4.197880268096924,
      "learning_rate": 5.6578007192284456e-09,
      "loss": 0.7658,
      "step": 18240
    },
    {
      "epoch": 0.9797605626241478,
      "grad_norm": 6.866054534912109,
      "learning_rate": 5.369288958110008e-09,
      "loss": 1.4916,
      "step": 18250
    },
    {
      "epoch": 0.9802974177269554,
      "grad_norm": 13.23369312286377,
      "learning_rate": 5.088319183249868e-09,
      "loss": 0.7967,
      "step": 18260
    },
    {
      "epoch": 0.9808342728297632,
      "grad_norm": 14.598579406738281,
      "learning_rate": 4.8148922441002e-09,
      "loss": 1.3152,
      "step": 18270
    },
    {
      "epoch": 0.981371127932571,
      "grad_norm": 4.72528600692749,
      "learning_rate": 4.549008967309754e-09,
      "loss": 1.0615,
      "step": 18280
    },
    {
      "epoch": 0.9819079830353787,
      "grad_norm": 6.146447658538818,
      "learning_rate": 4.290670156719967e-09,
      "loss": 1.617,
      "step": 18290
    },
    {
      "epoch": 0.9824448381381865,
      "grad_norm": 6.958872318267822,
      "learning_rate": 4.039876593363023e-09,
      "loss": 0.8482,
      "step": 18300
    },
    {
      "epoch": 0.9829816932409943,
      "grad_norm": 6.924693584442139,
      "learning_rate": 3.7966290354607416e-09,
      "loss": 0.9607,
      "step": 18310
    },
    {
      "epoch": 0.983518548343802,
      "grad_norm": 15.666465759277344,
      "learning_rate": 3.5609282184198567e-09,
      "loss": 1.009,
      "step": 18320
    },
    {
      "epoch": 0.9840554034466098,
      "grad_norm": 5.486618995666504,
      "learning_rate": 3.332774854831744e-09,
      "loss": 1.428,
      "step": 18330
    },
    {
      "epoch": 0.9845922585494176,
      "grad_norm": 5.269192695617676,
      "learning_rate": 3.1121696344701968e-09,
      "loss": 1.1813,
      "step": 18340
    },
    {
      "epoch": 0.9851291136522252,
      "grad_norm": 14.692547798156738,
      "learning_rate": 2.899113224287542e-09,
      "loss": 1.6131,
      "step": 18350
    },
    {
      "epoch": 0.985665968755033,
      "grad_norm": 4.017879962921143,
      "learning_rate": 2.6936062684151942e-09,
      "loss": 0.9784,
      "step": 18360
    },
    {
      "epoch": 0.9862028238578407,
      "grad_norm": 5.900289058685303,
      "learning_rate": 2.4956493881594935e-09,
      "loss": 1.1613,
      "step": 18370
    },
    {
      "epoch": 0.9867396789606485,
      "grad_norm": 5.436233043670654,
      "learning_rate": 2.305243182001149e-09,
      "loss": 0.6572,
      "step": 18380
    },
    {
      "epoch": 0.9872765340634563,
      "grad_norm": 5.662302494049072,
      "learning_rate": 2.122388225592742e-09,
      "loss": 1.1928,
      "step": 18390
    },
    {
      "epoch": 0.987813389166264,
      "grad_norm": 13.394158363342285,
      "learning_rate": 1.947085071757615e-09,
      "loss": 0.9659,
      "step": 18400
    },
    {
      "epoch": 0.9883502442690718,
      "grad_norm": 5.0899505615234375,
      "learning_rate": 1.7793342504876521e-09,
      "loss": 0.9742,
      "step": 18410
    },
    {
      "epoch": 0.9888870993718796,
      "grad_norm": 7.820817947387695,
      "learning_rate": 1.6191362689413347e-09,
      "loss": 0.7846,
      "step": 18420
    },
    {
      "epoch": 0.9894239544746872,
      "grad_norm": 13.910109519958496,
      "learning_rate": 1.4664916114440209e-09,
      "loss": 0.7813,
      "step": 18430
    },
    {
      "epoch": 0.989960809577495,
      "grad_norm": 8.051785469055176,
      "learning_rate": 1.3214007394837801e-09,
      "loss": 1.1285,
      "step": 18440
    },
    {
      "epoch": 0.9904976646803028,
      "grad_norm": 14.118709564208984,
      "learning_rate": 1.1838640917125055e-09,
      "loss": 1.2327,
      "step": 18450
    },
    {
      "epoch": 0.9910345197831105,
      "grad_norm": 5.370258331298828,
      "learning_rate": 1.0538820839425813e-09,
      "loss": 0.8404,
      "step": 18460
    },
    {
      "epoch": 0.9915713748859183,
      "grad_norm": 7.795014381408691,
      "learning_rate": 9.31455109147439e-10,
      "loss": 1.3685,
      "step": 18470
    },
    {
      "epoch": 0.992108229988726,
      "grad_norm": 14.720968246459961,
      "learning_rate": 8.165835374590592e-10,
      "loss": 1.1518,
      "step": 18480
    },
    {
      "epoch": 0.9926450850915338,
      "grad_norm": 6.658933162689209,
      "learning_rate": 7.092677161668615e-10,
      "loss": 1.3963,
      "step": 18490
    },
    {
      "epoch": 0.9931819401943416,
      "grad_norm": 6.427835941314697,
      "learning_rate": 6.095079697174267e-10,
      "loss": 1.0098,
      "step": 18500
    },
    {
      "epoch": 0.9937187952971493,
      "grad_norm": 4.885433673858643,
      "learning_rate": 5.173045997136639e-10,
      "loss": 0.9866,
      "step": 18510
    },
    {
      "epoch": 0.994255650399957,
      "grad_norm": 6.107245445251465,
      "learning_rate": 4.3265788491259066e-10,
      "loss": 0.7839,
      "step": 18520
    },
    {
      "epoch": 0.9947925055027648,
      "grad_norm": 6.574924945831299,
      "learning_rate": 3.5556808122561017e-10,
      "loss": 0.8508,
      "step": 18530
    },
    {
      "epoch": 0.9953293606055725,
      "grad_norm": 5.311215877532959,
      "learning_rate": 2.8603542171740107e-10,
      "loss": 0.5612,
      "step": 18540
    },
    {
      "epoch": 0.9958662157083803,
      "grad_norm": 7.114765644073486,
      "learning_rate": 2.2406011660508486e-10,
      "loss": 0.746,
      "step": 18550
    },
    {
      "epoch": 0.9964030708111881,
      "grad_norm": 5.029722213745117,
      "learning_rate": 1.6964235325794832e-10,
      "loss": 0.7767,
      "step": 18560
    },
    {
      "epoch": 0.9969399259139958,
      "grad_norm": 15.190839767456055,
      "learning_rate": 1.227822961963332e-10,
      "loss": 1.2298,
      "step": 18570
    },
    {
      "epoch": 0.9974767810168036,
      "grad_norm": 5.3325066566467285,
      "learning_rate": 8.34800870919139e-11,
      "loss": 0.7517,
      "step": 18580
    },
    {
      "epoch": 0.9980136361196114,
      "grad_norm": 6.228683948516846,
      "learning_rate": 5.1735844766587215e-11,
      "loss": 0.6129,
      "step": 18590
    },
    {
      "epoch": 0.9985504912224191,
      "grad_norm": 6.248929500579834,
      "learning_rate": 2.754966519219471e-11,
      "loss": 0.7911,
      "step": 18600
    },
    {
      "epoch": 0.9990873463252268,
      "grad_norm": 7.371974468231201,
      "learning_rate": 1.0921621490522782e-11,
      "loss": 1.4155,
      "step": 18610
    },
    {
      "epoch": 0.9996242014280345,
      "grad_norm": 5.834037780761719,
      "learning_rate": 1.851763933025108e-12,
      "loss": 0.9206,
      "step": 18620
    },
    {
      "epoch": 0.9999463144897193,
      "step": 18626,
      "total_flos": 1.64373383189176e+19,
      "train_loss": 1.1301016090994784,
      "train_runtime": 38012.227,
      "train_samples_per_second": 7.84,
      "train_steps_per_second": 0.49
    }
  ],
  "logging_steps": 10,
  "max_steps": 18627,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 1000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.64373383189176e+19,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
