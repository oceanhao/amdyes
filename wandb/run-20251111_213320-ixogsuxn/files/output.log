wandb: Detected [huggingface_hub.inference, openai] in use.
wandb: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
wandb: For more information, check out the docs at: https://weave-docs.wandb.ai/
  0%|          | 0/31875 [00:00<?, ?it/s]`generation_config` default values have been modified to match model-specific defaults: {'temperature': 1e-06, 'repetition_penalty': 1.05}. If this is not desired, please set these values explicitly.
Traceback (most recent call last):
  File "/remote-home/haohh/_cvpr2025/VG-LLM/src/qwen_vl/train/train_qwen_grpo.py", line 536, in <module>
    train(attn_implementation="flash_attention_2")
  File "/remote-home/haohh/_cvpr2025/VG-LLM/src/qwen_vl/train/train_qwen_grpo.py", line 521, in train
    grpo_trainer.train()
  File "/remote-home/haohh/anaconda3/envs/vgllm/lib/python3.10/site-packages/transformers/trainer.py", line 2245, in train
    return inner_training_loop(
  File "/remote-home/haohh/anaconda3/envs/vgllm/lib/python3.10/site-packages/transformers/trainer.py", line 2556, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File "/remote-home/haohh/anaconda3/envs/vgllm/lib/python3.10/site-packages/transformers/trainer.py", line 3764, in training_step
    self.accelerator.backward(loss, **kwargs)
  File "/remote-home/haohh/anaconda3/envs/vgllm/lib/python3.10/site-packages/accelerate/accelerator.py", line 2321, in backward
    self.deepspeed_engine_wrapped.backward(loss, **kwargs)
  File "/remote-home/haohh/anaconda3/envs/vgllm/lib/python3.10/site-packages/accelerate/utils/deepspeed.py", line 275, in backward
    self.engine.step()
  File "/remote-home/haohh/anaconda3/envs/vgllm/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2322, in step
    self._take_model_step(lr_kwargs)
  File "/remote-home/haohh/anaconda3/envs/vgllm/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2225, in _take_model_step
    self.optimizer.step()
  File "/remote-home/haohh/anaconda3/envs/vgllm/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 1914, in step
    self._optimizer_step(i)
  File "/remote-home/haohh/anaconda3/envs/vgllm/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 1820, in _optimizer_step
    self.optimizer.step()
  File "/remote-home/haohh/anaconda3/envs/vgllm/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/remote-home/haohh/anaconda3/envs/vgllm/lib/python3.10/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/remote-home/haohh/anaconda3/envs/vgllm/lib/python3.10/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/remote-home/haohh/anaconda3/envs/vgllm/lib/python3.10/site-packages/torch/optim/adamw.py", line 209, in step
    has_complex = self._init_group(
  File "/remote-home/haohh/anaconda3/envs/vgllm/lib/python3.10/site-packages/torch/optim/adamw.py", line 148, in _init_group
    state["exp_avg"] = torch.zeros_like(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 11.49 GiB. GPU 0 has a total capacity of 79.14 GiB of which 6.08 GiB is free. Process 3689064 has 28.62 GiB memory in use. Process 131405 has 44.43 GiB memory in use. Of the allocated memory 37.84 GiB is allocated by PyTorch, and 5.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]: Traceback (most recent call last):
[rank0]:   File "/remote-home/haohh/_cvpr2025/VG-LLM/src/qwen_vl/train/train_qwen_grpo.py", line 536, in <module>
[rank0]:     train(attn_implementation="flash_attention_2")
[rank0]:   File "/remote-home/haohh/_cvpr2025/VG-LLM/src/qwen_vl/train/train_qwen_grpo.py", line 521, in train
[rank0]:     grpo_trainer.train()
[rank0]:   File "/remote-home/haohh/anaconda3/envs/vgllm/lib/python3.10/site-packages/transformers/trainer.py", line 2245, in train
[rank0]:     return inner_training_loop(
[rank0]:   File "/remote-home/haohh/anaconda3/envs/vgllm/lib/python3.10/site-packages/transformers/trainer.py", line 2556, in _inner_training_loop
[rank0]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank0]:   File "/remote-home/haohh/anaconda3/envs/vgllm/lib/python3.10/site-packages/transformers/trainer.py", line 3764, in training_step
[rank0]:     self.accelerator.backward(loss, **kwargs)
[rank0]:   File "/remote-home/haohh/anaconda3/envs/vgllm/lib/python3.10/site-packages/accelerate/accelerator.py", line 2321, in backward
[rank0]:     self.deepspeed_engine_wrapped.backward(loss, **kwargs)
[rank0]:   File "/remote-home/haohh/anaconda3/envs/vgllm/lib/python3.10/site-packages/accelerate/utils/deepspeed.py", line 275, in backward
[rank0]:     self.engine.step()
[rank0]:   File "/remote-home/haohh/anaconda3/envs/vgllm/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2322, in step
[rank0]:     self._take_model_step(lr_kwargs)
[rank0]:   File "/remote-home/haohh/anaconda3/envs/vgllm/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2225, in _take_model_step
[rank0]:     self.optimizer.step()
[rank0]:   File "/remote-home/haohh/anaconda3/envs/vgllm/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 1914, in step
[rank0]:     self._optimizer_step(i)
[rank0]:   File "/remote-home/haohh/anaconda3/envs/vgllm/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 1820, in _optimizer_step
[rank0]:     self.optimizer.step()
[rank0]:   File "/remote-home/haohh/anaconda3/envs/vgllm/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
[rank0]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)
[rank0]:   File "/remote-home/haohh/anaconda3/envs/vgllm/lib/python3.10/site-packages/torch/optim/optimizer.py", line 487, in wrapper
[rank0]:     out = func(*args, **kwargs)
[rank0]:   File "/remote-home/haohh/anaconda3/envs/vgllm/lib/python3.10/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
[rank0]:     ret = func(self, *args, **kwargs)
[rank0]:   File "/remote-home/haohh/anaconda3/envs/vgllm/lib/python3.10/site-packages/torch/optim/adamw.py", line 209, in step
[rank0]:     has_complex = self._init_group(
[rank0]:   File "/remote-home/haohh/anaconda3/envs/vgllm/lib/python3.10/site-packages/torch/optim/adamw.py", line 148, in _init_group
[rank0]:     state["exp_avg"] = torch.zeros_like(
[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 11.49 GiB. GPU 0 has a total capacity of 79.14 GiB of which 6.08 GiB is free. Process 3689064 has 28.62 GiB memory in use. Process 131405 has 44.43 GiB memory in use. Of the allocated memory 37.84 GiB is allocated by PyTorch, and 5.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
